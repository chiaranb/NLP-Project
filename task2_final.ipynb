{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24a66d6c"
   },
   "source": [
    "# Task 2: Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning of the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VF8q5DVSP65"
   },
   "source": [
    "Mount the google drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1900,
     "status": "ok",
     "timestamp": 1746728417226,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "XeIabhZ8STiN",
    "outputId": "3f7464be-9d25-4d8f-80a9-5ce4ff7254ec"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0979823"
   },
   "source": [
    "### Dataset cleaning process\n",
    "\n",
    "The aim of this section is to create a single dataset using the two datasets at our disposal to simplify the retrieval of the information. Specifically we are interested in the descriptions of the problems, the various examples for the solutions and the respective time and space complexities. The aim is to simplify the retrieval of the information contained in the datasets. We then proceed to eliminate the na values, fix the index, and eliminate the comments from the code solutions.\n",
    "\n",
    "\n",
    "First, the datasets problem_and_human_solutions_list and complexity_labels_light are downloaded and turned into DataFrames.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 4305,
     "status": "ok",
     "timestamp": 1746728421953,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "670417a4"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15508,
     "status": "ok",
     "timestamp": 1746728437457,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "6afa05bb",
    "outputId": "f07160d1-bc41-457d-b3f8-0e1bbdcc993a"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from Hugging Face Hub\n",
    "file_path_1 = hf_hub_download(\n",
    "    repo_id=\"facebook/BigOBench\",\n",
    "    filename=\"data/problem_and_human_solutions_list.jsonl\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Read the JSONL file and convert it to a DataFrame\n",
    "df_problem_and_human_solutions_list = []\n",
    "with open(file_path_1, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            df_problem_and_human_solutions_list.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Erorr JSON:\", e)\n",
    "\n",
    "df_problem_and_human_solutions_list = pd.DataFrame(df_problem_and_human_solutions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 7253,
     "status": "ok",
     "timestamp": 1746728444705,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "44287494"
   },
   "outputs": [],
   "source": [
    "# Download the second dataset from Hugging Face Hub\n",
    "file_path_2 = hf_hub_download(\n",
    "    repo_id=\"facebook/BigOBench\",\n",
    "    filename=\"data/complexity_labels_light.jsonl\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Read the JSONL file and convert it to a DataFrame\n",
    "df_complexity_labels_light = []\n",
    "with open(file_path_2, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            df_complexity_labels_light.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Erorr JSON:\", e)\n",
    "\n",
    "df_complexity_labels_light = pd.DataFrame(df_complexity_labels_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 774,
     "status": "ok",
     "timestamp": 1746728445484,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "3f96c98f",
    "outputId": "2ff9110b-cdf2-441c-81ad-230da8180e59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>description</th>\n",
       "      <th>correct_solution_list</th>\n",
       "      <th>data_source</th>\n",
       "      <th>source_specific_limits</th>\n",
       "      <th>codeforces_specific_metadata</th>\n",
       "      <th>tests</th>\n",
       "      <th>human_accuracy_rate</th>\n",
       "      <th>dataclass</th>\n",
       "      <th>complexity_framework</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>{'text': 'Xenia has a set of weights and pan s...</td>\n",
       "      <td>[{'solution_id': '0_0', 'solution_code': '__au...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 2, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 339, 'cf_index': 'C', 'cf_po...</td>\n",
       "      <td>{'public_tests': [{'input': '0000000101\n",
       "3\n",
       "', '...</td>\n",
       "      <td>0.281633</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.40579710144927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1547_E. Air Conditioners</td>\n",
       "      <td>{'text': 'On a strip of land of length n there...</td>\n",
       "      <td>[{'solution_id': '1_0', 'solution_code': 'def ...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 2, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 1547, 'cf_index': 'E', 'cf_p...</td>\n",
       "      <td>{'public_tests': [{'input': '5\n",
       "\n",
       "6 2\n",
       "2 5\n",
       "14 16\n",
       "...</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.11409395973154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>268_C. Beautiful Sets of Points</td>\n",
       "      <td>{'text': 'Manao has invented a new mathematica...</td>\n",
       "      <td>[{'solution_id': '2_0', 'solution_code': 'if _...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 1, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 268, 'cf_index': 'C', 'cf_po...</td>\n",
       "      <td>{'public_tests': [{'input': '2 2\n",
       "', 'output': ...</td>\n",
       "      <td>0.525066</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.05527638190954...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>478_C. Table Decorations</td>\n",
       "      <td>{'text': 'You have r red, g green and b blue b...</td>\n",
       "      <td>[{'solution_id': '3_0', 'solution_code': 'a = ...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 1, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 478, 'cf_index': 'C', 'cf_po...</td>\n",
       "      <td>{'public_tests': [{'input': '1 1 1\n",
       "', 'output'...</td>\n",
       "      <td>0.562264</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.41610738255033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5_C. Longest Regular Bracket Sequence</td>\n",
       "      <td>{'text': 'This is yet another problem dealing ...</td>\n",
       "      <td>[{'solution_id': '4_0', 'solution_code': 'stri...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 2, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 5, 'cf_index': 'C', 'cf_poin...</td>\n",
       "      <td>{'public_tests': [{'input': ')((())))(()())\n",
       "',...</td>\n",
       "      <td>0.395939</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.03846153846153...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  problem_id                           problem_name  \\\n",
       "0          0               339_C. Xenia and Weights   \n",
       "1          1               1547_E. Air Conditioners   \n",
       "2          2        268_C. Beautiful Sets of Points   \n",
       "3          3               478_C. Table Decorations   \n",
       "4          4  5_C. Longest Regular Bracket Sequence   \n",
       "\n",
       "                                         description  \\\n",
       "0  {'text': 'Xenia has a set of weights and pan s...   \n",
       "1  {'text': 'On a strip of land of length n there...   \n",
       "2  {'text': 'Manao has invented a new mathematica...   \n",
       "3  {'text': 'You have r red, g green and b blue b...   \n",
       "4  {'text': 'This is yet another problem dealing ...   \n",
       "\n",
       "                               correct_solution_list data_source  \\\n",
       "0  [{'solution_id': '0_0', 'solution_code': '__au...  CODEFORCES   \n",
       "1  [{'solution_id': '1_0', 'solution_code': 'def ...  CODEFORCES   \n",
       "2  [{'solution_id': '2_0', 'solution_code': 'if _...  CODEFORCES   \n",
       "3  [{'solution_id': '3_0', 'solution_code': 'a = ...  CODEFORCES   \n",
       "4  [{'solution_id': '4_0', 'solution_code': 'stri...  CODEFORCES   \n",
       "\n",
       "                              source_specific_limits  \\\n",
       "0  {'time_limit': {'seconds': 2, 'nanos': 0}, 'me...   \n",
       "1  {'time_limit': {'seconds': 2, 'nanos': 0}, 'me...   \n",
       "2  {'time_limit': {'seconds': 1, 'nanos': 0}, 'me...   \n",
       "3  {'time_limit': {'seconds': 1, 'nanos': 0}, 'me...   \n",
       "4  {'time_limit': {'seconds': 2, 'nanos': 0}, 'me...   \n",
       "\n",
       "                        codeforces_specific_metadata  \\\n",
       "0  {'cf_contest_id': 339, 'cf_index': 'C', 'cf_po...   \n",
       "1  {'cf_contest_id': 1547, 'cf_index': 'E', 'cf_p...   \n",
       "2  {'cf_contest_id': 268, 'cf_index': 'C', 'cf_po...   \n",
       "3  {'cf_contest_id': 478, 'cf_index': 'C', 'cf_po...   \n",
       "4  {'cf_contest_id': 5, 'cf_index': 'C', 'cf_poin...   \n",
       "\n",
       "                                               tests  human_accuracy_rate  \\\n",
       "0  {'public_tests': [{'input': '0000000101\n",
       "3\n",
       "', '...             0.281633   \n",
       "1  {'public_tests': [{'input': '5\n",
       "\n",
       "6 2\n",
       "2 5\n",
       "14 16\n",
       "...             0.620833   \n",
       "2  {'public_tests': [{'input': '2 2\n",
       "', 'output': ...             0.525066   \n",
       "3  {'public_tests': [{'input': '1 1 1\n",
       "', 'output'...             0.562264   \n",
       "4  {'public_tests': [{'input': ')((())))(()())\n",
       "',...             0.395939   \n",
       "\n",
       "                                           dataclass  \\\n",
       "0  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "1  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "2  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "3  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "4  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "\n",
       "                                complexity_framework  \n",
       "0  {'time_complexity_fail_rate': 0.40579710144927...  \n",
       "1  {'time_complexity_fail_rate': 0.11409395973154...  \n",
       "2  {'time_complexity_fail_rate': 0.05527638190954...  \n",
       "3  {'time_complexity_fail_rate': 0.41610738255033...  \n",
       "4  {'time_complexity_fail_rate': 0.03846153846153...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_problem_and_human_solutions_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746728445837,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "2fa53c30",
    "outputId": "44760977-2764-4ea3-bd80-f2ae87756a80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "      <th>time_curve_coefficient</th>\n",
       "      <th>space_curve_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_0</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.012433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_2</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_4</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  problem_id              problem_name solution_id time_complexity_inferred  \\\n",
       "0          0  339_C. Xenia and Weights         0_0                     O(1)   \n",
       "1          0  339_C. Xenia and Weights         0_1                     None   \n",
       "2          0  339_C. Xenia and Weights         0_2                     O(1)   \n",
       "3          0  339_C. Xenia and Weights         0_3                     None   \n",
       "4          0  339_C. Xenia and Weights         0_4                     O(1)   \n",
       "\n",
       "  space_complexity_inferred  time_curve_coefficient  space_curve_coefficient  \n",
       "0                   O(n**2)                0.000019                 0.012433  \n",
       "1                      None                     NaN                      NaN  \n",
       "2                      O(1)                0.000004               584.000000  \n",
       "3                      None                     NaN                      NaN  \n",
       "4                      O(1)                0.000013               323.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity_labels_light.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADwtm79WOsZL"
   },
   "source": [
    "The attribute 'correst_solution_list' of the first dataset contains a list of code solutions for each sample. These solutions are stored as dictionaries containing as keys 'solution_id' and 'solution_code'. The aim is to obtain two new columns (attributes) in the final dataset one containing the solution id, and the other containing one solution code per sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBMnCngnQPc6"
   },
   "source": [
    "The Datasets results too big to be managed with the resources at our disposal, so it was decided to reduce the size of the datasets reducing the number of solutions for each description. After many considerations, it was choosen 300000 as the maximum amount of total samples for the final dataset. The following code reduces to 100 the maximum number of solutions for each description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1746728446551,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "OyMNA4YRueWr",
    "outputId": "99f13d0d-7669-4ea4-d553-400aa880d979"
   },
   "outputs": [],
   "source": [
    "# reduce num of possible correct solutions in correct_solution_list\n",
    "max = 100  # num of corresct solutions kept\n",
    "\n",
    "for problem in df_problem_and_human_solutions_list.correct_solution_list:\n",
    "    if len(problem) > max:\n",
    "        problem[:] = problem[:max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1746728446553,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "nglRhQOR7HP7",
    "outputId": "90685974-cbbe-4b2b-b36e-c73559fbde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 69\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 64\n",
      "Number of samples: 75\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 54\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 85\n",
      "Number of samples: 100\n",
      "Number of samples: 75\n",
      "Number of samples: 86\n",
      "Number of samples: 100\n",
      "Number of samples: 90\n",
      "Number of samples: 78\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 98\n",
      "Number of samples: 100\n",
      "Number of samples: 91\n"
     ]
    }
   ],
   "source": [
    "# check num of solutions:\n",
    "for problem in df_problem_and_human_solutions_list.correct_solution_list[:30]:\n",
    "    print('Number of samples:', len(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgJTWN44RB3i"
   },
   "source": [
    "Now we create a new DataFrame with columns 'solution_id' and 'solution_code' from the attribute 'correct_solution_list', and 'problem_description' from the atribute 'description' of the dataset df_problem_and_human_solutions_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746728446564,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "gH6WnYt4-ZcW",
    "outputId": "7d967719-13d0-4adf-9381-9af2a6f2d055"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_65</td>\n",
       "      <td>p, m = [i for i, x in enumerate(input(), 1) if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_66</td>\n",
       "      <td>s=input()\\nA=[]\\nfor i in range(1,11):\\n    if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_67</td>\n",
       "      <td>def go(i, d, last):\\n    if i&gt;0 and d&lt;=0 or d&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_68</td>\n",
       "      <td>from sys import setrecursionlimit\\nsetrecursio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>On a strip of land of length n there are k air...</td>\n",
       "      <td>1_0</td>\n",
       "      <td>def ii(): return int(input())\\ndef si(): retur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  problem_description solution_id  \\\n",
       "0   Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1   Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2   Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3   Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4   Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "..                                                ...         ...   \n",
       "65  Xenia has a set of weights and pan scales. Eac...        0_65   \n",
       "66  Xenia has a set of weights and pan scales. Eac...        0_66   \n",
       "67  Xenia has a set of weights and pan scales. Eac...        0_67   \n",
       "68  Xenia has a set of weights and pan scales. Eac...        0_68   \n",
       "69  On a strip of land of length n there are k air...         1_0   \n",
       "\n",
       "                                        solution_code  \n",
       "0   __author__ = 'ratnesh.mishra'\\n\\nweights = map...  \n",
       "1   s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...  \n",
       "2   import sys\\nsys.setrecursionlimit (1000000)\\n\\...  \n",
       "3   from __future__ import division, print_functio...  \n",
       "4   # Target - Expert on CF\\n# Be Humblefool\\n\\nim...  \n",
       "..                                                ...  \n",
       "65  p, m = [i for i, x in enumerate(input(), 1) if...  \n",
       "66  s=input()\\nA=[]\\nfor i in range(1,11):\\n    if...  \n",
       "67  def go(i, d, last):\\n    if i>0 and d<=0 or d>...  \n",
       "68  from sys import setrecursionlimit\\nsetrecursio...  \n",
       "69  def ii(): return int(input())\\ndef si(): retur...  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify correct_solutions_list to have two different attributes (solution_id and solution_code) insted of a list of them\n",
    "solutions_list = df_problem_and_human_solutions_list.correct_solution_list.copy()\n",
    "descriptions = df_problem_and_human_solutions_list.description.copy()\n",
    "\n",
    "rows = []\n",
    "for desc, solutions in zip(descriptions, solutions_list):\n",
    "    for solution in solutions:\n",
    "        rows.append({\n",
    "            'problem_description': desc['text'],\n",
    "            'solution_id': solution['solution_id'],\n",
    "            'solution_code': solution['solution_code']\n",
    "        })\n",
    "new_matrix = pd.DataFrame(rows)\n",
    "\n",
    "new_matrix.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746728446568,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "pOQnh-1n8RVF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293586, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746728446575,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "p7lup8GP98nd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>293586</td>\n",
       "      <td>293586</td>\n",
       "      <td>293586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3031</td>\n",
       "      <td>293586</td>\n",
       "      <td>289745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hilbert's Hotel is a very unusual hotel since ...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>from sys import stdin, stdout\\ninput = stdin.b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      problem_description solution_id  \\\n",
       "count                                              293586      293586   \n",
       "unique                                               3031      293586   \n",
       "top     Hilbert's Hotel is a very unusual hotel since ...         0_0   \n",
       "freq                                                  200           1   \n",
       "\n",
       "                                            solution_code  \n",
       "count                                              293586  \n",
       "unique                                             289745  \n",
       "top     from sys import stdin, stdout\\ninput = stdin.b...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRatAW5IZBSG"
   },
   "source": [
    "Now the Dataset new_matrix and the second dataset are merged to abtain one line for each solution and keep only the solutions present in the new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1746728454460,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "lVBcqT8kapAI",
    "outputId": "ff4e3617-2cca-4d45-faa8-a0d43672be8e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3  Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...  \n",
       "1  s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...  \n",
       "2  import sys\\nsys.setrecursionlimit (1000000)\\n\\...  \n",
       "3  from __future__ import division, print_functio...  \n",
       "4  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "      <th>time_curve_coefficient</th>\n",
       "      <th>space_curve_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_0</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.012433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_2</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_4</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  problem_id              problem_name solution_id time_complexity_inferred  \\\n",
       "0          0  339_C. Xenia and Weights         0_0                     O(1)   \n",
       "1          0  339_C. Xenia and Weights         0_1                     None   \n",
       "2          0  339_C. Xenia and Weights         0_2                     O(1)   \n",
       "3          0  339_C. Xenia and Weights         0_3                     None   \n",
       "4          0  339_C. Xenia and Weights         0_4                     O(1)   \n",
       "\n",
       "  space_complexity_inferred  time_curve_coefficient  space_curve_coefficient  \n",
       "0                   O(n**2)                0.000019                 0.012433  \n",
       "1                      None                     NaN                      NaN  \n",
       "2                      O(1)                0.000004               584.000000  \n",
       "3                      None                     NaN                      NaN  \n",
       "4                      O(1)                0.000013               323.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity_labels_light.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1746728455521,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "OjxHgVN-ZbRc"
   },
   "outputs": [],
   "source": [
    "df = new_matrix.merge(df_complexity_labels_light, on='solution_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1746728455901,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "McbzBbQKZ2o8",
    "outputId": "d86e6ad6-2344-45dd-a666-c098bb22227c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "      <th>time_curve_coefficient</th>\n",
       "      <th>space_curve_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.012433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3  Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code problem_id  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...          0   \n",
       "1  s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...          0   \n",
       "2  import sys\\nsys.setrecursionlimit (1000000)\\n\\...          0   \n",
       "3  from __future__ import division, print_functio...          0   \n",
       "4  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...          0   \n",
       "\n",
       "               problem_name time_complexity_inferred  \\\n",
       "0  339_C. Xenia and Weights                     O(1)   \n",
       "1  339_C. Xenia and Weights                     None   \n",
       "2  339_C. Xenia and Weights                     O(1)   \n",
       "3  339_C. Xenia and Weights                     None   \n",
       "4  339_C. Xenia and Weights                     O(1)   \n",
       "\n",
       "  space_complexity_inferred  time_curve_coefficient  space_curve_coefficient  \n",
       "0                   O(n**2)                0.000019                 0.012433  \n",
       "1                      None                     NaN                      NaN  \n",
       "2                      O(1)                0.000004               584.000000  \n",
       "3                      None                     NaN                      NaN  \n",
       "4                      O(1)                0.000013               323.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746728455903,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "7336xkAPbXNU",
    "outputId": "35118156-2cec-4858-8995-6d0242ecf079"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293586, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRk8ieRhcRM0"
   },
   "source": [
    "Remove from df the atributes that we aren't going to use during the modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746728455904,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "l9qLS7aucP2D"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['problem_id', 'time_curve_coefficient', 'space_curve_coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1746728456411,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "JYK5acYyc5Gd",
    "outputId": "846d97ab-b5a3-4d33-9bf2-67381082254b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3  Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
       "1  s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...   \n",
       "2  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
       "3  from __future__ import division, print_functio...   \n",
       "4  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...   \n",
       "\n",
       "               problem_name time_complexity_inferred space_complexity_inferred  \n",
       "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
       "1  339_C. Xenia and Weights                     None                      None  \n",
       "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
       "3  339_C. Xenia and Weights                     None                      None  \n",
       "4  339_C. Xenia and Weights                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RmgbYXZbcaU"
   },
   "source": [
    "From the preliminary analysis we know that only the second dataset has na values. The na values are now eliminated form df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1746728456958,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "hpmLQVcFde7X",
    "outputId": "e74ffb16-4539-441d-efa8-01dc91780154"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_5</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit(100000)\\n\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_7</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit(1500)\\ndef d...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_9</td>\n",
       "      <td>s=input()\\nm=int(input())\\na=[]\\nfor j in rang...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293369</th>\n",
       "      <td>You are given an integer L. Construct a direct...</td>\n",
       "      <td>3102_83</td>\n",
       "      <td>import sys\\ninput = sys.stdin.readline\\n\\nL = ...</td>\n",
       "      <td>p03267 AtCoder Beginner Contest 108 - All Your...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293422</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_36</td>\n",
       "      <td>from itertools import accumulate\\nn, *A = map(...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293431</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_45</td>\n",
       "      <td>n, *A = map(int, open(0).read().split())\\ndef ...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293446</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_60</td>\n",
       "      <td>n = int(input())\\na = list(map(int,input().spl...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293453</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_67</td>\n",
       "      <td>_,*l=map(int,open(0).read().split())\\ndef f(s)...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48709 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      problem_description solution_id  \\\n",
       "1       Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "3       Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "5       Xenia has a set of weights and pan scales. Eac...         0_5   \n",
       "7       Xenia has a set of weights and pan scales. Eac...         0_7   \n",
       "9       Xenia has a set of weights and pan scales. Eac...         0_9   \n",
       "...                                                   ...         ...   \n",
       "293369  You are given an integer L. Construct a direct...     3102_83   \n",
       "293422  You are given an integer sequence of length N....     3103_36   \n",
       "293431  You are given an integer sequence of length N....     3103_45   \n",
       "293446  You are given an integer sequence of length N....     3103_60   \n",
       "293453  You are given an integer sequence of length N....     3103_67   \n",
       "\n",
       "                                            solution_code  \\\n",
       "1       s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...   \n",
       "3       from __future__ import division, print_functio...   \n",
       "5       import sys\\nsys.setrecursionlimit(100000)\\n\\n\\...   \n",
       "7       import sys\\nsys.setrecursionlimit(1500)\\ndef d...   \n",
       "9       s=input()\\nm=int(input())\\na=[]\\nfor j in rang...   \n",
       "...                                                   ...   \n",
       "293369  import sys\\ninput = sys.stdin.readline\\n\\nL = ...   \n",
       "293422  from itertools import accumulate\\nn, *A = map(...   \n",
       "293431  n, *A = map(int, open(0).read().split())\\ndef ...   \n",
       "293446  n = int(input())\\na = list(map(int,input().spl...   \n",
       "293453  _,*l=map(int,open(0).read().split())\\ndef f(s)...   \n",
       "\n",
       "                                             problem_name  \\\n",
       "1                                339_C. Xenia and Weights   \n",
       "3                                339_C. Xenia and Weights   \n",
       "5                                339_C. Xenia and Weights   \n",
       "7                                339_C. Xenia and Weights   \n",
       "9                                339_C. Xenia and Weights   \n",
       "...                                                   ...   \n",
       "293369  p03267 AtCoder Beginner Contest 108 - All Your...   \n",
       "293422     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "293431     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "293446     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "293453     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "\n",
       "       time_complexity_inferred space_complexity_inferred  \n",
       "1                          None                      None  \n",
       "3                          None                      None  \n",
       "5                          None                      None  \n",
       "7                          None                      None  \n",
       "9                          None                      None  \n",
       "...                         ...                       ...  \n",
       "293369                     None                      None  \n",
       "293422                     None                      None  \n",
       "293431                     None                      None  \n",
       "293446                     None                      None  \n",
       "293453                     None                      None  \n",
       "\n",
       "[48709 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many values of df are nan\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1746728456975,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "-uzFuKf5bqxx"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1746728457331,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "I16SN3XDb2Y7",
    "outputId": "d0677b4c-bdc5-4ef9-e05e-fd505266acb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244877, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the temp_df after removal of na values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPFydkG2hzRD"
   },
   "source": [
    "Rename the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1746728457351,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "Pht6z9RUh7q8"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746728457358,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "yHAsW8P4iGCO",
    "outputId": "19c2a4c8-6767-49b6-84ba-a07fbab2184f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
       "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
       "2  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...   \n",
       "\n",
       "               problem_name time_complexity_inferred space_complexity_inferred  \n",
       "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
       "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
       "2  339_C. Xenia and Weights                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf1sm6uFsjNE"
   },
   "source": [
    "### Clean the code solutions\n",
    "In the code of the solutions are present many elements that would render the training of models harder, such as comments (both inline and multiple lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CvdyoEw-dNO"
   },
   "source": [
    "Apon inspection of the code solutions, one code solution resulted not correct sintatically, so it was decided to drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746728457370,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "qbXoaeiG-qCN",
    "outputId": "1de61e61-1bb5-4c3d-db3d-a96753c86f8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A,B = map(int,input().split(\" \"))\\n\\ns1 = \"####################################################################################################\\\\n\\\\\\n.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#\\\\n\"\\ns2 = \"....................................................................................................\\\\n\\\\\\n#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.\\\\n\"\\nblack = \\'#\\' * 100 + \\'\\\\n\\'\\nwhite = \\'.\\' * 100 + \\'\\\\n\\'\\n\\ncnt = (A-1) // 50\\nmer = (A - 1) % 50\\nif mer == 0:\\n\\tup = s1 * cnt + black\\nelse:\\n\\tup = s1 * cnt + black + (\\'.#\\' * mer) + (\\'#\\' * (100 - 2*mer)) + \\'\\\\n\\' + black\\n\\ncnt = (B-1) // 50\\nmer = (B - 1) % 50\\n\\nif mer == 0:\\n\\tdown = s2 * cnt + white\\nelse:\\n\\tdown = s2 * cnt + white + (\\'#.\\' * mer) + (\\'.\\' * (100 - 2*mer)) + \\'\\\\n\\' + white\\n\\nprint(str(len((up+down)) // 100) + \" 100\")\\nprint(up + down)\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.solution_code[114411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1746728457898,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "0EL4n2sy-woN"
   },
   "outputs": [],
   "source": [
    "df = df.drop(df.index[114411], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746728457910,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "JBG_3KJw_2Kd",
    "outputId": "5787ecff-a13c-4560-b6b2-4d6a420ee781"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114409</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_0</td>\n",
       "      <td>\\nimport sys\\nsys.setrecursionlimit(1 &lt;&lt; 25)\\n...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114410</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_1</td>\n",
       "      <td>from math import ceil\\n\\nA, B = map(int, input...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(n+m)</td>\n",
       "      <td>O(n+m)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114412</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_3</td>\n",
       "      <td>a, b = map(int, input().split())\\na -= 1\\nb -=...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(nlogn)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114413</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_4</td>\n",
       "      <td>ab = list(map(int, input().split()))\\n\\ng = [[...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      problem_description solution_id  \\\n",
       "114409  You are given two integers A and B.\\n\\nPrint a...      1447_0   \n",
       "114410  You are given two integers A and B.\\n\\nPrint a...      1447_1   \n",
       "114412  You are given two integers A and B.\\n\\nPrint a...      1447_3   \n",
       "114413  You are given two integers A and B.\\n\\nPrint a...      1447_4   \n",
       "\n",
       "                                            solution_code  \\\n",
       "114409  \\nimport sys\\nsys.setrecursionlimit(1 << 25)\\n...   \n",
       "114410  from math import ceil\\n\\nA, B = map(int, input...   \n",
       "114412  a, b = map(int, input().split())\\na -= 1\\nb -=...   \n",
       "114413  ab = list(map(int, input().split()))\\n\\ng = [[...   \n",
       "\n",
       "                                             problem_name  \\\n",
       "114409  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "114410  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "114412  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "114413  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "\n",
       "       time_complexity_inferred space_complexity_inferred  \n",
       "114409                     O(1)                      O(n)  \n",
       "114410                   O(n+m)                    O(n+m)  \n",
       "114412                 O(nlogn)                   O(n**2)  \n",
       "114413                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[114409:114413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746728457916,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "EDbKGIGh_FqQ"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1746728458627,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "KbGimJKf_Oa9",
    "outputId": "133b239f-3a26-403c-b18b-4e5079a41e4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244876, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1AyCnxNtLhe"
   },
   "source": [
    "Visualize inline comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1746728461656,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "sBRj8TvBs_il",
    "outputId": "ae81e075-b41a-4389-84f2-8308a9713a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'\\n#\\n': 2906, ' #!/usr/bin/env python3\\n': 1645, ' # -*- coding: utf-8 -*-\\n': 1553, '\\n# region fastio\\n': 1209, ' #\\n': 1012, ' #!/usr/bin/env python\\n': 693, '\\n# endregion\\n': 683, '\\n#     else:\\n': 545, ' # cook your dish here\\n': 540, '\\n# ------------------------------\\n': 436, ...})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "inline_comment_regex = '([^\\\"|^\\']\\#[^\\'|^\\\"|^\\n]*\\n{1})'\n",
    "concatenated_code = ' '.join(df.solution_code)\n",
    "inline_comments_in_code = re.findall(inline_comment_regex, concatenated_code)\n",
    "counts = nltk.FreqDist(inline_comments_in_code)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11647,
     "status": "ok",
     "timestamp": 1746728473304,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "5y_9rYhJv-1C",
    "outputId": "7ed47d4f-8512-4ce4-cfb2-01aab46ce857"
   },
   "outputs": [],
   "source": [
    "for solution in df.solution_code:\n",
    "    comment_code = re.findall(inline_comment_regex, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746728473311,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "XoQO6ylU3TiN",
    "outputId": "c7c3c976-7248-49ad-9879-31bc3193e3c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n = int(input())\\ncont = 0\\nmatriz = []\\nfor i in range(n):\\n    linha = list(input())\\n    matriz.append(linha)\\n    for i in range(n):\\n        if linha[i] == \\'#\\':\\n            cont+=1\\nif cont%5 != 0:\\n    print(\"NO\")\\nelse:\\n    for i in range(0,n-1):\\n        for j in range(1,n):\\n            if matriz[i][j] == \\'#\\' and i < n-2 and j < n-1: # topo da cruz\\n                if matriz[i+1][j] != \\'#\\' or matriz[i+2][j] != \\'#\\' or matriz[i+1][j-1] != \\'#\\' or matriz[i+1][j+1]!= \\'#\\':\\n                    continue\\n    \\n                else:\\n                    matriz[i][j] = \\'.\\'\\n                    matriz[i+1][j] = \\'.\\'\\n                    matriz[i+2][j] = \\'.\\'\\n                    matriz[i+1][j-1] = \\'.\\'\\n                    matriz[i+1][j+1] = \\'.\\'\\n                    cont -= 5\\n    if cont == 0:\\n        print(\"YES\")\\n    else:\\n        print(\"NO\")\\n            \\n \\t   \\t\\t\\t  \\t\\t \\t \\t \\t \\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print an example of code solution that contains inline comments\n",
    "df.solution_code[65669]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtQIGhQbvJRb"
   },
   "source": [
    "Remove inline comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1746728473674,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "qnBXi_KKvMiO"
   },
   "outputs": [],
   "source": [
    "for solution in df.solution_code:\n",
    "    solution = re.sub(inline_comment_regex, '\\n', solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746728473682,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "3s3zMH5svjaT",
    "outputId": "3289d7ab-54b4-45ce-ca00-3609c77479ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n = int(input())\\ncont = 0\\nmatriz = []\\nfor i in range(n):\\n    linha = list(input())\\n    matriz.append(linha)\\n    for i in range(n):\\n        if linha[i] == \\'#\\':\\n            cont+=1\\nif cont%5 != 0:\\n    print(\"NO\")\\nelse:\\n    for i in range(0,n-1):\\n        for j in range(1,n):\\n            if matriz[i][j] == \\'#\\' and i < n-2 and j < n-1: # topo da cruz\\n                if matriz[i+1][j] != \\'#\\' or matriz[i+2][j] != \\'#\\' or matriz[i+1][j-1] != \\'#\\' or matriz[i+1][j+1]!= \\'#\\':\\n                    continue\\n    \\n                else:\\n                    matriz[i][j] = \\'.\\'\\n                    matriz[i+1][j] = \\'.\\'\\n                    matriz[i+2][j] = \\'.\\'\\n                    matriz[i+1][j-1] = \\'.\\'\\n                    matriz[i+1][j+1] = \\'.\\'\\n                    cont -= 5\\n    if cont == 0:\\n        print(\"YES\")\\n    else:\\n        print(\"NO\")\\n            \\n \\t   \\t\\t\\t  \\t\\t \\t \\t \\t \\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the solution code now without inline comments\n",
    "df.solution_code[65669]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoew1xSyDPmc"
   },
   "source": [
    "Consider now multi-line comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1746728474371,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "286JCDc5DkTZ",
    "outputId": "85003e66-6ee8-4ef1-e405-b9f514a2bd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_comment_regex = '([\\'\\\"])\\1\\1[\\d\\D]*?\\1{3}'\n",
    "concatenated_code = ' '.join(df.solution_code)\n",
    "multi_comments_in_code = re.findall(multi_comment_regex, concatenated_code)\n",
    "counts = nltk.FreqDist(multi_comments_in_code)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746728474396,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "sUOOJeqkDkTc"
   },
   "outputs": [],
   "source": [
    "for solution in df.solution_code:\n",
    "    comment_code = re.findall(multi_comment_regex, solution)\n",
    "    if comment_code:\n",
    "        print(df.loc[df['solution_code'] == solution].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AcTklgyF9Eg"
   },
   "source": [
    "There are no multiline comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6te2LxHiGgLW"
   },
   "source": [
    "### Save final Dataset in storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = path_to_project + '/final_ds.csv' if IN_COLAB else './final_ds.csv'\n",
    "df.to_csv(saving_path, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for Text Generation, Training and Fine-Tuning\n",
    "In this section we are going to train and evaluate Transformers models to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Goodle Drive (if needed), add testing variables to the environment, and upload the dataset from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_project + '/final_ds.csv' if IN_COLAB else './final_ds.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c0cfe19d-52f1-429d-a535-4edaf277c72d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td>import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0cfe19d-52f1-429d-a535-4edaf277c72d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c0cfe19d-52f1-429d-a535-4edaf277c72d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c0cfe19d-52f1-429d-a535-4edaf277c72d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-afe982b2-bb0b-45f9-a6b5-03823a606083\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afe982b2-bb0b-45f9-a6b5-03823a606083')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-afe982b2-bb0b-45f9-a6b5-03823a606083 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
       "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
       "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "               problem_name time_complexity_inferred space_complexity_inferred  \n",
       "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
       "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
       "2  339_C. Xenia and Weights                     O(1)                      O(1)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train T5-Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developers of the Text-To-Text Transfer Transformer (T5) write:\n",
    "\n",
    "'With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.'\n",
    "\n",
    "The model has 223M parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries that are going to be used in the following subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Model T5-Base only 8000 samples are kept from the dataset due to the limitation of the resources availabe to us. We decided to keep the training (and fine-tuning) time of the models at approximately 2 hours for this section, and the amount of samples allowed us to maintain this contraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## only in initial part: reduce ds to allow faster testing of the code\n",
    "df = df.head(8000).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data into training dataset, validation dataset and test dataset. We decided to use 10% of the dataset for tesing, and the remaining for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1)\n",
    "train, val = train_test_split(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train instances:  5760\n",
      "# test instances:   800\n",
      "# val instances:    1440\n"
     ]
    }
   ],
   "source": [
    "print('# train instances: ', train.shape[0])\n",
    "print('# test instances:  ', test.shape[0])\n",
    "print('# val instances:   ', val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google-t5/t5-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f3852a115a496bbb4cfddba48272e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76da375e3df04483a63e1cbe95fa551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6658b732794510bcd7fbdc762adc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some useful information on the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  32000\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â–big', 600),\n",
       " ('â–God', 601),\n",
       " ('â–dass', 602),\n",
       " ('im', 603),\n",
       " ('â–30', 604),\n",
       " ('â–event', 605),\n",
       " ('â–development', 606),\n",
       " ('â–form', 607),\n",
       " ('â–read', 608),\n",
       " ('â–hand', 609)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer.get_vocab().items())[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–You', 'â–have', 'â–an', 'â–array', 'â–in', 'â–input', ',', 'â–order', 'â–the', 'â–elements', 'â–in', 'â–it', 'â–in', 'â–O', '(', 'n', ')', 'â–time', 'â–complexity', '.', 'â–Add', 'â–', 'a', 'â–wrong', 'â–word', 'd']\n"
     ]
    }
   ],
   "source": [
    "text = \"You have an array in input, order the elements in it in O(n) time complexity. Add a wrong wordd\"\n",
    "encoded_input = tokenizer._tokenize(text)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148, 43, 46, 5590, 16, 3785, 6, 455, 8, 2479, 16, 34, 16, 411, 599, 29, 61, 97, 11641, 5, 2334, 3, 9, 1786, 1448, 26, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = tokenizer(text)['input_ids']\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is uploaded and connected to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d287ceaf052947a58fff893483a947f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad9095acdb14d4cb6d5414d658447db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
    "t5.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32100, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222882048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_params = sum(param.numel() for param in t5.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: shared.weight\n",
      "Parameter shape: torch.Size([32100, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter shape: torch.Size([32, 12])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.final_layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter shape: torch.Size([32, 12])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.final_layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in t5.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.size()}\")\n",
    "    print(f\"Is trainable: {param.requires_grad}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both Tokenizer and Model we can tokenize the dataset and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a single string for each conversation of problem description and solution, where the user input and the system response are separated by eos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eos_token(idx, df, eos_token):\n",
    "    # build the user input including the problem description, the time complexity and the space complexity\n",
    "    chat_string = 'User:' + df.loc[idx, 'problem_description'] + ' Time complexity: ' + df.loc[idx, 'time_complexity_inferred'] + '; Space complexity: ' + df.loc[idx, 'space_complexity_inferred']\n",
    "    # now add the eos token and the response from the assistant, the code solution for the problem\n",
    "    chat_string = chat_string + eos_token + 'Assistant: ' + df.loc[idx, 'solution_code'] + eos_token\n",
    "    return chat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = [apply_eos_token(idx, train, tokenizer.eos_token) for idx in train.index]\n",
    "test_str = [apply_eos_token(idx, test, tokenizer.eos_token) for idx in test.index]\n",
    "val_str = [apply_eos_token(idx, val, tokenizer.eos_token) for idx in val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'User:It is known that the area of a regular dodecagon inscribed in a circle of radius a is 3a^2.\\n\\nGiven an integer r, find the area of a regular dodecagon inscribed in a circle of radius r.\\n\\nConstraints\\n\\n* 1 \\\\leq r \\\\leq 100\\n* r is an integer.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nr\\n\\n\\nOutput\\n\\nPrint an integer representing the area of the regular dodecagon.\\n\\nExamples\\n\\nInput\\n\\n4\\n\\n\\nOutput\\n\\n48\\n\\n\\nInput\\n\\n15\\n\\n\\nOutput\\n\\n675\\n\\n\\nInput\\n\\n80\\n\\n\\nOutput\\n\\n19200 Time complexity: O(1); Space complexity: O(1)</s>Assistant: N = int(input())\\n\\nprint(3*(N**2))</s>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings where the eos token was applied, are now put in a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'chat': train_str})\n",
    "test_data = Dataset.from_dict({'chat': test_str})\n",
    "val_data = Dataset.from_dict({'chat': val_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['val'] = val_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6361bf11624c1c9932038818e8297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8809b40f9154d6a87ca645c1c6b004a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e8dc975dcd495f88121e62c44fed39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples[\"chat\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids,\n",
    "        'attention_mask': input_encodings.attention_mask,\n",
    "        'labels': input_encodings.input_ids.copy()\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sequences in same batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"t5_trainer\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-4,       # t5 needs a higher lr than other models\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,\n",
    "    report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= t5,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_data['train'],\n",
    "    eval_dataset= tokenized_data['val'],\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time taken by the model to finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 2:08:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.07308270597347506, metrics={'train_runtime': 7709.3416, 'train_samples_per_second': 2.241, 'train_steps_per_second': 0.14, 'total_flos': 1.05227923488768e+16, 'train_loss': 0.07308270597347506, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  7709.833685159683\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Training time: \", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model has 257 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "shared.weight                                           (32100, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight            (768, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight            (768, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight            (768, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight            (768, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight     (32, 12)\n",
      "encoder.block.0.layer.0.layer_norm.weight                     (768,)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight         (3072, 768)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight         (768, 3072)\n",
      "encoder.block.0.layer.1.layer_norm.weight                     (768,)\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight            (768, 768)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight            (768, 768)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight            (768, 768)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "decoder.block.11.layer.2.layer_norm.weight                    (768,)\n",
      "decoder.final_layer_norm.weight                               (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(t5.named_parameters())\n",
    "\n",
    "print('The T5 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/t5_train_2025_05_18_12_35_46'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "t5_training_path = path_to_project + '/Transformer-trained-models/' + f\"t5_train_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer.save_pretrained(t5_training_path)\n",
    "t5.save_pretrained(t5_training_path)\n",
    "print(f\"Checkpoint saved at: \\'{t5_training_path}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "#t5_training_path = '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/t5_train_2025_05_18_12_35_46'\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_training_path)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(t5_training_path, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we first extract one conversation from the test data randomly, give it as input to the model, and we see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "User: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research.\n",
      "\n",
      "First of all, Vlad assigned individual indices to all cafes. Then, he wrote down indices of cafes he visited in a row, in order of visiting them. Now, Vlad wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that he hasn't been there for as long as possible. Help Vlad to find that cafe.\n",
      "\n",
      "Input\n",
      "\n",
      "In first line there is one integer n (1 â‰¤ n â‰¤ 2Â·105) â€” number of cafes indices written by Vlad.\n",
      "\n",
      "In second line, n numbers a1, a2, ..., an (0 â‰¤ ai â‰¤ 2Â·105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted.\n",
      "\n",
      "Output\n",
      "\n",
      "Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "5\n",
      "1 3 2 1 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "6\n",
      "2 1 2 2 4 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "2\n",
      "\n",
      "Note\n",
      "\n",
      "In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. \n",
      "\n",
      "In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some numbers while numerating the cafes. Time complexity: O(n); Space complexity: O(n)</s>Assistant: from collections import defaultdict\n",
      "n = int(input())\n",
      "a = list(map(int,input().split()))\n",
      "\n",
      "c = defaultdict(lambda: 0)\n",
      "\n",
      "for i in range(len(a)):\n",
      "    c[a[i]] = i\n",
      "\n",
      "maxi = 0\n",
      "k = len(a)\n",
      "ne = 0\n",
      "for key in c:\n",
      "    if(k - c[key] > maxi):\n",
      "        ne = key\n",
      "    maxi = max(maxi, k - c[key])\n",
      "\n",
      "print(ne)</s>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      " User: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research.\n",
      "\n",
      "First of all, Vlad assigned individual indices to all cafes. Then, he wrote down indices of cafes he visited in a row, in order of visiting them. Now, Vlad wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that he hasn't been there for as long as possible. Help Vlad to find that cafe.\n",
      "\n",
      "Input\n",
      "\n",
      "In first line there is one integer n (1 â‰¤ n â‰¤ 2Â·105) â€” number of cafes indices written by Vlad.\n",
      "\n",
      "In second line, n numbers a1, a2, ..., an (0 â‰¤ ai â‰¤ 2Â·105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted.\n",
      "\n",
      "Output\n",
      "\n",
      "Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "5\n",
      "1 3 2 1 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "6\n",
      "2 1 2 2 4 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "2\n",
      "\n",
      "Note\n",
      "\n",
      "In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. \n",
      "\n",
      "In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some numbers while numerating the cafes. Time complexity: O(n); Space complexity: O(n)</s> Assistant: \n",
      "User input lenght:  1601\n",
      "####\n",
      "Correct solution output: \n",
      " Assistant: from collections import defaultdict\n",
      "n = int(input())\n",
      "a = list(map(int,input().split()))\n",
      "\n",
      "c = defaultdict(lambda: 0)\n",
      "\n",
      "for i in range(len(a)):\n",
      "    c[a[i]] = i\n",
      "\n",
      "maxi = 0\n",
      "k = len(a)\n",
      "ne = 0\n",
      "for key in c:\n",
      "    if(k - c[key] > maxi):\n",
      "        ne = key\n",
      "    maxi = max(maxi, k - c[key])\n",
      "\n",
      "print(ne)</s>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, en = dialogue.split('</s>')\n",
    "\n",
    "# add eos token at the end of test_input and test_output\n",
    "test_input = test_input + tokenizer.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer.eos_token\n",
    "\n",
    "print('User input: \\n', test_input)\n",
    "print('User input lenght: ', len(test_input))\n",
    "print('####')\n",
    "print('Correct solution output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> User: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research. First of all, Vlad assigned individual indices to all cafes. Then, he wrote down indices of cafes he visited in a row, in order of visiting them. Now, Vlad wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that he hasn't been there for as long as possible. Help Vlad to find that cafe. Input In first line there is one integer n (1 <unk> n <unk> 2<unk> 105) â€” number of cafes indices written by Vlad. In second line, n numbers a1, a2, ..., an (0 <unk> ai <unk> 2<unk> 105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted. Output Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible. Examples Input 5 1 3 2 1 2 Output 3 Input 6 2 1 2 2 4 1 Output 2 Note In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some numbers while numerating the cafes. Time complexity: O(n); Space complexity: O(n) Assistant: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research. First of all, Vlad assigned individual indices to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. Input In first line there is one integer n (1 <unk> n <unk> 2<unk> 105) â€” number of cafes indices written by Vlad. In second line, n numbers a1, a2, ..., an (0 <unk> ai <unk> 2<unk>105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted. Output Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible. Examples Input 5 1 3 2 1 2 Output 3 Input 6 2 1 2 2 4 1 Output 2 Note In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some\n",
      " Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research. First of all, Vlad assigned individual indices to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. Input In first line there is one integer n (1 <unk> n <unk> 2<unk> 105) â€” number of cafes indices written by Vlad. In second line, n numbers a1, a2, ..., an (0 <unk> ai <unk> 2<unk>105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted. Output Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible. Examples Input 5 1 3 2 1 2 Output 3 Input 6 2 1 2 2 4 1 Output 2 Note In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(test_input, return_tensors=\"pt\", max_length = 2000).to(device)\n",
    "output = t5.generate(**input_ids, max_new_tokens=800)\n",
    "gen_text = tokenizer.decode(output[0])\n",
    "\n",
    "print(gen_text)\n",
    "print(gen_text.split('Assistant:')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not perform very well and struggles to produce code in output and to maintain the conversation format given to it. What it tries to do is to describe the solution process without explicitly generating code. \n",
    "\n",
    "We expected this type of reults, the T5 model has very good performance on translation and sumarization, while it wasn't trained as a chatbot to produce code initially. Moreover, due to our limited resources, 8000 samples are too little to train such a big model (223M parameters) in an efective way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to consider the metrics Perplexity, BLEU and F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the first three samples of the test set due to limitations in the RAM available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('</s>')[0] + tokenizer.eos_token + 'Assistant: ' for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('</s>')[1] + tokenizer.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lmppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4056: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23387.28944204467, 1260.3922689555861, 22569.483405842497]\n",
      "average perplexity: 15739.055038947583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lmppl\n",
    "\n",
    "scorer = lmppl.EncoderDecoderLM(t5_training_path)\n",
    "\n",
    "ppl = scorer.get_perplexity(input_texts= test_input, output_texts=test_output)\n",
    "print(list( ppl))\n",
    "print(f\"average perplexity: {sum(ppl)/len(ppl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user input: \n",
      "User: Gleb ordered pizza home. When the courier delivered the pizza, he was very upset, because several pieces of sausage lay on the crust, and he does not really like the crust.\n",
      "\n",
      "The pizza is a circle of radius r and center at the origin. Pizza consists of the main part â€” circle of radius r - d with center at the origin, and crust around the main part of the width d. Pieces of sausage are also circles. The radius of the i -th piece of the sausage is ri, and the center is given as a pair (xi, yi).\n",
      "\n",
      "Gleb asks you to help determine the number of pieces of sausage caught on the crust. A piece of sausage got on the crust, if it completely lies on the crust.\n",
      "\n",
      "Input\n",
      "\n",
      "First string contains two integer numbers r and d (0 â‰¤ d < r â‰¤ 500) â€” the radius of pizza and the width of crust.\n",
      "\n",
      "Next line contains one integer number n â€” the number of pieces of sausage (1 â‰¤ n â‰¤ 105).\n",
      "\n",
      "Each of next n lines contains three integer numbers xi, yi and ri ( - 500 â‰¤ xi, yi â‰¤ 500, 0 â‰¤ ri â‰¤ 500), where xi and yi are coordinates of the center of i-th peace of sausage, ri â€” radius of i-th peace of sausage.\n",
      "\n",
      "Output\n",
      "\n",
      "Output the number of pieces of sausage that lay on the crust.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "8 4\n",
      "7\n",
      "7 8 1\n",
      "-7 3 2\n",
      "0 2 1\n",
      "0 -2 2\n",
      "-3 -3 1\n",
      "0 6 2\n",
      "5 3 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "10 8\n",
      "4\n",
      "0 0 9\n",
      "0 0 10\n",
      "1 0 1\n",
      "1 0 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "0\n",
      "\n",
      "Note\n",
      "\n",
      "Below is a picture explaining the first example. Circles of green color denote pieces of sausage lying on the crust.\n",
      "\n",
      "<image> Time complexity: O(n**2); Space complexity: O(1)</s>Assistant: \n",
      "#################\n",
      "prediction: \n",
      "Assistant: import math\n",
      "from sys import stdin, stdout\n",
      "def main():\n",
      "    x = stdin.readline().split()\n",
      "    r = int(x[0])\n",
      "    d = int(x[1])\n",
      "    r -= d\n",
      "    n = int(stdin.readline())\n",
      "    ans = 0\n",
      "    for i in range(n):\n",
      "        x = stdin.readline().split()\n",
      "        a = int(x[0])\n",
      "        b = int(x[1])\n",
      "        ri = int(x[2])\n",
      "        D = math.sqrt((a*a) + (b*b))\n",
      "        if D-ri >= r and d >= 2*ri and D+ri <= r+d:\n",
      "            ans += 1\n",
      "    print(ans)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()</s>\n"
     ]
    }
   ],
   "source": [
    "print(f\"user input: \\n{test_input[ppl.index(min(ppl))]}\")\n",
    "print('#################')\n",
    "print(f\"prediction: \\n{test_output[ppl.index(min(ppl))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**\n",
    "\n",
    "BLEU focuses on precision by counting matching n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 3.804e-09\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import BleuMetric\n",
    "\n",
    "input_ids = tokenizer(test_input[0], return_tensors=\"pt\", max_length = 2000).to(device)\n",
    "output = t5.generate(**input_ids, max_new_tokens=800)\n",
    "gen_text = tokenizer.decode(output[0])\n",
    "\n",
    "bleu = BleuMetric.compute(gen_text, [test_output[0]])\n",
    "print(f\"BLEU: {bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bleu metric has a very low value, indicating very little overlap between the generated text and the solution of the test sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.03226\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import F1Metric\n",
    "\n",
    "f1_score = F1Metric.compute(gen_text, [test_output[0]])\n",
    "print(f\"F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning T5-small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discaimer:** We decided to not include the whole code for this section: the code was very similar to the one used to train the T5-base model and, given the already very long notebook and the not-excellent performance after the fine-tuning, we considered best to not include it.\n",
    "\n",
    "The T5-small model has 60M parameters, and is part of the family of the T5 text-to-text models. It was trained on Common Crawl.\n",
    "For the fine-tuning of T5-small we decided to keep 5000 samples. The model is relatively small, with only 60M parameters and already trained: a higher amount of samples led to worse performance because it caused the training to overwrite parameters unecessarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer\n",
    "We include the model specifics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google-t5/t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ac14146d2642fba5c2e5cb2a815caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c81648f36f44b088194190ddf8e428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593dc97b085d415e82f8695676687a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b646753724640f7bc734376016da4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbff42bad7d14a1482bace3887551b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561f0690a14c4ef1b13965c76ca1d97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
    "t5.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32100, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity calculated before the fine-tuning was: [66.54613901709006, 105.79112153990572, 35432.329456935135] with\n",
    "average perplexity: 11868.222239164044.\n",
    "\n",
    "The training lasted 1314.5914108753204 seconds.\n",
    "\n",
    "We then tested the text generation of the fine-tuned model on a random sample from the test set: also in this case, the model does not perform well. It doesn't generate the code, mainly copies the input, and struggles to keep the separation between 'User' and 'Assistant'. Overall the generated text from T5-small is less clear than the text generated by the training of T5-base: the bigger amount of parameters of T5-base guarantees better text quality.\n",
    "\n",
    "We considered the evaluation metrics Perplexity, Bleu and F1.\n",
    "\n",
    "**Perplexity:** it was calculated considering three samples form the test set, the results are: [439.4691293912123, 720.7729464902942, 1981.2242516858746] with average perplexity: 1047.1554425224604. We can see that the fine-tuning did not increase considerably the performance of the model. The average of the perplexity decreased though, indicating a better understanding of the new functionality with respect to the model prior the fine-tuning.\n",
    "\n",
    "**Bleu:** 7.417e-06\n",
    "\n",
    "**F1-score:** 0.0744\n",
    "\n",
    "Also Bleu and F1 scores are extremely low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning gpt2-small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
    "\n",
    "Moreover, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n",
    "\n",
    "This is the smallest version of GPT-2, with 124M parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Goodle Drive and upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_project + '/final_ds.csv' if IN_COLAB else '/final_ds.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries that are going to be used in the following subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fine-tuning of gpt2-small we decided to keep 15000 samples. The training lasts approximately 2 hours, and, given the size and the pre-training of the model, gpt2 had better performance with a bigger amount of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(15000).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data into training dataset, validation dataset and test dataset. We decided to use 10% of the dataset for tesing, and the remaining for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1)\n",
    "train, val = train_test_split(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train instances:  10800\n",
      "# test instances:   1500\n",
      "# val instances:    2700\n"
     ]
    }
   ],
   "source": [
    "print('# train instances: ', train.shape[0])\n",
    "print('# test instances:  ', test.shape[0])\n",
    "print('# val instances:   ', val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  50257\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", tokenizer_gpt2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('int', 600),\n",
       " ('ress', 601),\n",
       " ('ations', 602),\n",
       " ('ail', 603),\n",
       " ('Ä 4', 604),\n",
       " ('ical', 605),\n",
       " ('Ä them', 606),\n",
       " ('Ä her', 607),\n",
       " ('ount', 608),\n",
       " ('Ä Ch', 609)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer_gpt2.get_vocab().items())[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'Ä have', 'Ä an', 'Ä array', 'Ä in', 'Ä input', ',', 'Ä order', 'Ä the', 'Ä elements', 'Ä in', 'Ä it', 'Ä in', 'Ä O', '(', 'n', ')', 'Ä time', 'Ä complexity', '.', 'Ä Add', 'Ä a', 'Ä wrong', 'Ä word', 'd']\n"
     ]
    }
   ],
   "source": [
    "text = \"You have an array in input, order the elements in it in O(n) time complexity. Add a wrong wordd\"\n",
    "encoded_input = tokenizer_gpt2._tokenize(text)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1639, 423, 281, 7177, 287, 5128, 11, 1502, 262, 4847, 287, 340, 287, 440, 7, 77, 8, 640, 13357, 13, 3060, 257, 2642, 1573, 67]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = tokenizer_gpt2(text)['input_ids']\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to use GPT2LMHeadModel instead of GPT2MOdel, beacuse the latter is not compatible with the Trainer due to a mismatch of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(model_name, device_map=device)\n",
    "gpt2.resize_token_embeddings(len(tokenizer_gpt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_params = sum(param.numel() for param in gpt2.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: transformer.wte.weight\n",
      "Parameter shape: torch.Size([50257, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.wpe.weight\n",
      "Parameter shape: torch.Size([1024, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in gpt2.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.size()}\")\n",
    "    print(f\"Is trainable: {param.requires_grad}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both Tokenizer and Model we can tokenize the dataset and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eos tokens have to be added by hand to obtain a single string for each 'conversation'. In a single string we include the user input, that includes the description of the problem, and the time and space complexities, and the espected answer from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eos_token(idx, df, eos_token):\n",
    "    # build the user input including the problem description, the time complexity and the space complexity\n",
    "    chat_string = 'User:' + df.loc[idx, 'problem_description'] + ' Time complexity: ' + df.loc[idx, 'time_complexity_inferred'] + '; Space complexity: ' + df.loc[idx, 'space_complexity_inferred']\n",
    "    # now add the eos token and the response from the assistant, the code solution for the problem\n",
    "    chat_string = chat_string + eos_token + 'Assistant: ' + df.loc[idx, 'solution_code'] + eos_token\n",
    "    return chat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = [apply_eos_token(idx, train, tokenizer_gpt2.eos_token) for idx in train.index]\n",
    "test_str = [apply_eos_token(idx, test, tokenizer_gpt2.eos_token) for idx in test.index]\n",
    "val_str = [apply_eos_token(idx, val, tokenizer_gpt2.eos_token) for idx in val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"User: Naruto has sneaked into the Orochimaru's lair and is now looking for Sasuke. There are T rooms there. Every room has a door into it, each door can be described by the number n of seals on it and their integer energies a_1, a_2, ..., a_n. All energies a_i are nonzero and do not exceed 100 by absolute value. Also, n is even.\\n\\nIn order to open a door, Naruto must find such n seals with integer energies b_1, b_2, ..., b_n that the following equality holds: a_{1} â‹… b_{1} + a_{2} â‹… b_{2} + ... + a_{n} â‹… b_{n} = 0. All b_i must be nonzero as well as a_i are, and also must not exceed 100 by absolute value. Please find required seals for every room there.\\n\\nInput\\n\\nThe first line contains the only integer T (1 â‰¤ T â‰¤ 1000) standing for the number of rooms in the Orochimaru's lair. The other lines contain descriptions of the doors.\\n\\nEach description starts with the line containing the only even integer n (2 â‰¤ n â‰¤ 100) denoting the number of seals.\\n\\nThe following line contains the space separated sequence of nonzero integers a_1, a_2, ..., a_n (|a_{i}| â‰¤ 100, a_{i} â‰  0) denoting the energies of seals.\\n\\nOutput\\n\\nFor each door print a space separated sequence of nonzero integers b_1, b_2, ..., b_n (|b_{i}| â‰¤ 100, b_{i} â‰  0) denoting the seals that can open the door. If there are multiple valid answers, print any. It can be proven that at least one answer always exists.\\n\\nExample\\n\\nInput\\n\\n\\n2\\n2\\n1 100\\n4\\n1 2 3 6\\n\\n\\nOutput\\n\\n\\n-100 1\\n1 1 1 -1\\n\\nNote\\n\\nFor the first door Naruto can use energies [-100, 1]. The required equality does indeed hold: 1 â‹… (-100) + 100 â‹… 1 = 0.\\n\\nFor the second door Naruto can use, for example, energies [1, 1, 1, -1]. The required equality also holds: 1 â‹… 1 + 2 â‹… 1 + 3 â‹… 1 + 6 â‹… (-1) = 0. Time complexity: O(n*m); Space complexity: O(n*m)<|endoftext|>Assistant: a = int(input())\\nfor i in range(a):\\n    n = int(input())\\n    f = list(map(int, input().split()))\\n    ans = []\\n    for i in range(0, n, 2):\\n        ans.append(-1 * f[i + 1])\\n        ans.append(f[i])\\n    print(*ans)<|endoftext|>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings where the eos token was applied, are now put in a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'chat': train_str})\n",
    "test_data = Dataset.from_dict({'chat': test_str})\n",
    "val_data = Dataset.from_dict({'chat': val_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['val'] = val_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c52e170969a44158db480a827a0d495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d2f7a6ffdb427aaa79275b497db212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f439e6d7985d4d3aa00fe5fcb5f80f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer_gpt2(examples[\"chat\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids,\n",
    "        'attention_mask': input_encodings.attention_mask,\n",
    "        'labels': input_encodings.input_ids.copy()\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sequences in same batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer_gpt2, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity before fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random conversation from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|>Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input:  User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant: \n",
      "####\n",
      "Correct solution output:  Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "test_input = test_input + tokenizer_gpt2.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: ', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: ', test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use pipeline from the transformers library to generate the output of the model, indicating the task 'text-generation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipeline transformers evaluate\n",
    "from transformers import pipeline\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant: __________________\n",
      "\n",
      "\n",
      "\"Don't ever say it aloud or ask it aloud before you start saying it, you might end up being a fool! It is not a good look for a fool! If you have no idea what you need you can always just tell someone off.\"\n",
      "\n",
      "-John Dillinger, 1963\n",
      "\n",
      "__________________\n",
      "\n",
      "\"I know that there is one thing that every and all man must do when he sees and hears men and he takes them to be men without even touching them except for the most slight and the smallest. But in the man's world all is the same; he is the same for himself. But now he has a whole world to go to which he has no control; and therefore the man must go to his own home in the greatest number of places.\"\n",
      "\n",
      "-Frank Sinatra, A Song of Ice and Fire by Paul McCartney\n",
      "\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe(test_input, max_new_tokens=256)\n",
    "print(\"Generated text:\\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text has nothing to do with python code and includes references to pop culture, probably included in the pre-training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8cc511ba0e4cb5a3e23041a662e9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da6489352864ee3ba66ca0f0a9d92f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity before fine-tuning: {'perplexities': [28.182695388793945, 14.845183372497559, 14.715679168701172], 'mean_perplexity': np.float64(19.24785264333089)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity_before_finetuning = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    add_start_token=False,\n",
    "    model_id= 'gpt2'\n",
    ")\n",
    "print(\"Perplexity before fine-tuning:\", perplexity_before_finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning\n",
    "Now we can start the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"gpt2_trainer\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8, \n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,  \n",
    "    report_to=None,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= gpt2,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_data['train'],\n",
    "    eval_dataset= tokenized_data['val'],\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time taken by the model to finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2025' max='2025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2025/2025 2:01:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2025, training_loss=0.4775619372615108, metrics={'train_runtime': 7315.882, 'train_samples_per_second': 4.429, 'train_steps_per_second': 0.277, 'total_flos': 8465861836800000.0, 'train_loss': 0.4775619372615108, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  7316.4667019844055\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Training time: \", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPT-2 model has 148 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (50257, 768)\n",
      "transformer.wpe.weight                                   (1024, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "transformer.h.0.ln_1.weight                                   (768,)\n",
      "transformer.h.0.ln_1.bias                                     (768,)\n",
      "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
      "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias                              (768,)\n",
      "transformer.h.0.ln_2.weight                                   (768,)\n",
      "transformer.h.0.ln_2.bias                                     (768,)\n",
      "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.ln_f.weight                                       (768,)\n",
      "transformer.ln_f.bias                                         (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(gpt2.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: '/content/drive/MyDrive/UnB/NLP_Project/Transformer-trained-models/gpt2_fine_tuning_(chat+code)_2025_05_15_13_50_58'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "gpt2_finetune_path = path_to_project + '/Transformer-trained-models/' + f\"gpt2_fine_tuning_(chat+code)_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer_gpt2.save_pretrained(gpt2_finetune_path)\n",
    "gpt2.save_pretrained(gpt2_finetune_path)\n",
    "print(f\"Checkpoint saved at: \\'{gpt2_finetune_path}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "gpt2_finetune_path = '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/gpt2_fine_tuning_(chat+code)_2025_05_15_13_50_58'\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(gpt2_finetune_path)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_finetune_path, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we first extract one chat from the test data randomly, give it as input to the model, and we see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|>Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      " User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant: \n",
      "####\n",
      "Correct solution output: \n",
      " Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "# add eos token at the end of test_input and test_output\n",
    "test_input = test_input + tokenizer_gpt2.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: \\n', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipeline transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_finetuned = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after the fine-tuning: \n",
      " User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant:    n = int(input())\n",
      "    arr = list(map(int,input().split()))\n",
      "arr[0] = 1\n",
      "for i in range(n):\n",
      "    arr[i]+=1\n",
      "print(n-1)\n",
      "\n",
      "for i in range(0,n):\n",
      "    if arr[i]!=n:\n",
      "        print(2-1)\n",
      "    else:\n",
      "        print(2 - 1)\n",
      "    elif arr[i]==2 or arr[i+1]==2:\n",
      "         print(2-1)\n",
      "          print(2)\n",
      "    print(sum(arr[-1]-1-arr[-2]).append(2))\n",
      "else:\n",
      "    print(math.log(lambda x: int(input().strip('\\n') + 1,i)))\n",
      "\n",
      "print(f(length(arr),0))\n",
      "print(max_length(arr))))\n",
      "\n",
      "count = 0\n",
      "for i in range(n):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe_finetuned(test_input, max_new_tokens=256)\n",
    "print(\"Generated text after the fine-tuning: \\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be noticed, after the fine-tuning the model is able to generate python code. The generated text starts with a reference to the user input, but it able to go forward and include an answer from the 'Assistant'. The model is not yet able to generate a correct solution for the problem, but given the smaller size of gpt2-small, it was expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to consider the metrics Perplexity and BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797fa7049fb64a96aaa28f8052347743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity post fine-tuning: {'perplexities': [1.0604777336120605, 1.047598958015442, 1.0766164064407349], 'mean_perplexity': np.float64(1.0615643660227458)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    add_start_token=False,\n",
    "    model_id= gpt2_finetune_path\n",
    ")\n",
    "print(\"Perplexity post fine-tuning:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity calculated before the fine-tuning was [28.182695388793945, 14.845183372497559, 14.715679168701172] with mean_perplexity: np.float64(19.24785264333089)\n",
    "\n",
    "We can see, now, that the perplexity values have sistematically decreased, indicating that the fine-tuning  process was able to improve the performance of the model on this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**\n",
    "\n",
    "BLEU focuses on precision by counting matching n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660ef625dbc3478fba406e861e625092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3b04b12ae741c2a880a3ded62f815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8247ea07a842c9a28b6deae512e772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.12839728594651886, 'precisions': [0.3142857142857143, 0.14537444933920704, 0.09271523178807947, 0.06415929203539823], 'brevity_penalty': 1.0, 'length_ratio': 1.6666666666666667, 'translation_length': 455, 'reference_length': 273}\n"
     ]
    }
   ],
   "source": [
    "# BLEU Score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu_metric.compute(\n",
    "    predictions= [pipe_finetuned(test_input[0], max_new_tokens=256)[0]['generated_text']], \n",
    "    references= [test_output[0]]  \n",
    ")\n",
    "print(\"BLEU Score:\", bleu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bleu scores are still low overall, but are much higher than the Bleu scores for the T5 models, further indicating how the better performance among similar sized model was achieved by the gpt2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning gpt2-CodeParrot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a base GPT-2 architecture with 124M parameters. It was trained on the huggingface-course/codeparrot-ds-valid dataset, which is a small subset of the original WebText dataset used to train GPT-2, and includes python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Goodle Drive and upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_project + '/final_ds.csv' if IN_COLAB else '/final_ds.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries that are going to be used in the following subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the small size of the model and the limited data used during its training, we decided to reduce the number of samples used for the fine-tuning to avoid overwriting parameters unecessarily and degrade the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(5000).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data into training dataset, validation dataset and test dataset. We decided to use 10% of the dataset for tesing, and the remaining for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1)\n",
    "train, val = train_test_split(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train instances:  3600\n",
      "# test instances:   500\n",
      "# val instances:    900\n"
     ]
    }
   ],
   "source": [
    "print('# train instances: ', train.shape[0])\n",
    "print('# test instances:  ', test.shape[0])\n",
    "print('# val instances:   ', val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreiev the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kailasps/GPT2-codeparrot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12718fbcdc654652895919e860807dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1087eb2ffb48548b32667d2650db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/789k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a785399f3614ffbb250c866ee956712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/448k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5930f72657487db293be04e2c0b33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc4f11775a7489ba5864f105da06db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", tokenizer_gpt2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ä Sources', 46176),\n",
       " ('Ä cac', 33927),\n",
       " ('Ä segm', 31074),\n",
       " ('electric', 30949),\n",
       " ('Ä untagged', 43874),\n",
       " ('ellipse', 22936),\n",
       " ('Ä Subscription', 26798),\n",
       " ('Ä looping', 24598),\n",
       " ('`', 64),\n",
       " ('HierarchySession', 26069)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer_gpt2.get_vocab().items())[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'Ä have', 'Ä an', 'Ä array', 'Ä in', 'Ä input', ',', 'Ä order', 'Ä the', 'Ä elements', 'Ä in', 'Ä it', 'Ä in', 'Ä O', '(', 'n', ')', 'Ä time', 'Ä complexity', '.', 'Ä Add', 'Ä a', 'Ä wrong', 'Ä word', 'd']\n"
     ]
    }
   ],
   "source": [
    "text = \"You have an array in input, order the elements in it in O(n) time complexity. Add a wrong wordd\"\n",
    "encoded_input = tokenizer_gpt2.tokenize(text)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6147, 1054, 309, 960, 253, 935, 12, 1444, 256, 2347, 253, 577, 253, 690, 8, 78, 9, 626, 16989, 14, 1822, 231, 6401, 1975, 68]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = tokenizer_gpt2(text)['input_ids']\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is uploaded and connected to the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb0b85e73c4faf8f038fa5ed04e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/898 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbfe5fed7f6443baae62f5c80d47151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/497M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c351cea2915349d2995af5b77369bb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50000, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "gpt2.resize_token_embeddings(len(tokenizer_gpt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50000, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124242432"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_params = sum(param.numel() for param in gpt2.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: transformer.wte.weight\n",
      "Parameter shape: torch.Size([50000, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.wpe.weight\n",
      "Parameter shape: torch.Size([1024, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in gpt2.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.size()}\")\n",
    "    print(f\"Is trainable: {param.requires_grad}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both Tokenizer and Model we can tokenize the dataset and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eos_token(idx, df, eos_token):\n",
    "    # build the user input including the problem description, the time complexity and the space complexity\n",
    "    chat_string = 'User:' + df.loc[idx, 'problem_description'] + ' Time complexity: ' + df.loc[idx, 'time_complexity_inferred'] + '; Space complexity: ' + df.loc[idx, 'space_complexity_inferred']\n",
    "    # now add the eos token and the response from the assistant, the code solution for the problem\n",
    "    chat_string = chat_string + eos_token + 'Assistant: ' + df.loc[idx, 'solution_code'] + eos_token\n",
    "    return chat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = [apply_eos_token(idx, train, tokenizer_gpt2.eos_token) for idx in train.index]\n",
    "test_str = [apply_eos_token(idx, test, tokenizer_gpt2.eos_token) for idx in test.index]\n",
    "val_str = [apply_eos_token(idx, val, tokenizer_gpt2.eos_token) for idx in val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'User: Manao has invented a new mathematical term â€” a beautiful set of points. He calls a set of points on a plane beautiful if it meets the following conditions:\\n\\n  1. The coordinates of each point in the set are integers. \\n  2. For any two points from the set, the distance between them is a non-integer. \\n\\n\\n\\nConsider all points (x, y) which satisfy the inequations: 0 â‰¤ x â‰¤ n; 0 â‰¤ y â‰¤ m; x + y > 0. Choose their subset of maximum size such that it is also a beautiful set of points.\\n\\nInput\\n\\nThe single line contains two space-separated integers n and m (1 â‰¤ n, m â‰¤ 100).\\n\\nOutput\\n\\nIn the first line print a single integer â€” the size k of the found beautiful set. In each of the next k lines print a pair of space-separated integers â€” the x- and y- coordinates, respectively, of a point from the set.\\n\\nIf there are several optimal solutions, you may print any of them.\\n\\nExamples\\n\\nInput\\n\\n2 2\\n\\n\\nOutput\\n\\n3\\n0 1\\n1 2\\n2 0\\n\\n\\nInput\\n\\n4 3\\n\\n\\nOutput\\n\\n4\\n0 3\\n2 1\\n3 0\\n4 2\\n\\nNote\\n\\nConsider the first sample. The distance between points (0, 1) and (1, 2) equals <image>, between (0, 1) and (2, 0) â€” <image>, between (1, 2) and (2, 0) â€” <image>. Thus, these points form a beautiful set. You cannot form a beautiful set with more than three points out of the given points. Note that this is not the only solution. Time complexity: O(n+m); Space complexity: O(n+m)<|endoftext|>Assistant: n,m=map(int,input().split())\\nif n>m:\\n    print(m+1)\\n    for i in range(m+1):\\n        print(n-i,i)\\nelse:\\n    print(n+1)\\n    for i in range(n+1):\\n        print(i,m-i)<|endoftext|>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings where the eos token was applied, are now put in a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'chat': train_str})\n",
    "test_data = Dataset.from_dict({'chat': test_str})\n",
    "val_data = Dataset.from_dict({'chat': val_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['val'] = val_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096e3cf827674134aa59ab45adff3d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b9e0817954d40ab8d41990bde0b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17543e30d4f0451d9c281a4dfdbfd7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer_gpt2(examples[\"chat\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids,\n",
    "        'attention_mask': input_encodings.attention_mask,\n",
    "        'labels': input_encodings.input_ids.copy()\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sequences in same batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer_gpt2, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Perplexity before fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random conversation from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input:  User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|> Assistant: \n",
      "####\n",
      "Correct solution output:  Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "test_input = test_input + tokenizer_gpt2.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: ', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: ', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipeline\n",
      "  Downloading pipeline-0.1.0-py3-none-any.whl.metadata (483 bytes)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pipeline, evaluate\n",
      "Successfully installed evaluate-0.4.3 pipeline-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pipeline transformers evaluate\n",
    "from transformers import pipeline\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|> Assistant:  In 1-15 is the data.\n",
      "[2. 2D 2. # (d as the current row from the first\n",
      "    :math.0, in the same and 7.  J.\n",
      "\n",
      "N and *k (P-D array to be defined from the model) for the `s, Ral is no set by default value, *6f1.0 (or R.0, which that ``b\n",
      "\"\"\"  The first column are the full\n",
      "* are the following\n",
      "    G/6.0. In the input-4.18 (not \"L2.e.  The color the maximum label of the same number of the Si -6/E-2.\n",
      " *a with the Apache problem. the\n",
      "\n",
      "The value of the training size in the maximum input). If not to be a same data, a numpy and x.\n",
      "\n",
      "\n",
      "\n",
      "* the C-1.\n",
      "T-2. The data.0 *x, * np.\n",
      "/2.0 and the given time.0.\n",
      "\n",
      "The number of the \"o.0 in the same kernel of the same class, which. *Y are been two features file, - `r(a. -4.\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe(test_input, max_new_tokens=256)\n",
    "print(\"Generated text:\\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text does not include the solution code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84b152fff5d4f0299a8a844882ede08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5631bc2a7047aaab49992ebb704e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity before fine-tuning: {'perplexities': [653.0138549804688, 570.7035522460938, 424.50201416015625], 'mean_perplexity': np.float64(549.4064737955729)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity_before_finetuning = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    model_id= model_name\n",
    ")\n",
    "print(\"Perplexity before fine-tuning:\", perplexity_before_finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning\n",
    "Now we can start the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"gpt2-parrot_trainer\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8, \n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,  \n",
    "    report_to=None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= gpt2,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_data['train'],\n",
    "    eval_dataset= tokenized_data['val'],\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time taken by the model to finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 39:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.458700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=675, training_loss=1.1649107078269676, metrics={'train_runtime': 2385.8294, 'train_samples_per_second': 4.527, 'train_steps_per_second': 0.283, 'total_flos': 2821953945600000.0, 'train_loss': 1.1649107078269676, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  2386.2620153427124\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Training time: \", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPT-2 model has 148 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (50000, 768)\n",
      "transformer.wpe.weight                                   (1024, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "transformer.h.0.ln_1.weight                                   (768,)\n",
      "transformer.h.0.ln_1.bias                                     (768,)\n",
      "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
      "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias                              (768,)\n",
      "transformer.h.0.ln_2.weight                                   (768,)\n",
      "transformer.h.0.ln_2.bias                                     (768,)\n",
      "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.ln_f.weight                                       (768,)\n",
      "transformer.ln_f.bias                                         (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(gpt2.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50000, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: '/content/drive/MyDrive/SFX/NLP_Project/Transformer-trained-models/gpt2-parrot_fine_tuning_2025_05_16_14_58_31'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "gpt2_finetune_path = path_to_project + '/Transformer-trained-models/' + f\"gpt2-parrot_fine_tuning_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer_gpt2.save_pretrained(gpt2_finetune_path)\n",
    "gpt2.save_pretrained(gpt2_finetune_path)\n",
    "print(f\"Checkpoint saved at: \\'{gpt2_finetune_path}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "#gpt2_finetune_path = '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/gpt2-parrot_fine_tuning_2025_05_16_14_58_31'\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(gpt2_finetune_path)\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(gpt2_finetune_path, device_map = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we first extract one chat from the test data randomly, give it as input to the model, and we see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      " User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>\n",
      "####\n",
      "Correct solution output: \n",
      " Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "# add eos token at the end of test_input and test_output\n",
    "test_input = test_input + tokenizer_gpt2.eos_token\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: \\n', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipeline transformers evaluate\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "pipe_finetuned = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after the fine-tuning: \n",
      " User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>Assistant: n=int(input())\n",
      "l=list(map(int,input().split()))\n",
      "    1=0})\n",
      "for i in n:\n",
      "  if j==1:\n",
      "    print('NO')\n",
      "  else:\n",
      "    print(\"NO\")\n",
      "else:\n",
      "    print(\"NO\")\n",
      "else:\n",
      "    print(\"NO\")\n",
      "else:\n",
      "    print(1%2)\n",
      "  print(\"NO\")\n",
      "else:\n",
      "    def __join(int(n):\n",
      "    print(int(n-n-0)\n",
      "    print('NO')\n",
      "    print([])\n",
      "    print(list(int(int,1))\n",
      "    print(\"0 or (n-1,k2))\n",
      "    print(\"NO\")\n",
      "    print(\"NO\")\n",
      "    print(\"YES\")\n",
      "else: A1!=0\")\n",
      "    print(3\")//2 // 2 % 2) +(1))\n",
      "    \n",
      "    for i in range(max(n-2):\n",
      "        print(\"NO\")\n",
      "    else:\n",
      "        print(\"NO\")\n",
      "\n",
      "\n",
      "if n*m):\n",
      "        print(\"YES\")\n",
      "       elif(s[0):\n",
      "        print(\"YES\")\n",
      "    else:\n",
      "        print(\"YES\")\n",
      "    else:\n",
      "        print(\"YES\")\n",
      "    print(\"NO\")\n",
      "    if(3[0]\n",
      "    else\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe_finetuned(test_input, max_new_tokens=256)\n",
    "print(\"Generated text after the fine-tuning: \\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text after the fine-tuning includes now the python code. The code is still not correct, especially from a syntactical point of view, but the model is able to maintain the espected format of the response ('user:' and 'assistant:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to consider the metrics Perplexity and BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffd5be745124ab58514a9d00fd161c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity post fine-tuning: {'perplexities': [1.080601692199707, 1.1230411529541016, 1.0684187412261963], 'mean_perplexity': np.float64(1.0906871954600017)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    add_start_token=False,\n",
    "    model_id= gpt2_finetune_path\n",
    ")\n",
    "print(\"Perplexity post fine-tuning:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity calculated before the fine-tuning was [653.0138549804688, 570.7035522460938, 424.50201416015625] with mean_perplexity: np.float64(549.4064737955729)\n",
    "\n",
    "Now we can notice how the values for the perplexity are much lower, similar to those achieved by the gpt2-small model, indicating how the fine-tuning process managed to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**\n",
    "\n",
    "BLEU focuses on precision by counting matching n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52870268b7134df49be0e7b83d29d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00fed6b270f4f83bca8953b54bde8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafb1988b06b47609d38a246e30eb6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.05315658078526119, 'precisions': [0.3118081180811808, 0.10351201478743069, 0.03333333333333333, 0.0074211502782931356], 'brevity_penalty': 1.0, 'length_ratio': 1.7828947368421053, 'translation_length': 542, 'reference_length': 304}\n"
     ]
    }
   ],
   "source": [
    "# BLEU Score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu_metric.compute(\n",
    "    predictions= [pipe_finetuned(test_input[0], max_new_tokens=256)[0]['generated_text']], \n",
    "    references= [test_output[0]]  \n",
    ")\n",
    "print(\"BLEU Score:\", bleu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bleu score, even if higher than the scores achieved by the T5 models, are still low, indicating that the generated text is still far from the reference text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f292e135",
   "metadata": {
    "id": "f292e135"
   },
   "source": [
    "# Try some LLMs on the task\n",
    "\n",
    "The notebook tests how different pre-trained language models perform code generation from a textual problem description. Three prompting techniques are explored:\n",
    "\n",
    "* Zero-shot: The model receives only the problem statement.\n",
    "\n",
    "* One-shot: One example of a solved problem is provided.\n",
    "\n",
    "* Few-shot: Multiple examples are given before asking the model to solve a new problem.\n",
    "\n",
    "Three different LLMs are compared across these settings:\n",
    "\n",
    "\n",
    "* TinyLlama-1.1B: 1.1B parameters (https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
    "* deepseek-coder-1.3b-base: 1.3B parameters (https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base)\n",
    "* Mistral-7B-Instruct-v0.1: 7B parameters (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3bb191",
   "metadata": {
    "id": "7b3bb191"
   },
   "source": [
    "## Imports\n",
    "\n",
    "This section installs and imports all necessary libraries for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea36cb1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ea36cb1d",
    "outputId": "bf708708-147a-46ff-b320-6a4502747a66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
      "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
      "Collecting colorama (from sacrebleu)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: portalocker, colorama, sacrebleu\n",
      "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944392b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "944392b9",
    "outputId": "b6a0e689-ba42-49bd-b54a-eb84ca9ba211"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cd3991a622edd739af4a9ae10905bdea7e64d0c09a9b9a6bc7ac3fc56813fa84\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3db4d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ede3db4d",
    "outputId": "868edb40-eb6d-4ab3-8813-e791bdb379d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
      "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    }
   ],
   "source": [
    "!pip install -U bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b00d7e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51b00d7e",
    "outputId": "52816d48-6940-4066-ec54-0c7ee38ef7c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 1.6.0\n",
      "    Uninstalling accelerate-1.6.0:\n",
      "      Successfully uninstalled accelerate-1.6.0\n",
      "Successfully installed accelerate-1.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacfe386",
   "metadata": {
    "id": "cacfe386"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3905c4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0b3905c4",
    "outputId": "5b7ebee8-7f2d-4fd5-d59f-aca955b8610e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-community langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1836a0",
   "metadata": {
    "id": "bb1836a0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41fe725",
   "metadata": {
    "id": "d41fe725"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195cab4",
   "metadata": {
    "id": "a195cab4"
   },
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db394b",
   "metadata": {
    "id": "28db394b"
   },
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the dataset that has already been preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d9c1fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "a4d9c1fd",
    "outputId": "a38fa7c7-7342-4d77-d7aa-a4d38680ac90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/.shortcut-targets-by-id/17WgJO1gfIBADpYX2jVdb41q7HCbwWcOU/NLP_Project'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "path = 'Colab Notebooks/NLP/NLP_Project'\n",
    "\n",
    "os.chdir(f'/content/drive/MyDrive/{path}')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed1f3c0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eed1f3c0",
    "outputId": "88736781-cf2b-4387-879d-0be2eb807c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 problem_description solution_id  \\\n",
      "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
      "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
      "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
      "3  Xenia has a set of weights and pan scales. Eac...         0_6   \n",
      "4  Xenia has a set of weights and pan scales. Eac...         0_8   \n",
      "\n",
      "                                       solution_code  \\\n",
      "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
      "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
      "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
      "3  MOD = 10**9 + 7\\nI = lambda:list(map(int,input...   \n",
      "4  to_print = []\\ndef dfs(d, ini, s, depth, m):\\n...   \n",
      "\n",
      "               problem_name time_complexity_inferred space_complexity_inferred  \n",
      "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
      "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "3  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "4  339_C. Xenia and Weights                     O(1)                      O(1)  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_ds.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc30840a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc30840a",
    "outputId": "d5073d1c-2617-4602-9234-522db84a51e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244876, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5500776",
   "metadata": {
    "id": "d5500776"
   },
   "source": [
    "## Try TinyLlama on the task\n",
    "\n",
    "The TinyLlama-1.1B-Chat-v1.0 is a compact open-source language model with 1.1 billion parameters, trained for chat and code generation tasks. This version is fine-tuned for multi-turn conversations and instruction-following behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7118aa6",
   "metadata": {
    "id": "d7118aa6"
   },
   "source": [
    "Here we initializes a Hugging Face pipeline for text generation, the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72306705",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363,
     "referenced_widgets": [
      "33e12e57a3a44609816c57fd9369313a",
      "1bbc6d7ada6740a98c1cd1011a8942d6",
      "08f19498b215477f9f66c3e255d4fc8d",
      "10363323aac14ee68a113a3023b688b7",
      "7de60d1676754d25992e0c56c36d4021",
      "e13ee6b3517641e4b9ce1bd0f3784b93",
      "b103b30abd5846cc833964fc5f223dcc",
      "52ee37f3e62443d59d7a93bd5aa5c4ef",
      "1e53ef3270f140ceb73da442f8e5b6f0",
      "3604917ccef14975ae759f2116ac9fef",
      "f729215829964eb9a0dfd112351810df",
      "4e71d4787c8f4ea58d82de252ca9d48c",
      "93d4b9a899b1433f9ada5cffd9a0beb5",
      "03de041c94704b248438d90eb161db31",
      "895994746ce14a52a30e05f593063c3f",
      "6c4754381ec54e13bafc0ba36de8f232",
      "a47a07b05fbf44e9a624fb43f38c690e",
      "fa73e7c00c784bc0b718090cc8361103",
      "790092184906487bb99ded5135278955",
      "8220f521b6d7402f93b247d426479c93",
      "08a843493e3d49338a55d681d816362b",
      "d13831c435ce40b5b1ef5680af69623d",
      "0cb38e67f2e14148b30cd8c40e88305b",
      "dc860a07db014cea97bc2cc289eceb7f",
      "14155752c9d64899b784e696f3286f59",
      "2856fdce52eb4da0ad5f6b5ccd39df73",
      "59e127b4ae7a4c5d88d040b15b58bcb7",
      "d802ddbd96214a36a46019c9b8470875",
      "df30d5558e19476cada8f71c9ae10b48",
      "e18fdfb0cf1f407daea988565ae021ff",
      "71e554f7b69f4441b84ca73bf1f051b9",
      "c85fad37b83b4ba089e88edf673e485a",
      "ad5079c77b3a40f9a7af88a016127f47",
      "551622d595c3410e9064e3d245dc9efb",
      "53064cf23c2c4780bc5ce4486fa2e745",
      "3c1741a1db5e4bcf86375bb7a38b570d",
      "3f2dbd4cfb89422581e674b1c014cc58",
      "53dc523deb3048c88b912d4b61bffcf3",
      "168ee20b8c784985aadc8c10eefe8282",
      "c9d36a6174614b1d9fc7f1ec6ca5d0b0",
      "74fdb5165d8f4576909df7f47c7d9dd3",
      "d7cd30e16f2e4a9fb6faba3ba88fa474",
      "f0252a42b8054dfba4ffb0b031474a9e",
      "1ccb444b5a784aaeb3713efbeb94978b",
      "4f4fbdd063ba46509eb3774e6e460dc9",
      "c9212337f663463fba892853a0fca04a",
      "d5d1075e9f734f5f89d6efa2c9a5c332",
      "850121d4cd0a45e293127f39db720d87",
      "f981f779839c4c9d9ab65f2971560e04",
      "aa4d89d66718446186db72914abf1ddf",
      "c8f75b9d3dfa4237871098c4b0817fa5",
      "e5eb3f0548ad4ed9a5a0139f775abb67",
      "075c8fe7d6274b9c8f40423902610a28",
      "e1486a16ea4f4ffca0d71adbbf4ff0d7",
      "4a82b93e9a1146d3a69c7697bd7adec2",
      "39d5156e58034ff69dde71c1ffa892ad",
      "428b2de4cfde490e9c5e8346c61ab433",
      "e31fd63683084f1f90a3dc8209cd7088",
      "80a8c525d6a041b79d34cac1d4d90a17",
      "945fa66824ea4cac93547a12c04771fb",
      "a3d204bea94f47c29c9f038f11facd7b",
      "ac4144836be24270b8fba210931e08f0",
      "57712c4f4f2d437381fc30d25d6607bc",
      "14d5f227e9ce444b98a51f3f7ac24bee",
      "463a242bb5cf405898d0ec48ed70fd16",
      "ab1b520d88254a939042ea4b978065c5",
      "1ef96045a0a442b996a71f1da51c954d",
      "9013fac9ba054f4ca7504684a5ff77b6",
      "10f8d7771d40468e8320b67736611dfd",
      "09a0fdb6fc6c44239b8f4385d65de7e5",
      "986980aa1f954f41a3cc2cc676ecf99a",
      "ee69b7f5b2cc49a69a6bfcb5a35c222e",
      "f3fed1da9ba948888081f8eef633c45f",
      "45e01c47dd83425fbd71c538ebd3f5de",
      "0adaae9c96ef4656941ffd38c559f93b",
      "bade19de629b44279788c83496fa4d65",
      "3d96a0c8054d4e2a89832b718aa632ab"
     ]
    },
    "id": "72306705",
    "outputId": "197409c6-99c2-4170-f5ef-4d15632a91e6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e12e57a3a44609816c57fd9369313a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e71d4787c8f4ea58d82de252ca9d48c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb38e67f2e14148b30cd8c40e88305b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551622d595c3410e9064e3d245dc9efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f4fbdd063ba46509eb3774e6e460dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d5156e58034ff69dde71c1ffa892ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef96045a0a442b996a71f1da51c954d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0957e2",
   "metadata": {
    "id": "2f0957e2"
   },
   "source": [
    "This function creates a list of chat-style messages to prompt a language model for code generation, using zero-shot, one-shot or few-shot prompting strategies.\n",
    "\n",
    "The returned messages list is formatted with \"system\", \"user\", and \"assistant\" roles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f1fb3a",
   "metadata": {
    "id": "b0f1fb3a"
   },
   "outputs": [],
   "source": [
    "def build_messages(df, index, mode='zero-shot', num_few_shot=2, random_state=0):\n",
    "    problem = clean_problem_description(df.loc[index, 'problem_description'])\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    if mode == 'zero-shot':\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Can you please solve the problem below by writing the corresponding Python code.\\n\\n\"\n",
    "                       f\"### Problem:\\n{problem}\\n### Your Solution:\"\n",
    "        })\n",
    "\n",
    "    elif mode == 'one-shot':\n",
    "        example_idx = df.drop(index).sample(1, random_state=random_state).index[0]\n",
    "        example_problem = clean_problem_description(df.loc[example_idx, 'problem_description'])\n",
    "        example_solution = df.loc[example_idx, 'solution_code']\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"### Problem:\\n{example_problem}\\n### Solution Expected:\"\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{example_solution}\"\n",
    "        })\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Now please solve the problem below by writing the corresponding Python code.\\n\\n\"\n",
    "                f\"### Problem:\\n{problem}\\n### Your Solution:\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "    elif mode == 'few-shot':\n",
    "        example_indices = df.drop(index).sample(num_few_shot, random_state=random_state).index\n",
    "\n",
    "        for i in example_indices:\n",
    "            example_problem = clean_problem_description(df.loc[i, 'problem_description'])\n",
    "            example_solution = df.loc[i, 'solution_code']\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"### Problem:\\n{example_problem}\\n### Solution Expected:\"\n",
    "            })\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"{example_solution}\"\n",
    "            })\n",
    "\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": (\n",
    "                \"Now please solve the problem below by writing the corresponding Python code.\\n\\n\"\n",
    "                f\"### Problem:\\n{problem}\\n### Your Solution:\"\n",
    "            )\n",
    "        })\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ad451d",
   "metadata": {
    "id": "29ad451d"
   },
   "source": [
    "This function uses the chat-formatted messages to build a prompt and pass it to the language model for code generation. The following generation parameters are used:\n",
    "\n",
    "* max_new_tokens=256: limits the number of new tokens the model can generate\n",
    "* do_sample=True: enables sampling, which allows the model to produce more diverse and creative outputs instead of always choosing the most likely next token.\n",
    "* temperature=0.7: controls the randomness of the predictions.\n",
    "* top_k=50: restricts sampling to the top 50 most probable tokens at each step.\n",
    "* top_p=0.95: enables nucleus sampling, where the model considers only the smallest possible set of tokens whose cumulative probability is at least 95%.\n",
    "\n",
    "The model returns the generated Python solution as plain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ba4005",
   "metadata": {
    "id": "48ba4005"
   },
   "outputs": [],
   "source": [
    "def generate_solution(messages, max_new_tokens=256):\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    return outputs[0][\"generated_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f9f467",
   "metadata": {
    "id": "75f9f467"
   },
   "source": [
    "This function evaluates the similarity between a generated solution and a reference solution:\n",
    "* BLEU: Uses sacrebleu to compute n-gram precision.\n",
    "* ROUGE-1 / ROUGE-L: Measures recall-based overlap (unigrams and longest common subsequence).\n",
    "* Exact Match: Checks if the strings are exactly equal after stripping whitespace.\n",
    "\n",
    "Returns a dictionary with all four metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902c6fd",
   "metadata": {
    "id": "2902c6fd"
   },
   "outputs": [],
   "source": [
    "def evaluate_metrics(reference, hypothesis):\n",
    "    bleu_score = sacrebleu.corpus_bleu([hypothesis], [[reference]]).score\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "    rouge_scores = scorer.score(reference, hypothesis)\n",
    "    exact = int(reference.strip() == hypothesis.strip())\n",
    "    return {\n",
    "        'bleu': bleu_score,\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
    "        'exact_match': exact\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec751d",
   "metadata": {
    "id": "0aec751d"
   },
   "source": [
    "This function is used to remove the examples inputs and outputs of the problem description, so the one and few shot prompts are less confusing for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ca9af7",
   "metadata": {
    "id": "41ca9af7"
   },
   "outputs": [],
   "source": [
    "def clean_problem_description(text: str) -> str:\n",
    "    lines = text.strip().splitlines()\n",
    "    result = []\n",
    "    for line in lines:\n",
    "        if line.strip().lower().startswith(\"examples\"):\n",
    "            break\n",
    "        result.append(line)\n",
    "    return \"\\n\".join(result).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f7e595",
   "metadata": {
    "id": "08f7e595"
   },
   "source": [
    "Here we get a random sample of the dataset so we can test the three prompt types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54929d7e",
   "metadata": {
    "id": "54929d7e"
   },
   "outputs": [],
   "source": [
    "example = df.sample(n=1, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02af2f2e",
   "metadata": {
    "id": "02af2f2e"
   },
   "outputs": [],
   "source": [
    "n = example.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8740a141",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8740a141",
    "outputId": "f6db42ff-02dd-4d10-9c39-23945aac2259"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(15399)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1899961",
   "metadata": {
    "id": "f1899961"
   },
   "outputs": [],
   "source": [
    "example_text = df.iloc[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d54cefc",
   "metadata": {
    "id": "3d54cefc"
   },
   "outputs": [],
   "source": [
    "clean_text = clean_problem_description(df.loc[n, 'problem_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28362204",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "28362204",
    "outputId": "d7fa5350-03c4-43a4-bc5a-906c5fee3075"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'There are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0010fe0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0010fe0",
    "outputId": "cacc5837-af6b-4505-ddd0-6ab81eea99be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "3 2\n",
      "2 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "1\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "5 5\n",
      "2 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "6\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "2 4\n",
      "2 4\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(example_text['problem_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ae18c2",
   "metadata": {
    "id": "55ae18c2"
   },
   "source": [
    "Here we can the the description of the problem we are going to use to test the prompts. It receives four numbers a, b, c and d as inputs and should output (a-c)*(b-d) as the number of remaining white cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcb029d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4fcb029d",
    "outputId": "1529aa81-36bf-4dc2-990b-a95257ef31fb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'a,b=map(int,input().split())\\nc,d=map(int,input().split())\\nprint((a-c)*(b-d))'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text['solution_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2d85fb",
   "metadata": {
    "id": "aa2d85fb"
   },
   "source": [
    "First we need to build the zero shot prompt using the build_messages function and specifying the zero-shot approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b0ea1a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9b0ea1a",
    "outputId": "4dfd5c38-4910-4abc-a05e-6c51ad58f14a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.'}, {'role': 'user', 'content': 'Can you please solve the problem below by writing the corresponding Python code.\\n\\n### Problem:\\nThere are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n### Your Solution:'}]\n"
     ]
    }
   ],
   "source": [
    "zero_shot_messages = build_messages(df, example.index[0], mode='zero-shot')\n",
    "print(zero_shot_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b298450",
   "metadata": {
    "id": "9b298450"
   },
   "source": [
    "Now we pass this prompt to the system and print the generated output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e032ba",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a3e032ba",
    "outputId": "77171511-1409-4d13-a217-1b28b2761236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.</s>\n",
      "<|user|>\n",
      "Can you please solve the problem below by writing the corresponding Python code.\n",
      "\n",
      "### Problem:\n",
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "### Your Solution:</s>\n",
      "<|assistant|>\n",
      "Here's the Python code for the problem:\n",
      "\n",
      "```python\n",
      "import sys\n",
      "\n",
      "# Check input format\n",
      "if len(sys.argv)!= 2:\n",
      "    print(\"Usage: python3 solution.py <input_file_name>\")\n",
      "    sys.exit(1)\n",
      "\n",
      "try:\n",
      "    input_file = open(sys.argv[1], \"r\")\n",
      "except FileNotFoundError:\n",
      "    print(\"Input file not found!\")\n",
      "    sys.exit(1)\n",
      "\n",
      "try:\n",
      "    n = int(input_file.readline())\n",
      "    h = int(input_file.readline())\n",
      "    w = int(input_file.readline())\n",
      "    input_file.close()\n",
      "except ValueError:\n",
      "    print(\"Invalid input format!\")\n",
      "    sys.exit(1)\n",
      "\n",
      "# Check if all values are integers\n",
      "while h > 0 or w > 0:\n",
      "    try:\n",
      "        h = int(input_file.readline())\n",
      "    except ValueError:\n",
      "        print(\"Invalid input format!\")\n",
      "        sys.exit(1)\n",
      "    try:\n",
      "        w = int(input_file.read\n"
     ]
    }
   ],
   "source": [
    "zero_shot_generated = generate_solution(zero_shot_messages)\n",
    "print(zero_shot_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366e7f7c",
   "metadata": {
    "id": "366e7f7c"
   },
   "source": [
    "The code generated by the model for this problem was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b398de",
   "metadata": {
    "id": "24b398de"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check input format\n",
    "if len(sys.argv)!= 2:\n",
    "    print(\"Usage: python3 solution.py <input_file_name>\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    input_file = open(sys.argv[1], \"r\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Input file not found!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    n = int(input_file.readline())\n",
    "    h = int(input_file.readline())\n",
    "    w = int(input_file.readline())\n",
    "    input_file.close()\n",
    "except ValueError:\n",
    "    print(\"Invalid input format!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Check if all values are integers\n",
    "while h > 0 or w > 0:\n",
    "    try:\n",
    "        h = int(input_file.readline())\n",
    "    except ValueError:\n",
    "        print(\"Invalid input format!\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        w = int(input_file.read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff9b79d",
   "metadata": {
    "id": "9ff9b79d"
   },
   "source": [
    "The generated code attempts to read input from a file using sys.argv, but it is incorrect for the proposed problem. It closes the file before finishing reading, and contains incomplete and invalid chunks. And it does not solve the problem correctly or print the expected output.\n",
    "\n",
    "However, we can see that the model is already generating Python code with a reasonable structure, including basic input handling, error checking, and use of standard libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fd62c7",
   "metadata": {
    "id": "63fd62c7"
   },
   "source": [
    "Now we see if the model performs better with the one-shot prompt approach. In this cell we generate the one shot prompt using also a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914c751d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "914c751d",
    "outputId": "eff3cd4b-4267-4146-8b8d-2f7b10858442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.'}, {'role': 'user', 'content': '### Problem:\\nYou are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\\n\\nAt least how many operations are necessary to satisfy the following conditions?\\n\\n* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\\n* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\\n\\nConstraints\\n\\n* 2 â‰¤ n â‰¤ 10^5\\n* |a_i| â‰¤ 10^9\\n* Each a_i is an integer.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nn\\na_1 a_2 ... a_n\\n\\n\\nOutput\\n\\nPrint the minimum necessary count of operations.\\n### Solution Expected:'}, {'role': 'assistant', 'content': 'N = int(input())\\na = list(map(int, input().split()))\\n\\nans1 = 0\\ns = 0\\nflg = 1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans1 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nans2 = 0\\ns = 0\\nflg = -1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans2 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nprint(min(ans1, ans2))'}, {'role': 'user', 'content': 'Now please solve the problem below by writing the corresponding Python code.\\n\\n### Problem:\\nThere are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n### Your Solution:'}]\n"
     ]
    }
   ],
   "source": [
    "one_shot_messages = build_messages(df, example.index[0], mode='one-shot')\n",
    "print(one_shot_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c966872",
   "metadata": {
    "id": "3c966872"
   },
   "source": [
    "And now we generate the output for this prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8a2503",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f8a2503",
    "outputId": "9194e906-c17b-4ac2-ac74-504607a92c84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.</s>\n",
      "<|user|>\n",
      "### Problem:\n",
      "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "### Solution Expected:</s>\n",
      "<|assistant|>\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))</s>\n",
      "<|user|>\n",
      "Now please solve the problem below by writing the corresponding Python code.\n",
      "\n",
      "### Problem:\n",
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "### Your Solution:</s>\n",
      "<|assistant|>\n",
      "N = int(input())\n",
      "h = int(input())\n",
      "w = int(input())\n",
      "\n",
      "if h * w <= 0:\n",
      "    print(1)\n",
      "else:\n",
      "    cells_remaining = 0\n",
      "    for row in range(h):\n",
      "        for col in range(w):\n",
      "            if row * w + col <= h * w:\n",
      "                cells_remaining += 1\n",
      "    print(cells_remaining)\n",
      "```\n",
      "\n",
      "Explanation:\n",
      "\n",
      "The problem is to find the number of white cells that will remain in a grid with `h` rows and `w` columns, where each row or column contains cells of size `h`x`w`. The `h` and `w` values are obtained from input, and the cells_remaining variable is initialized to 0.\n",
      "\n",
      "The cells_remaining variable is iterated over each row and column. For each row, we check if the number of white cells is still less than or equal to the product of `h` and `w`. If this condition is met, we increment cells_remaining by 1. If this condition is not met, we do not increment cells_remaining and\n"
     ]
    }
   ],
   "source": [
    "one_shot_generated = generate_solution(one_shot_messages)\n",
    "print(one_shot_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c46f888",
   "metadata": {
    "id": "9c46f888"
   },
   "source": [
    "Now the code generated was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2939bf71",
   "metadata": {
    "id": "2939bf71"
   },
   "outputs": [],
   "source": [
    "N = int(input())\n",
    "h = int(input())\n",
    "w = int(input())\n",
    "\n",
    "if h * w <= 0:\n",
    "    print(1)\n",
    "else:\n",
    "    cells_remaining = 0\n",
    "    for row in range(h):\n",
    "        for col in range(w):\n",
    "            if row * w + col <= h * w:\n",
    "                cells_remaining += 1\n",
    "    print(cells_remaining)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11187750",
   "metadata": {
    "id": "11187750"
   },
   "source": [
    "Which still has a well structured python code but logically doesn't solve the proposed problem. Now we can also see that after the code the model tried to explain the logic behind it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06532c21",
   "metadata": {
    "id": "06532c21"
   },
   "source": [
    "The last approach if the few-shot, giving the model two examples instead of just one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c501d1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8c501d1c",
    "outputId": "8398aa04-1739-4dec-9791-a6c305ca67f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': 'You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.'}, {'role': 'user', 'content': '### Problem:\\nYou are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\\n\\nAt least how many operations are necessary to satisfy the following conditions?\\n\\n* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\\n* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\\n\\nConstraints\\n\\n* 2 â‰¤ n â‰¤ 10^5\\n* |a_i| â‰¤ 10^9\\n* Each a_i is an integer.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nn\\na_1 a_2 ... a_n\\n\\n\\nOutput\\n\\nPrint the minimum necessary count of operations.\\n### Solution Expected:'}, {'role': 'assistant', 'content': 'N = int(input())\\na = list(map(int, input().split()))\\n\\nans1 = 0\\ns = 0\\nflg = 1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans1 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nans2 = 0\\ns = 0\\nflg = -1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans2 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nprint(min(ans1, ans2))'}, {'role': 'user', 'content': '### Problem:\\nIn some other world, today is the day before Christmas Eve.\\n\\nMr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\\\leq i \\\\leq N) is p_i yen (the currency of Japan).\\n\\nHe has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\\n\\nConstraints\\n\\n* 2 \\\\leq N \\\\leq 10\\n* 100 \\\\leq p_i \\\\leq 10000\\n* p_i is an even number.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nN\\np_1\\np_2\\n:\\np_N\\n\\n\\nOutput\\n\\nPrint the total amount Mr. Takaha will pay.\\n### Solution Expected:'}, {'role': 'assistant', 'content': 'N=int(input())\\nP=sorted([int(input()) for _ in range(N)])\\nprint(sum(P)-P[-1]//2)'}, {'role': 'user', 'content': 'Now please solve the problem below by writing the corresponding Python code.\\n\\n### Problem:\\nThere are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n### Your Solution:'}]\n"
     ]
    }
   ],
   "source": [
    "few_shot_messages = build_messages(df, example.index[0], mode='few-shot')\n",
    "print(few_shot_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f173f6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56f173f6",
    "outputId": "793ce501-0a13-4c38-97cb-05b7ea108fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.</s>\n",
      "<|user|>\n",
      "### Problem:\n",
      "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "### Solution Expected:</s>\n",
      "<|assistant|>\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))</s>\n",
      "<|user|>\n",
      "### Problem:\n",
      "In some other world, today is the day before Christmas Eve.\n",
      "\n",
      "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
      "\n",
      "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 \\leq N \\leq 10\n",
      "* 100 \\leq p_i \\leq 10000\n",
      "* p_i is an even number.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "N\n",
      "p_1\n",
      "p_2\n",
      ":\n",
      "p_N\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the total amount Mr. Takaha will pay.\n",
      "### Solution Expected:</s>\n",
      "<|assistant|>\n",
      "N=int(input())\n",
      "P=sorted([int(input()) for _ in range(N)])\n",
      "print(sum(P)-P[-1]//2)</s>\n",
      "<|user|>\n",
      "Now please solve the problem below by writing the corresponding Python code.\n",
      "\n",
      "### Problem:\n",
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "### Your Solution:</s>\n",
      "<|assistant|>\n",
      "N=int(input())\n",
      "h=int(input()[0])\n",
      "w=int(input()[1])\n",
      "\n",
      "# Find the number of white cells in H rows and W columns\n",
      "rows_num=0\n",
      "for I in range(h):\n",
      "    for j in range(w):\n",
      "        if input()[i*w+j] == 'w':\n",
      "            rows_num+=1\n",
      "\n",
      "# Calculate the number of white cells in the remaining H rows and W columns\n",
      "remaining_rows_num=0\n",
      "for I in range(h-1, -1, -1):\n",
      "    for j in range(w):\n",
      "        if input()[i*w+j] == 'w':\n",
      "            remaining_rows_num+=1\n",
      "\n",
      "# Calculate the number of white cells remaining in H rows and W columns\n",
      "remaining_cells_num=rows_num-remaining_rows_num\n",
      "\n",
      "print(remaining_cells_num)\n",
      "\n",
      "# Example:\n",
      "# H = 3, W = 4\n",
      "# h w\n",
      "# 1 0 0 1 0 0 0\n",
      "# 0 1 1\n"
     ]
    }
   ],
   "source": [
    "few_shot_generated = generate_solution(few_shot_messages)\n",
    "print(few_shot_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c339fb",
   "metadata": {
    "id": "28c339fb"
   },
   "source": [
    "The code generated this time was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011fca4",
   "metadata": {
    "id": "0011fca4"
   },
   "outputs": [],
   "source": [
    "N=int(input())\n",
    "h=int(input()[0])\n",
    "w=int(input()[1])\n",
    "\n",
    "# Find the number of white cells in H rows and W columns\n",
    "rows_num=0\n",
    "for I in range(h):\n",
    "    for j in range(w):\n",
    "        if input()[i*w+j] == 'w':\n",
    "            rows_num+=1\n",
    "\n",
    "# Calculate the number of white cells in the remaining H rows and W columns\n",
    "remaining_rows_num=0\n",
    "for I in range(h-1, -1, -1):\n",
    "    for j in range(w):\n",
    "        if input()[i*w+j] == 'w':\n",
    "            remaining_rows_num+=1\n",
    "\n",
    "# Calculate the number of white cells remaining in H rows and W columns\n",
    "remaining_cells_num=rows_num-remaining_rows_num\n",
    "\n",
    "print(remaining_cells_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6bcfde",
   "metadata": {
    "id": "1c6bcfde"
   },
   "source": [
    "The few-shot output better aligns with the grid structure of the problem and introduces more descriptive variable names. However, it still misinterprets the input format and fails to implement the correct logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d53768",
   "metadata": {
    "id": "36d53768"
   },
   "source": [
    "Now we will run the three prompting approaches with a few samples to get some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd12947",
   "metadata": {
    "id": "2dd12947"
   },
   "outputs": [],
   "source": [
    "results = {'zero-shot': [], 'one-shot': [], 'few-shot': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b49365a",
   "metadata": {
    "id": "9b49365a"
   },
   "outputs": [],
   "source": [
    "sample_indices = df.sample(5, random_state=0).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681776e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "f681776e",
    "outputId": "d0743cb9-d299-4d39-bd2c-ecc7b9cbadcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: zero-shot, Example 219235\n",
      "bleu: 1.1744\n",
      "rouge1: 0.0337\n",
      "rougeL: 0.0337\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 116625\n",
      "bleu: 4.5137\n",
      "rouge1: 0.1049\n",
      "rougeL: 0.0918\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 161440\n",
      "bleu: 1.8577\n",
      "rouge1: 0.1154\n",
      "rougeL: 0.0726\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 177012\n",
      "bleu: 8.3524\n",
      "rouge1: 0.2725\n",
      "rougeL: 0.1755\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 26857\n",
      "bleu: 7.2144\n",
      "rouge1: 0.2652\n",
      "rougeL: 0.1215\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 219235\n",
      "bleu: 1.2996\n",
      "rouge1: 0.0182\n",
      "rougeL: 0.0182\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: one-shot, Example 116625\n",
      "bleu: 2.4819\n",
      "rouge1: 0.0784\n",
      "rougeL: 0.0610\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 161440\n",
      "bleu: 1.9589\n",
      "rouge1: 0.1223\n",
      "rougeL: 0.0699\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 177012\n",
      "bleu: 5.9250\n",
      "rouge1: 0.2358\n",
      "rougeL: 0.1440\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 26857\n",
      "bleu: 4.6816\n",
      "rouge1: 0.2055\n",
      "rougeL: 0.1054\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 219235\n",
      "bleu: 0.6755\n",
      "rouge1: 0.0095\n",
      "rougeL: 0.0095\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: few-shot, Example 116625\n",
      "bleu: 1.6039\n",
      "rouge1: 0.0285\n",
      "rougeL: 0.0268\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 161440\n",
      "bleu: 1.8570\n",
      "rouge1: 0.0734\n",
      "rougeL: 0.0500\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 177012\n",
      "bleu: 3.7599\n",
      "rouge1: 0.1353\n",
      "rougeL: 0.0844\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 26857\n",
      "bleu: 4.0815\n",
      "rouge1: 0.1560\n",
      "rougeL: 0.0705\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_results = []\n",
    "\n",
    "for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for idx in sample_indices:\n",
    "        messages = build_messages(df, idx, mode=mode, num_few_shot=3)\n",
    "        generated = generate_solution(messages, max_new_tokens=256)\n",
    "\n",
    "        expected = df.loc[idx, 'solution_code']\n",
    "\n",
    "        metrics = evaluate_metrics(expected, generated)\n",
    "\n",
    "        print(f\"Mode: {mode}, Example {idx}\")\n",
    "        #print(\"Generated:\\n\", generated)\n",
    "        #print(\"Expected:\\n\", expected)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        results[mode].append(metrics)\n",
    "\n",
    "        saved_results.append({\n",
    "            'mode': mode,\n",
    "            'index': idx,\n",
    "            'problem_description': df.loc[idx, 'problem_description'],\n",
    "            'expected_solution': expected,\n",
    "            'generated_solution': generated,\n",
    "            'bleu': metrics['bleu'],\n",
    "            'rouge1': metrics['rouge1'],\n",
    "            'rougeL': metrics['rougeL'],\n",
    "            'exact_match': metrics['exact_match']\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a3bf17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "45a3bf17",
    "outputId": "7e53205f-6152-4fc7-b916-fab41ce80fba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average metrics for zero-shot:\n",
      "bleu: 4.6225\n",
      "rouge1: 0.1583\n",
      "rougeL: 0.0990\n",
      "exact_match: 0.0000\n",
      "\n",
      "Average metrics for one-shot:\n",
      "bleu: 3.2694\n",
      "rouge1: 0.1321\n",
      "rougeL: 0.0797\n",
      "exact_match: 0.0000\n",
      "\n",
      "Average metrics for few-shot:\n",
      "bleu: 2.3955\n",
      "rouge1: 0.0805\n",
      "rougeL: 0.0482\n",
      "exact_match: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for mode in results:\n",
    "    avg_metrics = {}\n",
    "    for metric in results[mode][0].keys():\n",
    "        avg_metrics[metric] = sum([ex[metric] for ex in results[mode]]) / len(results[mode])\n",
    "\n",
    "    print(f\"\\nAverage metrics for {mode}:\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2488fc1b",
   "metadata": {
    "id": "2488fc1b"
   },
   "source": [
    "These results show that the zero-shot setting outperforms one-shot and few-shot in all metrics, including BLEU, ROUGE-1, ROUGE-L, and exact match (which is zero across the board). This suggests that providing examples (one-shot or few-shot) did not help the model improve code generation quality for this task, and may have introduced noise or confusion, leading to worse performance.\n",
    "This can be caused by poorly formatted prompts or overly long or complex example prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595bd38",
   "metadata": {
    "id": "4595bd38"
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(saved_results)\n",
    "df_results.to_csv('generated_codes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd949b07",
   "metadata": {
    "id": "dd949b07"
   },
   "outputs": [],
   "source": [
    "with open('generated_codes.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(saved_results, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518546be",
   "metadata": {
    "id": "518546be"
   },
   "source": [
    "## Try deepseek-coder on the task\n",
    "\n",
    "DeepSeek-Coder-1.3B-Base is a 1.3 billion parameter language model specialized in code generation and understanding. It is trained on a diverse set of programming languages and designed to assist with tasks such as code completion, bug fixing, and problem-solving by generating syntactically correct and contextually relevant code snippets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eb4f99",
   "metadata": {
    "id": "38eb4f99"
   },
   "source": [
    "We start by loading the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48098d7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "87599d7f2b264aab92680d59a0aa654b",
      "0f0b0db0eeac4212bc8a7b7fd395bbd3",
      "0ec7212dd1964ec4a2a6eec898fc18fc",
      "bae19b053bf3421ea2970d0137e66e3d",
      "000f135d3d5a451b9482e1f40a9a1e52",
      "3bfa3a0f137142b287ab40f49fbc7b60",
      "8ede59f33e4d4484b41929af233f01ab",
      "b24344d21f5f48b083d6caee1f3dd286",
      "1a4b1fe1c40c4e239ba870737d873f30",
      "d2f772e8736641819a04f57bd04dc1ef",
      "1582d0dc83cd4733b2124c7a9e640da6",
      "7d11f10d218c4b59a41d1db3d3450270",
      "5b475b0cf8264c21945726f6ee231a22",
      "886e9b0e92bc4bb49c6e7ab12aeec924",
      "aecad5b4c97b42f6b92beae8a5bbc446",
      "81168d91180348af95dcdd077d9ef227",
      "37fb19a974684cfa8f51186bccaac0e1",
      "75ae0cde7fce4d3c944f9a491d5e1652",
      "07cf273c8dd7466ea5d0dc25ad6c59ed",
      "79fad8a7322f472484c08f32af219993",
      "9637946ff9fd49a1a52aed5b1075f40e",
      "b1b750d4a12548a2861b7c5feb520512",
      "8d0e18d30d7e46bd81365c8a1680b97f",
      "3caecd3b9b6a4be7968b7ab6f2289332",
      "4f68886ba4804855ac349de3ee4ffd8d",
      "e192780ec4164bd6ae7d3273af88f510",
      "cd4d7cf77d554e0e81e2e09c75604fc0",
      "1d68cf4fb0a54f3f829430a7d1af2442",
      "667fa3ad502748f6bcff359bf473bc83",
      "1fde9bb4cda74c069522d3dca927720e",
      "ef51797d549445df82f5e37e1b33d25e",
      "c4173a0a08d14565b96943ee428c3c37",
      "0b34ed4c30a84f23821d4d6b099d3983",
      "0bbbee9109044bfdabfbc909fc3eb24c",
      "dca27add69b74eed92f435770a701cfd",
      "85561c08f85547d881fe636d916800e3",
      "2e6ca48621884e4283fc0e3a9b2d6293",
      "dda0304e45f14ed79fc72199fa3513b2",
      "14224e6531984e3fbe6482b7f65f98cf",
      "9c470bb938414ce18fb47c19feffc59f",
      "3eb2348ddcce4d459963f134ee308032",
      "25d6a1178ca0412198b6ae760226fe96",
      "0dd55454388a4852b757cb80917d39ba",
      "8edd819f814340b5a22cbb9d559f9896",
      "d5dad8717a5e492f8bcb542bac04723d",
      "5f073c292e3f48aab29dad48b4a293e3",
      "e2af7acb196243dc976de9146f7ac868",
      "1747779592844a848d7759664d88f3ce",
      "2247952a42874a8c8183d2d209b498a1",
      "d1126f14a72d42f8bda77b0fd19828a7",
      "6e6f0ccac09b492f891a960e7a3b33d0",
      "06d8d7e7f5734ef48e29fb2a9af1beb0",
      "1c8b100829bf4eda89b895e490688993",
      "ca575a4266d847198aff57b17d99bc26",
      "0af1b918f3524cc69b82717487dc7c7f",
      "16348bcb39e24f3bba1f0e4bdf8d9ffd",
      "1c25b326bc5d4ad5b0bf006c6096d805",
      "aacc7fe045924d1fadf1b513ca958420",
      "df458098f26c4567a0149bc975940726",
      "c13b0ad94dc64b01aa3a5bcc1365b3f5",
      "c43cd2c2221b4c07a3aeeabdc56a8b62",
      "507dc04548aa4286924b17ec4087a7f1",
      "4c2e17bd10584a388b13cd943ac3b173",
      "2645b7da0c0e47bfb18cc7f4dcc9ed53",
      "692ef209a3804b468a96ed62bdfa3351",
      "11b0868a202f40459a1241ecf983432b",
      "13c69e0e2076443ab5b3905d10bbf98b",
      "065ea327a9f540158fcddf2fc507491b",
      "7471d77221a84d818e26f61dd857402a",
      "df7fc469c3bf4e58b587f7cffa55dcea",
      "55c97d7ac752474bab9cda64b7347aa3",
      "acd8a01699b940dfa6f51428030e6d67",
      "1a09564f32c74f7480fb8e16871a78a7",
      "96bdd6e9d05f4e4e89bdf245695b052d",
      "1ac919024bbe44b5b7e3511e7f4f83b6",
      "1fd38e62726f43a1af4fc19b2fbeb8a0",
      "e2af7715a451412b9cceea20df674b16"
     ]
    },
    "id": "b48098d7",
    "outputId": "691ff322-69fc-40d9-c2ce-68d7ae23cecc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87599d7f2b264aab92680d59a0aa654b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d11f10d218c4b59a41d1db3d3450270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0e18d30d7e46bd81365c8a1680b97f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbbee9109044bfdabfbc909fc3eb24c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5dad8717a5e492f8bcb542bac04723d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16348bcb39e24f3bba1f0e4bdf8d9ffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c69e0e2076443ab5b3905d10bbf98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True).cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eabb76e",
   "metadata": {
    "id": "2eabb76e"
   },
   "source": [
    "This function is used to build the prompts in each of the approaches. It tries to give a well structured prompt with clearly separated examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f794f4",
   "metadata": {
    "id": "c6f794f4"
   },
   "outputs": [],
   "source": [
    "def build_prompt_deepseek(df, index, mode='zero-shot', num_few_shot=3, random_state=0):\n",
    "    problem = clean_problem_description(df.loc[index, 'problem_description'])\n",
    "    if mode == 'zero-shot':\n",
    "        prompt = f\"# Task: {problem}\\n# Solution:\\n\"\n",
    "\n",
    "    elif mode == 'one-shot':\n",
    "        example_idx = df.drop(index).sample(1, random_state=random_state).index[0]\n",
    "        example_problem = clean_problem_description(df.loc[example_idx, 'problem_description'])\n",
    "        example_solution = df.loc[example_idx, 'solution_code']\n",
    "        prompt = (\n",
    "            f\"# === Example 1 ===\\n\"\n",
    "            f\"# Task: {example_problem}\\n# Solution:\\n{example_solution}\\n\\n\"\n",
    "            f\"# Task: {problem}\\n# Solution:\\n\"\n",
    "        )\n",
    "\n",
    "    elif mode == 'few-shot':\n",
    "        example_indices = df.drop(index).sample(num_few_shot, random_state=random_state).index\n",
    "        prompt = \"\"\n",
    "        for j, i in enumerate(example_indices):\n",
    "            example_problem = clean_problem_description(df.loc[i, 'problem_description'])\n",
    "            example_solution = df.loc[i, 'solution_code']\n",
    "            prompt += (\n",
    "                f\"# === Example {j+1} ===\\n\"\n",
    "                f\"# Task: {example_problem}\\n\"\n",
    "                f\"# Solution:\\n{example_solution}\\n\\n\"\n",
    "            )\n",
    "        prompt += f\"# Task: {problem}\\n# Solution:\\n\"\n",
    "\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e5c64f",
   "metadata": {
    "id": "61e5c64f"
   },
   "source": [
    "This function is used to generate the model output using the prompt build before and returns the string containing the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d5981",
   "metadata": {
    "id": "7c2d5981"
   },
   "outputs": [],
   "source": [
    "def generate_solution_deepseek(prompt, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410e2f54",
   "metadata": {
    "id": "410e2f54"
   },
   "source": [
    "We will use the same problem example as before for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1119524",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1119524",
    "outputId": "e85e4f64-871b-4a46-ed39-299b59819b98"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(14480)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a164ba12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "id": "a164ba12",
    "outputId": "093ae257-3f0b-4bc5-cc7b-23acf6cc0fb2"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"example\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"problem_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"There are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n\\nExamples\\n\\nInput\\n\\n3 2\\n2 1\\n\\n\\nOutput\\n\\n1\\n\\n\\nInput\\n\\n5 5\\n2 3\\n\\n\\nOutput\\n\\n6\\n\\n\\nInput\\n\\n2 4\\n2 4\\n\\n\\nOutput\\n\\n0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"solution_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"190_3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"solution_code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"a,b=map(int,input().split())\\nc,d=map(int,input().split())\\nprint((a-c)*(b-d))\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"p03101 AtCoder Beginner Contest 121 - White Cells\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_complexity_inferred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"O(1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"space_complexity_inferred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"O(1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "example"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-b40aa2f5-312a-480f-b2ab-2b28faa6576c\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14480</th>\n",
       "      <td>There are H rows and W columns of white square...</td>\n",
       "      <td>190_3</td>\n",
       "      <td>a,b=map(int,input().split())\\nc,d=map(int,inpu...</td>\n",
       "      <td>p03101 AtCoder Beginner Contest 121 - White Cells</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b40aa2f5-312a-480f-b2ab-2b28faa6576c')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-b40aa2f5-312a-480f-b2ab-2b28faa6576c button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-b40aa2f5-312a-480f-b2ab-2b28faa6576c');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "  <div id=\"id_6302cffe-d1ae-4ec1-ac1e-4b2154f0db47\">\n",
       "    <style>\n",
       "      .colab-df-generate {\n",
       "        background-color: #E8F0FE;\n",
       "        border: none;\n",
       "        border-radius: 50%;\n",
       "        cursor: pointer;\n",
       "        display: none;\n",
       "        fill: #1967D2;\n",
       "        height: 32px;\n",
       "        padding: 0 0 0 0;\n",
       "        width: 32px;\n",
       "      }\n",
       "\n",
       "      .colab-df-generate:hover {\n",
       "        background-color: #E2EBFA;\n",
       "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "        fill: #174EA6;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate {\n",
       "        background-color: #3B4455;\n",
       "        fill: #D2E3FC;\n",
       "      }\n",
       "\n",
       "      [theme=dark] .colab-df-generate:hover {\n",
       "        background-color: #434B5C;\n",
       "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "        fill: #FFFFFF;\n",
       "      }\n",
       "    </style>\n",
       "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('example')\"\n",
       "            title=\"Generate code using this dataframe.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "    <script>\n",
       "      (() => {\n",
       "      const buttonEl =\n",
       "        document.querySelector('#id_6302cffe-d1ae-4ec1-ac1e-4b2154f0db47 button.colab-df-generate');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      buttonEl.onclick = () => {\n",
       "        google.colab.notebook.generateWithVariable('example');\n",
       "      }\n",
       "      })();\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                     problem_description solution_id  \\\n",
       "14480  There are H rows and W columns of white square...       190_3   \n",
       "\n",
       "                                           solution_code  \\\n",
       "14480  a,b=map(int,input().split())\\nc,d=map(int,inpu...   \n",
       "\n",
       "                                            problem_name  \\\n",
       "14480  p03101 AtCoder Beginner Contest 121 - White Cells   \n",
       "\n",
       "      time_complexity_inferred space_complexity_inferred  \n",
       "14480                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab25342",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "6ab25342",
    "outputId": "e6ca4146-f6a7-4913-ab18-8856ef8e7794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Task: There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "# Solution:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zero_shot_prompt = build_prompt_deepseek(df, example.index[0], mode='zero-shot')\n",
    "print(zero_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a310f686",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a310f686",
    "outputId": "6888c60c-9347-43b2-a95d-fbcaabe37bbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Task: There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "# Solution:\n",
      "\n",
      "\n",
      "def white_cells(H, W, h, w):\n",
      "    # Write your code here\n",
      "    return (H - h) * (W - w)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    H, W = map(int, input().split())\n",
      "    h, w = map(int, input().split())\n",
      "    print(white_cells(H, W, h, w))\n"
     ]
    }
   ],
   "source": [
    "zero_shot_generated = generate_solution_deepseek(zero_shot_prompt)\n",
    "print(zero_shot_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e23bafb",
   "metadata": {
    "id": "2e23bafb"
   },
   "source": [
    "The code generated by the zero-shot approach was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72034771",
   "metadata": {
    "id": "72034771"
   },
   "outputs": [],
   "source": [
    "def white_cells(H, W, h, w):\n",
    "    # Write your code here\n",
    "    return (H - h) * (W - w)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    H, W = map(int, input().split())\n",
    "    h, w = map(int, input().split())\n",
    "    print(white_cells(H, W, h, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134fbb3e",
   "metadata": {
    "id": "134fbb3e"
   },
   "source": [
    "Which correctly solves the problem proposed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704dbd2e",
   "metadata": {
    "id": "704dbd2e"
   },
   "source": [
    "TinyLlama, being a general-purpose language model with fewer parameters and less specialized training on code, may struggle with understanding problem constraints and generating correct, concise solutions. In contrast, DeepSeek-Coder is specifically trained on diverse programming tasks and code generation, enabling it to better grasp problem requirements and produce accurate, efficient code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a81901",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "e0a81901",
    "outputId": "7ee7878f-9c89-41ee-e9d1-fe7b142204b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# === Example 1 ===\n",
      "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "# Solution:\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))\n",
      "\n",
      "# Task: There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "# Solution:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "one_shot_prompt = build_prompt_deepseek(df, example.index[0], mode='one-shot')\n",
    "print(one_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9f962c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1d9f962c",
    "outputId": "4426d809-43c0-49f2-e46b-3bfe0e0c12f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# === Example 1 ===\n",
      "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "# Solution:\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))\n",
      "\n",
      "# Task: There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "# Solution:\n",
      "H, W = map(int, input().split())\n",
      "h, w = map(int, input().split())\n",
      "print((H - h + 1) * (W - w + 1) - len(set(input().split())))\n"
     ]
    }
   ],
   "source": [
    "one_shot_generated = generate_solution_deepseek(one_shot_prompt)\n",
    "print(one_shot_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7670377a",
   "metadata": {
    "id": "7670377a"
   },
   "source": [
    "The solution generated by the one-shot prompt was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6303e59",
   "metadata": {
    "id": "d6303e59"
   },
   "outputs": [],
   "source": [
    "# Solution:\n",
    "H, W = map(int, input().split())\n",
    "h, w = map(int, input().split())\n",
    "print((H - h + 1) * (W - w + 1) - len(set(input().split())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56dc6d5",
   "metadata": {
    "id": "a56dc6d5"
   },
   "source": [
    "The generated solution overcomplicates the problem by using an incorrect formula and expecting extra input that is not required. This likely happened because the one-shot example used in the prompt added noise or confusion, leading the model to misinterpret the problemâ€™s requirements and produce a more complex, incorrect solution than necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0dde48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "1a0dde48",
    "outputId": "42585ff4-648c-40f7-8506-4b583d541c0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# === Example 1 ===\n",
      "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "# Solution:\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))\n",
      "\n",
      "# === Example 2 ===\n",
      "# Task: In some other world, today is the day before Christmas Eve.\n",
      "\n",
      "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
      "\n",
      "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 \\leq N \\leq 10\n",
      "* 100 \\leq p_i \\leq 10000\n",
      "* p_i is an even number.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "N\n",
      "p_1\n",
      "p_2\n",
      ":\n",
      "p_N\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the total amount Mr. Takaha will pay.\n",
      "# Solution:\n",
      "N=int(input())\n",
      "P=sorted([int(input()) for _ in range(N)])\n",
      "print(sum(P)-P[-1]//2)\n",
      "\n",
      "# === Example 3 ===\n",
      "# Task: Two polar bears Menshykov and Uslada from the St.Petersburg zoo and elephant Horace from the Kiev zoo got six sticks to play with and assess the animals' creativity. Menshykov, Uslada and Horace decided to make either an elephant or a bear from those sticks. They can make an animal from sticks in the following way: \n",
      "\n",
      "  * Four sticks represent the animal's legs, these sticks should have the same length. \n",
      "  * Two remaining sticks represent the animal's head and body. The bear's head stick must be shorter than the body stick. The elephant, however, has a long trunk, so his head stick must be as long as the body stick. Note that there are no limits on the relations between the leg sticks and the head and body sticks. \n",
      "\n",
      "\n",
      "\n",
      "Your task is to find out which animal can be made from the given stick set. The zoo keeper wants the sticks back after the game, so they must never be broken, even bears understand it.\n",
      "\n",
      "Input\n",
      "\n",
      "The single line contains six space-separated integers li (1 â‰¤ li â‰¤ 9) â€” the lengths of the six sticks. It is guaranteed that the input is such that you cannot make both animals from the sticks.\n",
      "\n",
      "Output\n",
      "\n",
      "If you can make a bear from the given set, print string \"Bear\" (without the quotes). If you can make an elephant, print string \"Elephant\" (wÄ±thout the quotes). If you can make neither a bear nor an elephant, print string \"Alien\" (without the quotes).\n",
      "# Solution:\n",
      "import sys\n",
      "import math\n",
      "import itertools\n",
      "import collections\n",
      "\n",
      "\n",
      "def divs(n, start=1):\n",
      "    r = []\n",
      "    for i in range(start, int(math.sqrt(n) + 1)):\n",
      "        if (n % i == 0):\n",
      "            if (n / i == i):\n",
      "                r.append(i)\n",
      "            else:\n",
      "                r.extend([i, n // i])\n",
      "    return r\n",
      "def cdiv(n, k): return n // k + (n % k != 0)\n",
      "def ii(): return int(input())\n",
      "def mi(): return map(int, input().split())\n",
      "def li(): return list(map(int, input().split()))\n",
      "def lcm(a, b): return abs(a * b) // math.gcd(a, b)\n",
      "def wr(arr): return ' '.join(map(str, arr))\n",
      "def revn(n): return int(str(n)[::-1])\n",
      "def prime(n):\n",
      "    if n == 2: return True\n",
      "    if n % 2 == 0 or n <= 1: return False\n",
      "    sqr = int(math.sqrt(n)) + 1\n",
      "    for d in range(3, sqr, 2):\n",
      "        if n % d == 0: return False\n",
      "    return True\n",
      "def convn(number, base=3):\n",
      "    newnumber = ''\n",
      "    while number > 0:\n",
      "        newnumber = str(number % base) + newnumber\n",
      "        number //= base\n",
      "    return newnumber\n",
      "\n",
      "\n",
      "l = li()\n",
      "lc = collections.Counter(l)\n",
      "bear = [[4, 1, 1], [5, 1], [1, 5], [1, 4, 1], [1, 1, 4]]\n",
      "elephant = [[6], [4, 2], [2, 4]]\n",
      "if list(lc.values()) in bear:\n",
      "    print('Bear')\n",
      "elif list(lc.values()) in elephant:\n",
      "    print('Elephant')\n",
      "else:\n",
      "    print('Alien')\n",
      "\n",
      "# Task: There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "# Solution:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "few_shot_prompt = build_prompt_deepseek(df, example.index[0], mode='few-shot')\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00d936",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff00d936",
    "outputId": "2d5cb0ac-b28f-4e9e-efdb-2bef958f2bc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# === Example 1 ===\n",
      "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "# Solution:\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))\n",
      "\n",
      "# === Example 2 ===\n",
      "# Task: In some other world, today is the day before Christmas Eve.\n",
      "\n",
      "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
      "\n",
      "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 \\leq N \\leq 10\n",
      "* 100 \\leq p_i \\leq 10000\n",
      "* p_i is an even number.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "N\n",
      "p_1\n",
      "p_2\n",
      ":\n",
      "p_N\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the total amount Mr. Takaha will pay.\n",
      "# Solution:\n",
      "N=int(input())\n",
      "P=sorted([int(input()) for _ in range(N)])\n",
      "print(sum(P)-P[-1]//2)\n",
      "\n",
      "# === Example 3 ===\n",
      "# Task: Two polar bears Menshykov and Uslada from the St.Petersburg zoo and elephant Horace from the Kiev zoo got six sticks to play with and assess the animals' creativity. Menshykov, Uslada and Horace decided to make either an elephant or a bear from those sticks. They can make an animal from sticks in the following way: \n",
      "\n",
      "  * Four sticks represent the animal's legs, these sticks should have the same length. \n",
      "  * Two remaining sticks represent the animal's head and body. The bear's head stick must be shorter than the body stick. The elephant, however, has a long trunk, so his head stick must be as long as the body stick. Note that there are no limits on the relations between the leg sticks and the head and body sticks. \n",
      "\n",
      "\n",
      "\n",
      "Your task is to find out which animal can be made from the given stick set. The zoo keeper wants the sticks back after the game, so they must never be broken, even bears understand it.\n",
      "\n",
      "Input\n",
      "\n",
      "The single line contains six space-separated integers li (1 â‰¤ li â‰¤ 9) â€” the lengths of the six sticks. It is guaranteed that the input is such that you cannot make both animals from the sticks.\n",
      "\n",
      "Output\n",
      "\n",
      "If you can make a bear from the given set, print string \"Bear\" (without the quotes). If you can make an elephant, print string \"Elephant\" (wÄ±thout the quotes). If you can make neither a bear nor an elephant, print string \"Alien\" (without the quotes).\n",
      "# Solution:\n",
      "import sys\n",
      "import math\n",
      "import itertools\n",
      "import collections\n",
      "\n",
      "\n",
      "def divs(n, start=1):\n",
      "    r = []\n",
      "    for i in range(start, int(math.sqrt(n) + 1)):\n",
      "        if (n % i == 0):\n",
      "            if (n / i == i):\n",
      "                r.append(i)\n",
      "            else:\n",
      "                r.extend([i, n // i])\n",
      "    return r\n",
      "def cdiv(n, k): return n // k + (n % k != 0)\n",
      "def ii(): return int(input())\n",
      "def mi(): return map(int, input().split())\n",
      "def li(): return list(map(int, input().split()))\n",
      "def lcm(a, b): return abs(a * b) // math.gcd(a, b)\n",
      "def wr(arr): return ' '.join(map(str, arr))\n",
      "def revn(n): return int(str(n)[::-1])\n",
      "def prime(n):\n",
      "    if n == 2: return True\n",
      "    if n % 2 == 0 or n <= 1: return False\n",
      "    sqr = int(math.sqrt(n)) + 1\n",
      "    for d in range(3, sqr, 2):\n",
      "        if n % d == 0: return False\n",
      "    return True\n",
      "def convn(number, base=3):\n",
      "    newnumber = ''\n",
      "    while number > 0:\n",
      "        newnumber = str(number % base) + newnumber\n",
      "        number //= base\n",
      "    return newnumber\n",
      "\n",
      "\n",
      "l = li()\n",
      "lc = collections.Counter(l)\n",
      "bear = [[4, 1, 1], [5, 1], [1, 5], [1, 4, 1], [1, 1, 4]]\n",
      "elephant = [[6], [4, 2], [2, 4]]\n",
      "if list(lc.values()) in bear:\n",
      "    print('Bear')\n",
      "elif list(lc.values()) in elephant:\n",
      "    print('Elephant')\n",
      "else:\n",
      "    print('Alien')\n",
      "\n",
      "# Task: There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "# Solution:\n",
      "H,W=map(int,input().split())\n",
      "h,w=map(int,input().split())\n",
      "print((H-h)*(W-w))\n",
      "\n",
      "# === Example 4 ===\n",
      "# Task: You are given a sequence of N integers.\n",
      "\n",
      "Your task is to find the smallest sub-sequence of these integers, such that it is not possible to split the sequence into two sequences of equal sums.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer N (1 \\leq N \\leq 50000) â€” the number of elements in the sequence.\n",
      "\n",
      "The second line contains N integers A1, A2, ..., AN (1 \\leq A_i \\leq 10^9) â€” the elements of the sequence.\n",
      "\n",
      "Output\n",
      "\n",
      "Print the length of the smallest sub-sequence, such that it is not possible to split the sequence into two sequences of equal sums.\n",
      "# Solution:\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "s = sum(a)\n",
      "if s & 1:\n",
      "    print(1)\n",
      "    exit()\n",
      "\n",
      "s //= 2\n",
      "\n",
      "a.sort(reverse=True)\n",
      "\n",
      "for i in range(N):\n",
      "    s -= a[i]\n",
      "    if s <= 0:\n",
      "        print(i + 1)\n",
      "        exit()\n",
      "print(N)\n",
      "\n",
      "# === Example 5 ===\n",
      "# Task: You are given a sequence of N integers.\n",
      "\n",
      "Your task is to find the smallest sub-sequence of these integers, such that it is not possible to split the sequence into two sequences of equal sums.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer N (1 \\leq N \\leq 50000) â€” the number of elements in the sequence.\n",
      "\n",
      "The second line contains N integers A1, A2, ..., AN (1 \\leq A_i \\leq 10^9) â€” the elements of the sequence.\n",
      "\n",
      "Output\n",
      "\n",
      "Print the length of the smallest sub-sequence, such that it is not possible to split the sequence into two sequences of equal sums.\n",
      "# Solution:\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "s = sum(a)\n",
      "if s & 1:\n"
     ]
    }
   ],
   "source": [
    "few_shot_generated = generate_solution_deepseek(few_shot_prompt)\n",
    "print(few_shot_generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f31a28",
   "metadata": {
    "id": "87f31a28"
   },
   "source": [
    "The code generated by the model was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cb2fb9",
   "metadata": {
    "id": "12cb2fb9"
   },
   "outputs": [],
   "source": [
    "H,W=map(int,input().split())\n",
    "h,w=map(int,input().split())\n",
    "print((H-h)*(W-w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed7357",
   "metadata": {
    "id": "88ed7357"
   },
   "source": [
    "Which correctly solves the problem, and actually is almost the same one as the one provided in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338df1c7",
   "metadata": {
    "id": "338df1c7"
   },
   "outputs": [],
   "source": [
    "a,b=map(int,input().split())\n",
    "c,d=map(int,input().split())\n",
    "print((a-c)*(b-d))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb8b20",
   "metadata": {
    "id": "bffb8b20"
   },
   "source": [
    "But we can see that after the model continued generating new examples and solution codes, like it was continuing the prompt. Long or confusing examples can cause the model to not recognize where to end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eedcfd4",
   "metadata": {
    "id": "5eedcfd4"
   },
   "outputs": [],
   "source": [
    "results_deepseek = {'zero-shot': [], 'one-shot': [], 'few-shot': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd74482",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddd74482",
    "outputId": "b04bc510-4606-4f45-b0ed-c780c420302b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode: zero-shot, Example 219235\n",
      "bleu: 3.1388\n",
      "rouge1: 0.0556\n",
      "rougeL: 0.0556\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 116625\n",
      "bleu: 2.3394\n",
      "rouge1: 0.1863\n",
      "rougeL: 0.1242\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 161440\n",
      "bleu: 8.3381\n",
      "rouge1: 0.1355\n",
      "rougeL: 0.1030\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 177012\n",
      "bleu: 11.4481\n",
      "rouge1: 0.2696\n",
      "rougeL: 0.1569\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: zero-shot, Example 26857\n",
      "bleu: 14.0492\n",
      "rouge1: 0.3170\n",
      "rougeL: 0.1445\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 219235\n",
      "bleu: 1.2779\n",
      "rouge1: 0.0173\n",
      "rougeL: 0.0173\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 116625\n",
      "bleu: 2.8054\n",
      "rouge1: 0.0677\n",
      "rougeL: 0.0558\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 161440\n",
      "bleu: 5.9441\n",
      "rouge1: 0.1477\n",
      "rougeL: 0.0937\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 177012\n",
      "bleu: 11.4539\n",
      "rouge1: 0.2831\n",
      "rougeL: 0.1788\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: one-shot, Example 26857\n",
      "bleu: 4.1924\n",
      "rouge1: 0.1988\n",
      "rougeL: 0.0906\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 219235\n",
      "bleu: 0.6392\n",
      "rouge1: 0.0110\n",
      "rougeL: 0.0110\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 116625\n",
      "bleu: 1.7252\n",
      "rouge1: 0.0311\n",
      "rougeL: 0.0293\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 161440\n",
      "bleu: 1.9019\n",
      "rouge1: 0.0753\n",
      "rougeL: 0.0513\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 177012\n",
      "bleu: 5.6718\n",
      "rouge1: 0.1425\n",
      "rougeL: 0.1092\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n",
      "Mode: few-shot, Example 26857\n",
      "bleu: 3.5124\n",
      "rouge1: 0.1543\n",
      "rougeL: 0.0681\n",
      "exact_match: 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_results_deepseek = []\n",
    "\n",
    "for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
    "    for idx in sample_indices:\n",
    "        prompt = build_prompt_deepseek(df, idx, mode=mode, num_few_shot=3)\n",
    "        generated = generate_solution_deepseek(prompt, max_new_tokens=256)\n",
    "\n",
    "        expected = df.loc[idx, 'solution_code']\n",
    "\n",
    "        metrics = evaluate_metrics(expected, generated)\n",
    "\n",
    "        print(f\"Mode: {mode}, Example {idx}\")\n",
    "        #print(\"Generated:\\n\", generated)\n",
    "        #print(\"Expected:\\n\", expected)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "        print(\"\\n\")\n",
    "\n",
    "        results_deepseek[mode].append(metrics)\n",
    "\n",
    "        saved_results_deepseek.append({\n",
    "            'mode': mode,\n",
    "            'index': idx,\n",
    "            'problem_description': df.loc[idx, 'problem_description'],\n",
    "            'expected_solution': expected,\n",
    "            'generated_solution': generated,\n",
    "            'bleu': metrics['bleu'],\n",
    "            'rouge1': metrics['rouge1'],\n",
    "            'rougeL': metrics['rougeL'],\n",
    "            'exact_match': metrics['exact_match']\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93295bd4",
   "metadata": {
    "id": "93295bd4"
   },
   "source": [
    "Now we will run the three prompting approaches with a few samples to get some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd723c00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bd723c00",
    "outputId": "93ffb96a-d59a-46c1-de52-17cfded1f1f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average metrics for zero-shot:\n",
      "bleu: 7.8627\n",
      "rouge1: 0.1928\n",
      "rougeL: 0.1168\n",
      "exact_match: 0.0000\n",
      "\n",
      "Average metrics for one-shot:\n",
      "bleu: 5.1347\n",
      "rouge1: 0.1429\n",
      "rougeL: 0.0872\n",
      "exact_match: 0.0000\n",
      "\n",
      "Average metrics for few-shot:\n",
      "bleu: 2.6901\n",
      "rouge1: 0.0828\n",
      "rougeL: 0.0537\n",
      "exact_match: 0.0000\n"
     ]
    }
   ],
   "source": [
    "for mode in results_deepseek:\n",
    "    avg_metrics = {}\n",
    "    for metric in results_deepseek[mode][0].keys():\n",
    "        avg_metrics[metric] = sum([ex[metric] for ex in results_deepseek[mode]]) / len(results_deepseek[mode])\n",
    "\n",
    "    print(f\"\\nAverage metrics for {mode}:\")\n",
    "    for metric, value in avg_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac9424c",
   "metadata": {
    "id": "6ac9424c"
   },
   "source": [
    "Again we have worse metrics for the few-shot approach than to the other two. This can also be caused by the model continuing the generation after the solution, like the example above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44df4c5",
   "metadata": {
    "id": "a44df4c5"
   },
   "outputs": [],
   "source": [
    "df_results_deepseek = pd.DataFrame(saved_results_deepseek)\n",
    "df_results_deepseek.to_csv('generated_codes_deepseek.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9083ef",
   "metadata": {
    "id": "dc9083ef"
   },
   "outputs": [],
   "source": [
    "with open('generated_codes_deepseek.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(saved_results_deepseek, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a970c6f",
   "metadata": {
    "id": "3a970c6f"
   },
   "source": [
    "## Try Mistral-7B-Instruct-v0.1 on the task\n",
    "\n",
    "Mistral-7B-Instruct-v0.1 is a 7-billion parameter language model designed for instruction-following tasks. It offers strong performance in natural language understanding and generation. Due to its large size, we will use quantization techniques to reduce memory usage and enable efficient inference on limited hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c3887",
   "metadata": {
    "id": "951c3887"
   },
   "source": [
    "This code configures 4-bit quantization for loading a model with the BitsAndBytes library. It specifies that the model weights should be loaded in 4-bit precision to reduce memory usage, using the \"nf4\" quantization type and double quantization for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d990f0",
   "metadata": {
    "id": "48d990f0"
   },
   "outputs": [],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ab7f8",
   "metadata": {
    "id": "557ab7f8"
   },
   "source": [
    "To access the model we need to login to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310d417",
   "metadata": {
    "id": "7310d417"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "#token = userdata.get('HF_TOKEN')\n",
    "login(token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e78108",
   "metadata": {
    "id": "b0e78108"
   },
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b004e62",
   "metadata": {
    "id": "0b004e62"
   },
   "source": [
    "This code loads the Mistral-7B-Instruct-v0.1 model with 4-bit quantization enabled. The device_map=\"auto\" argument automatically assigns model layers to available hardware devices. The tokenizer is also loaded to preprocess input text for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c3f04",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "3384d61f63c64d17bdb40bc405314880",
      "e18e4ee8448f42ea944049040aadd60c",
      "ab9d0aaa48fc47b89e75fb4395c07964",
      "ee9d4f234fda405b83af5a3629cddbb3",
      "3cb6ba4e79ef4dde9a4e6f3768bcbca2",
      "e3a60341b04f43fba980f77fcf20a46d",
      "ff8ab8a323f64c3193dfdc19d7a7a5c7",
      "176de74294a14c2ea789e90b75983448",
      "12ed00aa654443768b8277095cd726a9",
      "8668b74f4b9744b8b98db0a42b271e3e",
      "ff95a671b80c48ffa045689b8fa5fc9b",
      "75241bb3aae242fdacb8cbaf95c4ddb0",
      "4565ee3ae7574462a4a29b77ef1b8fa5",
      "f024e287558e4c25ac02fff799de63b9",
      "c05cd1d54d6e48d79eec56499548ac95",
      "e7d3766ce9cc4d9baf1d143388ab77a2",
      "c993e259296d46409d4e93acf6e75544",
      "4793974e1a3e44ad9a85a229014b1edf",
      "22d57a641f5f4392ba9abe3620258216",
      "7401d9f1cce9479aa51d135937a68adf",
      "b5f2e11f076449a5ae123582bc94ead9",
      "7a877df4fc134cd0ac31c56df74fd1a7",
      "cad656b33c304d92baea228b30ddae3c",
      "afb4f7af30eb497fa6e27bc66728b042",
      "39f04187839e478281eaabf09f9ede58",
      "2b640a27dd3a4e08839b8a48b527ab69",
      "2008dd8b6ba94ae29111c619824344cd",
      "983014bd0f6544e2bbd61c5bc76c014c",
      "7bdb186ae7e04a8ab63c4e6c2d138027",
      "a6070bd65a11466dab43bdf6815949e4",
      "0b7800c30d884b28a774617a9bc0eea9",
      "a6da197786fa40eb960058cfae680d83",
      "a7db086951ab4a29b30f9ccd4ed52aa3",
      "def67dfcab454ce0a5fb96d7c2983d2c",
      "ebea9d2b834444a9b889a7378fdf08c6",
      "e52f594eb0ed4985a342f2814fb43b8b",
      "032596a109a540648165a28021030451",
      "23635d4a82ca47e59e60bd334017c8b5",
      "70bb0495aa73466996ffac2e748d357b",
      "5f0a12774bbd496fb12e441e0ce8effc",
      "a8eedcad300343909db6750365300a86",
      "916f80ed141a47faa9cac5187060f5f1",
      "db71e19525ea4a21a8703e5c7691d15a",
      "b6f513393bd840ccb68744f82df56874",
      "d3a251370e9d440394bf33bc3eceee74",
      "54165e88c017426c8930e1623774abf4",
      "1375379f7532430983cb9cd9051c8639",
      "cf298e6bd4134e73a1c35c0b1aa2b7c1",
      "ea6caaeb66624a53a29c33b9be1b8753",
      "4f746f112fc24a1e979f7ce038d1bb48",
      "077e1aa6110f44dbbebcd61a7be61bb6",
      "95e0f854301945fbb6a2e285c7d70570",
      "60adc2be8a6244d2bd167a160687e46c",
      "e078f08ced5c4ff7bcc755ae51c0b372",
      "339232950e27416fa90c5ea4869baa0d",
      "2cc0bc868d7340d3ba025b8f66cd9430",
      "09e0ccce62f94574902eda5635cd247c",
      "c4e818a77ea347e2bc2feee5ce8c278c",
      "0db0e80120ca452391d5b783523209aa",
      "2f3b92bac6c64db7af495b285f854e40",
      "dc9730ece2fa45d9b6215cc6c8608575",
      "107cda048d9f4586a9da74601c80ad17",
      "ac3c34f0903c44579842d14a4fbf45cf",
      "a36fa17fad9b412f9b6248476c314971",
      "5e3388886a974857bbd9065b3de0786d",
      "99629dffc898455f9f556e4496b55d06",
      "8c0dd24b7fec4ac4a2af4596cd7f5a67",
      "0dfd1eb077ac4e91a71e865237a42893",
      "989b6c461e754629af2856e1dee8a79f",
      "bcbda69191404770a18147575a1051a2",
      "6b1f2c9be5cc4303a0e9d87f4e88e79b",
      "c9233ade66f04fd1835ad8111920669f",
      "ed4dc1dd5758486099a86103639b2f1f",
      "3a5881c4df64427e828b8331d50c071c",
      "ef431faa747f4c92bb36d924bef64633",
      "c69e707976ae40278242622b74df606d",
      "72621ace1f8e4cd9b4aac2773a29af68",
      "ea7111607f634c71afe03abd06a26bdd",
      "88d37713083b4eaab021d78a5b2ce62f",
      "4cffca5653f3497385fbf3ecd6ed6d92",
      "361c4a0358194738a7af4130f1fbe2ba",
      "96b72fdc21c542fdb99b9815e2e2cdfa",
      "77ece29939c5496b8921b11dce4781d0",
      "a462543be7fd48d89b2bed824702407e",
      "7836f2c6accf4acd8267a2dd772e5570",
      "6723d69f303948279e53368b807b7c38",
      "cd352734cf0f48d48614bc9d2aec7909",
      "14d369d7e60a41d8924259f88bbfa2e0",
      "7465fc91e3ff4e16a11021d71ae8869b",
      "6c6e62d38f1a49e793a278b879d48d4b",
      "4e9dba9a115d4ee79f3a56cb3b446ccf",
      "4dea893c12d64a58a0881b67c9763122",
      "833aab080c2a479dbf5f1502028a8bec",
      "cc102fcda6b64f4c9f887aa716d3562b",
      "659d901f9f98403892cfa8b83a20c13e",
      "81d57d1969894d44b39cbeefc634c0c9",
      "5d81e494b2884a19bd873426672a8133",
      "2defc979bd044d35b33582593e36a1a9",
      "3b7d807dd3c54a6a9a1440ec854fb739",
      "1712e0119bcf42bd970cae44807073e2",
      "c4332b8193df47dc8fcf349fbccbb240",
      "b244650efcc34247a3eb1f74ab958c6c",
      "408631494fe04b90936baeff345a2caa",
      "889df641c07943a9ade39564ab8ce2b7",
      "ccf5205381db4978b9b651e2c4b2d1c4",
      "3ae22d1fa8d24cc189da82babfc56448",
      "2c3f2d45db00424cae837497ba49c370",
      "84718ef9c2b84b7c88b995e34b77f5ed",
      "aa1bd22aef6d430f983a294086d16aa4",
      "9c95eaf48062416ab39e12365e4add07",
      "ed6057671fcf4160be547173faed5f6b",
      "e786cf4b941c41b58099401648d1fbe8",
      "e9c75d3a969240f2b2594b610ab2ef97",
      "37b9a9aec392407b8a531a4ad202394f",
      "9521ad40645c47abbf1ce4afb8375133",
      "f268cec84fd24752b628961283b49741",
      "b4972ecbc5514657a67e25033a6c6ee4",
      "e6edd3a9551f4a3a9a62d332e1ed5a2f",
      "da430e728b904c36b63c8cc1f2f230d3",
      "851c71eefee540db9407d99083a07d10",
      "ced1ee5fcd2b47bf9e37eab50602d879"
     ]
    },
    "id": "260c3f04",
    "outputId": "d99bf532-ab97-4675-fa6f-151047c42627"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3384d61f63c64d17bdb40bc405314880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75241bb3aae242fdacb8cbaf95c4ddb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad656b33c304d92baea228b30ddae3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def67dfcab454ce0a5fb96d7c2983d2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a251370e9d440394bf33bc3eceee74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc0bc868d7340d3ba025b8f66cd9430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c0dd24b7fec4ac4a2af4596cd7f5a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea7111607f634c71afe03abd06a26bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7465fc91e3ff4e16a11021d71ae8869b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1712e0119bcf42bd970cae44807073e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed6057671fcf4160be547173faed5f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",quantization_config=quantization_config, )\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad9adf",
   "metadata": {
    "id": "dbad9adf"
   },
   "source": [
    "Now we create a text-generation pipeline using the 4-bit quantized Mistral model and its tokenizer. We configure generation parameters like maximum output length, sampling with top-k filtering, and device placement. The pipeline is then wrapped in a HuggingFacePipeline object for easy integration with other tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cd1f48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80cd1f48",
    "outputId": "597fe22f-073b-46bd-8aee-6b815a70500d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "<ipython-input-64-81eff7bed817>:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipeline_inst)\n"
     ]
    }
   ],
   "source": [
    "pipeline_inst = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_4bit,\n",
    "        tokenizer=tokenizer,\n",
    "        use_cache=True,\n",
    "        device_map=\"auto\",\n",
    "        max_length=2500,\n",
    "        truncation=True,\n",
    "        do_sample=True,\n",
    "        top_k=5,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipeline_inst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79808b9",
   "metadata": {
    "id": "a79808b9"
   },
   "source": [
    "This function builds a prompt and generates a Python code solution for a programming problem based on the given mode: zero-shot, one-shot, or few-shot.  The prompt is wrapped in a PromptTemplate and passed to an LLM chain (llm_chain) to produce the final code-only response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b6037f",
   "metadata": {
    "id": "f7b6037f"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates Python code to solve the given problem.\n",
    "You should respond only with the code, without explanation.\n",
    "\n",
    "### Problem\n",
    "{question}\n",
    "\n",
    "### Solution\n",
    "\"\"\"\n",
    "\n",
    "def generate_response(df, index, mode='zero-shot', num_few_shot=3, random_state=0):\n",
    "    random.seed(random_state)\n",
    "\n",
    "    question = clean_problem_description(df.loc[index, 'problem_description'])\n",
    "\n",
    "    if mode == 'zero-shot':\n",
    "        prompt = template.format(question=question)\n",
    "\n",
    "    elif mode == 'one-shot':\n",
    "        example_idx = df.drop(index).sample(1, random_state=random_state).index[0]\n",
    "        example_problem = clean_problem_description(df.loc[example_idx, 'problem_description'])\n",
    "        example_solution = df.loc[example_idx, 'solution_code']\n",
    "\n",
    "        prompt = (\n",
    "            \"You are a helpful assistant that generates Python code to solve the given problem.\\n\"\n",
    "            \"You should respond only with the code, without explanation.\\n\\n\"\n",
    "            \"### Problem\\n\"\n",
    "            f\"{example_problem}\\n\\n\"\n",
    "            \"### Solution\\n\"\n",
    "            f\"{example_solution.strip()}\\n\"\n",
    "            \"---\\n\"\n",
    "            \"### Problem\\n\"\n",
    "            f\"{question}\\n\\n\"\n",
    "            \"### Solution\\n\"\n",
    "        )\n",
    "\n",
    "    elif mode == 'few-shot':\n",
    "        example_indices = df.drop(index).sample(num_few_shot, random_state=random_state).index\n",
    "        prompt = (\n",
    "            \"You are a helpful assistant that generates Python code to solve the given problem.\\n\"\n",
    "            \"You should respond only with the code, without explanation.\\n\\n\"\n",
    "        )\n",
    "        for i in example_indices:\n",
    "            example_problem = clean_problem_description(df.loc[i, 'problem_description'])\n",
    "            example_solution = df.loc[i, 'solution_code']\n",
    "            prompt += (\n",
    "                \"### Problem\\n\"\n",
    "                f\"{example_problem}\\n\\n\"\n",
    "                \"### Solution\\n\"\n",
    "                f\"{example_solution.strip()}\\n\"\n",
    "                \"---\\n\"\n",
    "            )\n",
    "        prompt += (\n",
    "            \"### Problem\\n\"\n",
    "            f\"{question}\\n\\n\"\n",
    "            \"### Solution\\n\"\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode. Choose between 'zero-shot', 'one-shot', or 'few-shot'.\")\n",
    "\n",
    "    prompt_template = PromptTemplate(template=prompt, input_variables=[\"question\"])\n",
    "    llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
    "\n",
    "    response = llm_chain.run({\"question\": question})\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ae0f09",
   "metadata": {
    "id": "46ae0f09"
   },
   "source": [
    "We will also use the same example from before for the demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97081a88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97081a88",
    "outputId": "67ac5b8c-1d15-4f54-8f61-188d11c4be14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(14480)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84397857",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "84397857",
    "outputId": "84192026-7cef-41c1-ccb4-06a0d9a59259"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'There are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n\\nExamples\\n\\nInput\\n\\n3 2\\n2 1\\n\\n\\nOutput\\n\\n1\\n\\n\\nInput\\n\\n5 5\\n2 3\\n\\n\\nOutput\\n\\n6\\n\\n\\nInput\\n\\n2 4\\n2 4\\n\\n\\nOutput\\n\\n0'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[example.index[0], 'problem_description']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c4298",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a88c4298",
    "outputId": "9bfbd54f-95c4-44d7-84fe-b6531a5a8af6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-6cdfcba6b64f>:61: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
      "<ipython-input-65-6cdfcba6b64f>:63: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm_chain.run({\"question\": question})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that generates Python code to solve the given problem.\n",
      "You should respond only with the code, without explanation.\n",
      "Problem description:\n",
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "Solution (Python code):\n",
      "\n",
      "from itertools import combinations\n",
      "\n",
      "n, m = map(int, input().split())\n",
      "h, w = map(int, input().split())\n",
      "\n",
      "combs = [(i, j) for i, j in combinations(range(n), w)]\n",
      "\n",
      "white_cells = sum([1 for _, j in combs if j < m])\n",
      "print(white_cells)\n",
      "\n",
      "Note: This code uses Python 3.7 and above. It uses the itertools module to generate all possible combinations of rows to paint. It then calculates the number of white cells that will remain by iterating through each combination and checking if any of the columns in that combination are greater than the value of m. If so, it sets the number of white cells to 0 and continues to the next combination. If not, it adds 1 to the number of white cells. Finally, it prints the value of white_cells.\n"
     ]
    }
   ],
   "source": [
    "output = generate_response(df, example.index[0], mode='zero-shot')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf61ace",
   "metadata": {
    "id": "eaf61ace"
   },
   "source": [
    "The code generated by the zero shot approach was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4948d5f6",
   "metadata": {
    "id": "4948d5f6"
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "n, m = map(int, input().split())\n",
    "h, w = map(int, input().split())\n",
    "\n",
    "combs = [(i, j) for i, j in combinations(range(n), w)]\n",
    "\n",
    "white_cells = sum([1 for _, j in combs if j < m])\n",
    "print(white_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66188da",
   "metadata": {
    "id": "e66188da"
   },
   "source": [
    "This code attempts to solve the problem by generating combinations of rows and columns but contains some mistakes: it uses incorrect variable names (n, m instead of H, W), misinterprets the selection logic for rows and columns, and does not correctly calculate the number of remaining white cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4616cb35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4616cb35",
    "outputId": "55eec58e-fcc2-4586-e712-e8374eed62e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that generates Python code to solve the given problem.\n",
      "You should respond only with the code, without explanation.\n",
      "\n",
      "### Problem\n",
      "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "\n",
      "### Solution\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))\n",
      "---\n",
      "### Problem\n",
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "\n",
      "### Solution\n",
      "def count_white_cells(h: int, w: int) -> int:\n",
      "\n",
      "    n = (h + 1) * (w + 1)\n",
      "    white_count = 0\n",
      "\n",
      "    for i in range(n):\n",
      "        row = i // (w + 1)\n",
      "        col = i % (w + 1)\n",
      "        if i % 2 == 0:\n",
      "            white_count += 1 if row % 2 == 1 and col % 2 == 1 else (row % 2 == 0 and col % 2 == 0)\n",
      "        else:\n",
      "            white_count += 1 if row % 2 == 0 and col % 2!= 1 else (row % 2!= 0 and col % 2 == 1)\n",
      "\n",
      "    return white_count\n",
      "\n",
      "h = int(input())\n",
      "w = int(input())\n",
      "h = int(input())\n",
      "w = int(input())\n",
      "ans = count_white_cells(h, w)\n",
      "print(ans)\n",
      "---\n",
      "### Problem\n",
      "You are given a sequence of N integers a1, a2,..., an.\n",
      "\n",
      "For i = 1..N, a\\_i can be either positive or negative. Your task is to find the maximum possible sum of the elements a\\_1, a\\_2,..., a\\_N such that:\n",
      "\n",
      "* a\\_1 is positive\n",
      "\n",
      "* a\\_i = a\\_{i-1} + a\\_{i-2} for i = 2..N\n",
      "\n",
      "Input\n",
      "\n",
      "The input consists of one line with N space-separated integers a1, a2,..., an.\n",
      "\n",
      "Output\n",
      "\n",
      "Your output will consist of one line with an integer.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 1 â‰¤ n â‰¤ 10^5\n",
      "* 1 â‰¤ |ai| â‰¤ 10^9\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "3\n",
      "2 1 -2\n",
      "\n",
      "Output\n",
      "\n",
      "5\n",
      "\n",
      "Note:\n",
      "\n",
      "* The sum can be negative.\n",
      "* You can assume that a1 >= 0, since a0 = 0 by definition.\n",
      "\n",
      "### Solution\n",
      "def max_sum(n: int, a: List[int]) -> int:\n",
      "    return sum(max(a[i], a[i-1]+a[i-2]) for i in range(n))\n",
      "\n",
      "N = int(input())\n",
      "A = list(map(int, input().split()))\n",
      "ans = max_sum(N, A)\n",
      "print(ans)\n",
      "---\n",
      "### Problem\n",
      "You are given N integers a1, a2,..., an. Your task is to find two integers x and y such that:\n",
      "\n",
      "* a\\_x = max(a1, a2,..., an)\n",
      "* a\\_y = min(a1, a2,..., an)\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 1 â‰¤ n â‰¤ 10^6\n",
      "* 1 â‰¤ |ai| â‰¤ 10\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "5\n",
      "3 2 1 4 5\n",
      "\n",
      "Output\n",
      "\n",
      "4 1\n",
      "\n",
      "### Solution\n",
      "def min_max_sum(n: int, a: List[int]) -> int:\n",
      "    return sum(max(ai, min(ai, a[i+1])) for i in range(n-1))\n",
      "\n",
      "N = int(input())\n",
      "A = list(map(int, input().split()))\n",
      "ans = min_max_sum(N, A)\n",
      "print(ans)\n"
     ]
    }
   ],
   "source": [
    "output = generate_response(df, example.index[0], mode='one-shot')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993a35cf",
   "metadata": {
    "id": "993a35cf"
   },
   "source": [
    "The solution generated by the one-shot was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68571ada",
   "metadata": {
    "id": "68571ada"
   },
   "outputs": [],
   "source": [
    "def count_white_cells(h: int, w: int) -> int:\n",
    "\n",
    "    n = (h + 1) * (w + 1)\n",
    "    white_count = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        row = i // (w + 1)\n",
    "        col = i % (w + 1)\n",
    "        if i % 2 == 0:\n",
    "            white_count += 1 if row % 2 == 1 and col % 2 == 1 else (row % 2 == 0 and col % 2 == 0)\n",
    "        else:\n",
    "            white_count += 1 if row % 2 == 0 and col % 2!= 1 else (row % 2!= 0 and col % 2 == 1)\n",
    "\n",
    "    return white_count\n",
    "\n",
    "h = int(input())\n",
    "w = int(input())\n",
    "h = int(input())\n",
    "w = int(input())\n",
    "ans = count_white_cells(h, w)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbcfc18",
   "metadata": {
    "id": "bfbcfc18"
   },
   "source": [
    "Which also doesn't solve the problem. And we can see that this one also continues generating more examples and solutions after the one asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b1168",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "752b1168",
    "outputId": "386812bb-2173-49f3-93ec-d16be9bdd07f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant that generates Python code to solve the given problem.\n",
      "You should respond only with the code, without explanation.\n",
      "\n",
      "### Problem\n",
      "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
      "\n",
      "At least how many operations are necessary to satisfy the following conditions?\n",
      "\n",
      "* For every i (1â‰¤iâ‰¤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
      "* For every i (1â‰¤iâ‰¤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 â‰¤ n â‰¤ 10^5\n",
      "* |a_i| â‰¤ 10^9\n",
      "* Each a_i is an integer.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "n\n",
      "a_1 a_2 ... a_n\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the minimum necessary count of operations.\n",
      "\n",
      "### Solution\n",
      "N = int(input())\n",
      "a = list(map(int, input().split()))\n",
      "\n",
      "ans1 = 0\n",
      "s = 0\n",
      "flg = 1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans1 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "ans2 = 0\n",
      "s = 0\n",
      "flg = -1\n",
      "for ai in a:\n",
      "    s += ai\n",
      "    if s * flg <= 0:\n",
      "        ans2 += abs(s) + 1\n",
      "        s = flg\n",
      "    flg *= -1\n",
      "\n",
      "print(min(ans1, ans2))\n",
      "---\n",
      "### Problem\n",
      "In some other world, today is the day before Christmas Eve.\n",
      "\n",
      "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
      "\n",
      "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
      "\n",
      "Constraints\n",
      "\n",
      "* 2 \\leq N \\leq 10\n",
      "* 100 \\leq p_i \\leq 10000\n",
      "* p_i is an even number.\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "N\n",
      "p_1\n",
      "p_2\n",
      ":\n",
      "p_N\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the total amount Mr. Takaha will pay.\n",
      "\n",
      "### Solution\n",
      "N=int(input())\n",
      "P=sorted([int(input()) for _ in range(N)])\n",
      "print(sum(P)-P[-1]//2)\n",
      "---\n",
      "### Problem\n",
      "Two polar bears Menshykov and Uslada from the St.Petersburg zoo and elephant Horace from the Kiev zoo got six sticks to play with and assess the animals' creativity. Menshykov, Uslada and Horace decided to make either an elephant or a bear from those sticks. They can make an animal from sticks in the following way: \n",
      "\n",
      "  * Four sticks represent the animal's legs, these sticks should have the same length. \n",
      "  * Two remaining sticks represent the animal's head and body. The bear's head stick must be shorter than the body stick. The elephant, however, has a long trunk, so his head stick must be as long as the body stick. Note that there are no limits on the relations between the leg sticks and the head and body sticks. \n",
      "\n",
      "\n",
      "\n",
      "Your task is to find out which animal can be made from the given stick set. The zoo keeper wants the sticks back after the game, so they must never be broken, even bears understand it.\n",
      "\n",
      "Input\n",
      "\n",
      "The single line contains six space-separated integers li (1 â‰¤ li â‰¤ 9) â€” the lengths of the six sticks. It is guaranteed that the input is such that you cannot make both animals from the sticks.\n",
      "\n",
      "Output\n",
      "\n",
      "If you can make a bear from the given set, print string \"Bear\" (without the quotes). If you can make an elephant, print string \"Elephant\" (wÄ±thout the quotes). If you can make neither a bear nor an elephant, print string \"Alien\" (without the quotes).\n",
      "\n",
      "### Solution\n",
      "import sys\n",
      "import math\n",
      "import itertools\n",
      "import collections\n",
      "\n",
      "\n",
      "def divs(n, start=1):\n",
      "    r = []\n",
      "    for i in range(start, int(math.sqrt(n) + 1)):\n",
      "        if (n % i == 0):\n",
      "            if (n / i == i):\n",
      "                r.append(i)\n",
      "            else:\n",
      "                r.extend([i, n // i])\n",
      "    return r\n",
      "def cdiv(n, k): return n // k + (n % k != 0)\n",
      "def ii(): return int(input())\n",
      "def mi(): return map(int, input().split())\n",
      "def li(): return list(map(int, input().split()))\n",
      "def lcm(a, b): return abs(a * b) // math.gcd(a, b)\n",
      "def wr(arr): return ' '.join(map(str, arr))\n",
      "def revn(n): return int(str(n)[::-1])\n",
      "def prime(n):\n",
      "    if n == 2: return True\n",
      "    if n % 2 == 0 or n <= 1: return False\n",
      "    sqr = int(math.sqrt(n)) + 1\n",
      "    for d in range(3, sqr, 2):\n",
      "        if n % d == 0: return False\n",
      "    return True\n",
      "def convn(number, base=3):\n",
      "    newnumber = ''\n",
      "    while number > 0:\n",
      "        newnumber = str(number % base) + newnumber\n",
      "        number //= base\n",
      "    return newnumber\n",
      "\n",
      "\n",
      "l = li()\n",
      "lc = collections.Counter(l)\n",
      "bear = [[4, 1, 1], [5, 1], [1, 5], [1, 4, 1], [1, 1, 4]]\n",
      "elephant = [[6], [4, 2], [2, 4]]\n",
      "if list(lc.values()) in bear:\n",
      "    print('Bear')\n",
      "elif list(lc.values()) in elephant:\n",
      "    print('Elephant')\n",
      "else:\n",
      "    print('Alien')\n",
      "---\n",
      "### Problem\n",
      "There are H rows and W columns of white square cells.\n",
      "\n",
      "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
      "\n",
      "How many white cells will remain?\n",
      "\n",
      "It can be proved that this count does not depend on what rows and columns are chosen.\n",
      "\n",
      "Constraints\n",
      "\n",
      "* All values in input are integers.\n",
      "* 1 \\leq H, W \\leq 20\n",
      "* 1 \\leq h \\leq H\n",
      "* 1 \\leq w \\leq W\n",
      "\n",
      "Input\n",
      "\n",
      "Input is given from Standard Input in the following format:\n",
      "\n",
      "\n",
      "H W\n",
      "h w\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "Print the number of white cells that will remain.\n",
      "\n",
      "### Solution\n",
      "n = int(input())\n",
      "m = int(input())\n",
      "h = int(input())\n",
      "w = int(input())\n",
      "ans = 1\n",
      "\n",
      "for i in range(h):\n",
      "    cnt = 0\n",
      "    for j in range(w):\n",
      "        if m - h >= i and m - h >= j:\n",
      "            cnt += 1\n",
      "        else:\n",
      "            cnt -= 1\n",
      "    if cnt == 0 or cnt == 1:\n",
      "        ans *= 1\n",
      "    else:\n",
      "        ans *= -1\n",
      "ans *= -1\n",
      "print(ans)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "output = generate_response(df, example.index[0], mode='few-shot')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f7fa64",
   "metadata": {
    "id": "84f7fa64"
   },
   "source": [
    "The code generated was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec01ebd",
   "metadata": {
    "id": "9ec01ebd"
   },
   "outputs": [],
   "source": [
    "n = int(input())\n",
    "m = int(input())\n",
    "h = int(input())\n",
    "w = int(input())\n",
    "ans = 1\n",
    "\n",
    "for i in range(h):\n",
    "    cnt = 0\n",
    "    for j in range(w):\n",
    "        if m - h >= i and m - h >= j:\n",
    "            cnt += 1\n",
    "        else:\n",
    "            cnt -= 1\n",
    "    if cnt == 0 or cnt == 1:\n",
    "        ans *= 1\n",
    "    else:\n",
    "        ans *= -1\n",
    "ans *= -1\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cb0f65",
   "metadata": {
    "id": "06cb0f65"
   },
   "source": [
    "This code is incorrect and unrelated to the problem logic; it uses nested loops and conditions that do not correspond to counting remaining white cells. The approach is confused and does not calculate or print the correct result for the given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d4f4be",
   "metadata": {
    "id": "51d4f4be"
   },
   "source": [
    "The model likely failed to solve the problem in all three approaches because the problem requires a simple, direct mathematical insight, but the prompts or examples may have been unclear or noisy. This can cause the model to overcomplicate or misunderstand the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dee5fa8",
   "metadata": {
    "id": "8dee5fa8"
   },
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503cf170",
   "metadata": {
    "id": "503cf170"
   },
   "source": [
    "# Fine-tuning a small LLM for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6558a600",
   "metadata": {
    "id": "6558a600"
   },
   "source": [
    "## Fine tuning GPT-2\n",
    "\n",
    "GPT-2 Medium (https://huggingface.co/openai-community/gpt2-medium) is a transformer-based language model developed by OpenAI, containing approximately 345 million parameters. It is part of the GPT-2 family and was pretrained on a large corpus of internet text using unsupervised learning. Due to its moderate size, GPT-2 Medium is suitable for fine-tuning on domain-specific tasks using limited resources (e.g., Google Colab)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcf7437",
   "metadata": {
    "id": "cfcf7437"
   },
   "source": [
    "### Imports\n",
    "\n",
    "We start by doing the necessary library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51044655",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "51044655",
    "outputId": "f8f491ae-be2a-40ca-c219-3b035da33021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02ffb19",
   "metadata": {
    "id": "a02ffb19"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from google.colab import drive\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfcfdac",
   "metadata": {
    "id": "1dfcfdac"
   },
   "source": [
    "### Load the model\n",
    "\n",
    "Before we do the fine-tuning, let's see how the pretrained model does on our task of code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268059d8",
   "metadata": {
    "id": "268059d8"
   },
   "source": [
    "In this next cell we load the pretrained model and it's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454540e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425,
     "referenced_widgets": [
      "26f6ebc093a54999a37c68b6feef0bd8",
      "41040d60b3a743c8bd126894af00db10",
      "f470c49873bc408e83ed54718240ae9b",
      "1d4bc975e59e474a9da40c84e2b58c44",
      "8984ba2f14b149aaa2cb0dca4e5cc2f6",
      "41131aebf13d41e395e34e8cdb9aeb51",
      "af0e5804877a4af58b08e7587e7e1135",
      "85cf61350a5843fe8badb5ff58263686",
      "ef0b454445274f3ea1df449bbe7e1bbc",
      "153e54dc8b1c4a3eb891bf0919848b86",
      "e82c91bddf974532a883f2bee637a843",
      "1ab2748cba2f4ddaa3e1d48042720847",
      "65e010d946b04b4ca53b713270536f89",
      "8bddf3e8730f42e4bc908d360a368f7c",
      "724094383ceb4726982e4ebcb89b01c8",
      "1808ca858bc045b1996f84d47ae04884",
      "2d1f91c3a30847b4b32e1bc6c51722df",
      "8a356ae6655948699da2685b2631ba5a",
      "06a798f7b6ac4459a7e25c3baafd570a",
      "99bf649266f44165b0571ac3ee3b5ba4",
      "e4ea144c0e954710accb3d383f372728",
      "fbf46355fca045a9a5acfac5c8807359",
      "b3ac00fc63ab42609411b8443ca9cfa0",
      "58358ae9c3254f8ba15188c8a2d07c2a",
      "77627200bd1044b281243768ebb10591",
      "3a59980b67a842e28d59838fada61346",
      "ec52f19b16364481a1c430480313b725",
      "5596b77475ca431aa819603dfcdeffde",
      "e721020557784946b13ceb721e8cf959",
      "eb060de8fc214d7fabf6c3860cd5938a",
      "67273bdfb4f64313896c7b15f247e29f",
      "046a6435dd8141809fb2d16411092713",
      "473f7e434d5040e899ed66c243a01cd7",
      "19cc4862c856417aaa9c9eaa397dea85",
      "4aa0ef94997d4eb4b8f2a739d6df0b89",
      "70d706fb474d433681590a2782fbabf9",
      "c0b91bf995e446b5bf8691395fd6ccde",
      "b7308cfaf88c43a9899137edfd05c869",
      "6cea8748c058446ea101e46da84c28b2",
      "20e1e3fe24284b80afbe364d10ac9179",
      "ec17cd0ce9dd45e98a7748ca512e79a2",
      "a632a6f9b76c48ec94e19343680822c2",
      "dce75e7ce61c4263a505b3a282230ac4",
      "c34dcdc7af8843f4aba77c64fa423633",
      "c9bcb21f160040bf97593d92cb08bfb6",
      "fd8a27c831fd4428a2f585847b59acd8",
      "720199c600624f4a93ca9bfda5718028",
      "520b085046fc4745a391dac5ab29bc34",
      "d38f7a9c3fcd47c5a0bcd3783a5ca8bf",
      "fefa30bdbcc846adb72555feaf313d24",
      "6159412047e74ac8801f3a4249375da0",
      "9f5cae6343de421fb7682191e4a25869",
      "8ffb5d8bd1d34202b12a69464e328e03",
      "a15f46ceb6764952b2b4d0d71015845d",
      "8a646a262acd4791a5395fbfd2ad2333",
      "d8ce513a2eb846848afbbdb842ef2058",
      "bd3b1121b09344be8bf8b0637cc6797f",
      "c1a65c86b7e847d4aa5f6c2394fed22b",
      "587ba5216793454189d4c979be9f7911",
      "de2b249f3ebd4ccc8e7610d5652394f8",
      "71f16302765949209b20ca905b8022db",
      "7911e666c5a346ad99c74c16a541c9f6",
      "c9ac631e5c154e71840b27a9ce92184f",
      "5d0c4e75a68542cb9cbce2fbbda46acf",
      "11662c96b9374c969e621ce4bdf45490",
      "d78699f88c144f1d85d82fcac6b39c2b",
      "72ee5e8a062e4929927bbd17b981b0f8",
      "cbc2f72c0db84281805e1e835d60b42e",
      "8628b7fc177f4988b6fb1ede147be924",
      "f6821f64bc1e411ebe4554d0e02e6d9f",
      "fc686b5fb04a4d8285d259665499e7ac",
      "72352fc5509b48c687f24ef4eed71737",
      "41b1d98ec68f46b9813f93d9b2a15e3f",
      "d79db31c4c1f4fb2bd267f134aed1949",
      "beb9abb3afdb4a18907bd2df255bd1ec",
      "71954ec9ed0a41c98f360e8258795375",
      "43006730e0f846dd8b75d6e0b13eddbc"
     ]
    },
    "id": "454540e5",
    "outputId": "1955a6c5-bd8a-46c1-df86-2deb408214c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f6ebc093a54999a37c68b6feef0bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ab2748cba2f4ddaa3e1d48042720847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ac00fc63ab42609411b8443ca9cfa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19cc4862c856417aaa9c9eaa397dea85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9bcb21f160040bf97593d92cb08bfb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ce513a2eb846848afbbdb842ef2058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ee5e8a062e4929927bbd17b981b0f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 1024)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd829f6c",
   "metadata": {
    "id": "fd829f6c"
   },
   "source": [
    "And now we create the text generation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b6690",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f14b6690",
    "outputId": "b777babe-acaf-4691-cb2d-a078601c208d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2fe0b8",
   "metadata": {
    "id": "6a2fe0b8"
   },
   "source": [
    "We will use for demonstration this example prompt that is not in the dataset. The problem is to print YES if the input is even and NO if it is odd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae28d39",
   "metadata": {
    "id": "5ae28d39"
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"### Problem:\n",
    "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
    "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
    "\n",
    "#### Input:\n",
    "- An integer w, the weight of the watermelon.\n",
    "\n",
    "#### Output:\n",
    "- \"YES\" if the watermelon can be split into two even positive integers.\n",
    "- \"NO\" otherwise.\n",
    "\n",
    "#### Examples:\n",
    "Input: 8\n",
    "Output: YES\n",
    "\n",
    "Input: 3\n",
    "Output: NO\n",
    "\n",
    "Input: 4\n",
    "Output: YES\n",
    "\n",
    "### Solution:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90236d3",
   "metadata": {
    "id": "b90236d3"
   },
   "source": [
    "With the previous prompt, we generate the model's output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130cc60d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "130cc60d",
    "outputId": "f7acc369-f9f1-42f9-b019-0ba3c4b2e74a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output before fine-tuning:\n",
      "\n",
      "### Problem:\n",
      "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
      "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
      "\n",
      "### Solution:\n",
      "\n",
      "Add a new type of function called partition with the function `partition(w,partitionDim,w,partitionDim,w)`. This function takes an integer of `w` and returns a tuple containing two items: `partitionDim` and `partitionDim`.\n",
      "\n",
      "## Examples:\n",
      "\n",
      ">>> import multiprocessing import time >>> partition = partition(1, 2, 3, 4) >>> partition(0, 1) 'NO' >>> partition(0, 1) 'YES' >>> partition(1, 10) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO\n"
     ]
    }
   ],
   "source": [
    "output = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
    "print(\"Output before fine-tuning:\\n\")\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ffc1a",
   "metadata": {
    "id": "862ffc1a"
   },
   "source": [
    "We can see that the model doesn't generate the code for the solution. It tries to start explaining how to solve the problem but does not write any actual code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18770ad",
   "metadata": {
    "id": "a18770ad"
   },
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb54937",
   "metadata": {
    "id": "8eb54937"
   },
   "source": [
    "Now we will load out dataset to start the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b7e263",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "f1b7e263",
    "outputId": "3e94ca29-ef96-4885-b8a1-3a0c8d922be6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/.shortcut-targets-by-id/17WgJO1gfIBADpYX2jVdb41q7HCbwWcOU/NLP_Project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "path = 'Colab Notebooks/NLP/NLP_Project'\n",
    "\n",
    "os.chdir(f'/content/drive/MyDrive/{path}')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb67ecf6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb67ecf6",
    "outputId": "a1d58c5d-7cc8-46c8-e37b-bbb9cd1d96a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 problem_description solution_id  \\\n",
      "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
      "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
      "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
      "3  Xenia has a set of weights and pan scales. Eac...         0_6   \n",
      "4  Xenia has a set of weights and pan scales. Eac...         0_8   \n",
      "\n",
      "                                       solution_code  \\\n",
      "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
      "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
      "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
      "3  MOD = 10**9 + 7\\nI = lambda:list(map(int,input...   \n",
      "4  to_print = []\\ndef dfs(d, ini, s, depth, m):\\n...   \n",
      "\n",
      "               problem_name time_complexity_inferred space_complexity_inferred  \n",
      "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
      "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "3  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "4  339_C. Xenia and Weights                     O(1)                      O(1)  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_ds.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930134ff",
   "metadata": {
    "id": "930134ff"
   },
   "source": [
    "### Fine-tuning with 3k samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea69b688",
   "metadata": {
    "id": "ea69b688"
   },
   "source": [
    "First we will start with a small example, doing the fine-tuning with 3000 samples of the dataset. In this next cell we prepare the text that we will give to the model in the training putting together the descriptions and the code solutions, and splitting it into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed70b8fd",
   "metadata": {
    "id": "ed70b8fd"
   },
   "outputs": [],
   "source": [
    "df_small = df[[\"problem_description\", \"solution_code\"]].dropna().sample(3000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "df_small[\"text\"] = df_small.apply(\n",
    "    lambda row: f\"### Problem:\\n{row['problem_description']}\\n### Solution:\\n{row['solution_code']}\", axis=1\n",
    ")\n",
    "\n",
    "dataset = Dataset.from_pandas(df_small[[\"text\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0415a3da",
   "metadata": {
    "id": "0415a3da"
   },
   "source": [
    "Now we tokenize the selected data with the model's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2e6c4f",
   "metadata": {
    "id": "aa2e6c4f"
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    tokens = tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491ac66",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "e4c64ecf47134ec9bb8a7381568b2138",
      "e69599327e2d4efa93cda268c8fc4b39",
      "866c9211f04d429eafb173f7b8d62865",
      "de25b7d11f184efca0bf4a9c336c6c1b",
      "362c2a69977a41d28bbd1256fc8bd608",
      "e90e59b4229f416aaf386b1e2e40b2af",
      "06fb2e01ff674010b3547db457eae19e",
      "e9b187c1b2f14396a56652af92350597",
      "1198180365714b0d933b68467546e107",
      "b28c9d8a45734bb8a0a6710e8911181d",
      "60d0e165f56742b7804560d982cdb280",
      "be112a4e21d04263bb92565045600638",
      "ff8d86be7b874b46ac90caaccfd45328",
      "623838696bc248138645f785a6656205",
      "4108fe58a3f7411d9b106bc021b1a83a",
      "312c685049884d3087533f465c00c316",
      "db2bb2e2f12444589b8f86b6a7dd56d3",
      "b4d5512cb4cf4f608ba67a8a7ce84584",
      "79dddae3cba6494893db7c92b2745db9",
      "ab0121b14eac4b66b297c792914e80f0",
      "c4ceff94d5ba4a2093aeed2e38c194cd",
      "38df032532b5439bab9676486cc55141"
     ]
    },
    "id": "2491ac66",
    "outputId": "93a67537-fe79-4ec6-9cbf-0cffb525bae7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c64ecf47134ec9bb8a7381568b2138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be112a4e21d04263bb92565045600638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5926eb",
   "metadata": {
    "id": "fe5926eb"
   },
   "source": [
    "In this next step we create the Trainer, defining the training arguments and the model to be trained. We will train 3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f399519",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f399519",
    "outputId": "cc74001d-4893-4ebf-dc36-4506cb71201f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-aa4f4d72a588>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medium-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46136d",
   "metadata": {
    "id": "6a46136d"
   },
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792b5812",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "792b5812",
    "outputId": "270b1742-7b83-46e4-8dfc-d1561a1bf6ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4050' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4050/4050 33:43, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.646100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.539500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.436700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.412100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.401400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.293900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.335100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.332400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.344200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.254900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.218300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.129000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.157400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.163500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.101500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.101200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.048100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.084300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.107000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.056400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.108000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.950000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.995300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.990500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.017500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.975100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.917200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.964000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.947200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.978100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.998800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4050, training_loss=1.179877754493996, metrics={'train_runtime': 2023.7052, 'train_samples_per_second': 4.003, 'train_steps_per_second': 2.001, 'total_flos': 7522475625676800.0, 'train_loss': 1.179877754493996, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24bab7",
   "metadata": {
    "id": "5d24bab7"
   },
   "source": [
    "After training we save the model generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b0b3f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f2b0b3f",
    "outputId": "fe40e922-f79f-4076-d821-9def2dd8b289"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-medium-finetuned/tokenizer_config.json',\n",
       " './gpt2-medium-finetuned/special_tokens_map.json',\n",
       " './gpt2-medium-finetuned/vocab.json',\n",
       " './gpt2-medium-finetuned/merges.txt',\n",
       " './gpt2-medium-finetuned/added_tokens.json',\n",
       " './gpt2-medium-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-medium-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-medium-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace3cf3",
   "metadata": {
    "id": "cace3cf3"
   },
   "source": [
    "Now we will generate an output with the same prompt from the beggining to see how the finetuned model behaves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34a54d2",
   "metadata": {
    "id": "a34a54d2"
   },
   "source": [
    "First we create the pipeline with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b5dff8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a0b5dff8",
    "outputId": "43934dc7-8252-4cce-d66a-3feff332650b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe_finetuned = pipeline(\"text-generation\", model=\"./gpt2-medium-finetuned\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a486071",
   "metadata": {
    "id": "3a486071"
   },
   "source": [
    "And pass the same prompt again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb183053",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eb183053",
    "outputId": "22e5160e-05f7-4a47-d710-d15d67b36c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After fine-tuning:\n",
      " ### Problem:\n",
      "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
      "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
      "\n",
      "#### Input:\n",
      "- An integer w, the weight of the watermelon.\n",
      "\n",
      "#### Output:\n",
      "- \"YES\" if the watermelon can be split into two even positive integers.\n",
      "- \"NO\" otherwise.\n",
      "\n",
      "#### Examples:\n",
      "Input: 8  \n",
      "Output: YES\n",
      "\n",
      "Input: 3  \n",
      "Output: NO\n",
      "\n",
      "Input: 4  \n",
      "Output: YES\n",
      "\n",
      "### Solution:\n",
      "w = int(input())\n",
      "sum = 0\n",
      "for i in range(1, w+1):\n",
      "    while(i & 1):\n",
      "        if(w & (i & 0x00)) == 0:\n",
      "           sum += 1\n",
      "    else:\n",
      "         sum += sum*(i & 0x00)\n",
      "          sum *= 1\n",
      "          sum *= i\n",
      "\n",
      "print('YES' if sum == 'YES' else 'NO')\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### Problem:\n",
    "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
    "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
    "\n",
    "#### Input:\n",
    "- An integer w, the weight of the watermelon.\n",
    "\n",
    "#### Output:\n",
    "- \"YES\" if the watermelon can be split into two even positive integers.\n",
    "- \"NO\" otherwise.\n",
    "\n",
    "#### Examples:\n",
    "Input: 8\n",
    "Output: YES\n",
    "\n",
    "Input: 3\n",
    "Output: NO\n",
    "\n",
    "Input: 4\n",
    "Output: YES\n",
    "\n",
    "### Solution:\n",
    "\"\"\"\n",
    "\n",
    "output_finetuned = pipe_finetuned(prompt, max_new_tokens=256)\n",
    "print(\"After fine-tuning:\\n\", output_finetuned[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9784a97",
   "metadata": {
    "id": "a9784a97"
   },
   "source": [
    "We can see that the output, even if it's not the correct solution for the problem, is actually well structured python code, an improvement from the first output generated pre-finetuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270a427",
   "metadata": {
    "id": "f270a427"
   },
   "source": [
    "### Fine-tuning with 10k samples\n",
    "\n",
    "Now we will take a larger piece of the dataset for the finetuning, with 10000 samples, and again divide into train and test sets and tokenize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec113a",
   "metadata": {
    "id": "2eec113a"
   },
   "outputs": [],
   "source": [
    "df_10k = df[[\"problem_description\", \"solution_code\"]].dropna().sample(10000, random_state=42).reset_index(drop=True)\n",
    "df_10k[\"text\"] = df_10k.apply(\n",
    "    lambda row: f\"### Problem:\\n{row['problem_description']}\\n### Solution:\\n{row['solution_code']}\", axis=1\n",
    ")\n",
    "\n",
    "dataset_10k = Dataset.from_pandas(df_10k[[\"text\"]])\n",
    "dataset_10k = dataset_10k.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23211a00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "763b4bc496dd433d8720b6dc5bb60b4d",
      "876211be65e147f49a137ae25f9242c1",
      "edd23127ebd44e55910cecda82b3d525",
      "795f831527a94d6782863cc11198dabc",
      "70a52aa216874de9a3a3615270660d8c",
      "6f699d16b37d471f879bb73d7faa0f71",
      "93423f0b67234cbf862a3cad691802be",
      "8cd5e0d2fa664ffe92994f0cacbc3a4c",
      "5f51ae35625741d698f5a4dfc25ed987",
      "7e662dac1a594cbda5f497fe666bcc00",
      "d9e1581856ee47dbb8df0eb7e329811d",
      "18503693d4a94852a38b860372a88137",
      "96a464b34414475f8f3362c3091903ae",
      "529e5aca14b64c23bf80a64dc0dc0552",
      "6d4ead32d81f44d29fcde048850aa899",
      "ba80022d0a25408bb8d0163a26393a60",
      "d7977403e5a04d40ab7dfef0588be9f8",
      "55b40e4079b942e986f43dca2fa04188",
      "bc9b09f6958247a4840385e709f47713",
      "9e03c2d683344d8aa13691b13340dc1f",
      "7b8fb89182a44ba8b7052264dd15ea89",
      "d0e70101d38b4cfd8e2a085e07e3f925"
     ]
    },
    "id": "23211a00",
    "outputId": "38e76974-abf3-404d-b483-73fb3a46a7bc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763b4bc496dd433d8720b6dc5bb60b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18503693d4a94852a38b860372a88137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_10k_dataset = dataset_10k.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a699b8d6",
   "metadata": {
    "id": "a699b8d6"
   },
   "source": [
    "We define the Trainer and the training arguments, this time with 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6609b1df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6609b1df",
    "outputId": "44e98a29-9308-4a7d-d0e9-033d905eb828"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e0c5a36a666c>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-medium-finetuned-10kds\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=5000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_10k_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_10k_dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3963a6",
   "metadata": {
    "id": "4f3963a6"
   },
   "source": [
    "And start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffad43d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2ffad43d",
    "outputId": "71c2efc7-1d6a-4f45-ee21-584d9b389681"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9000/9000 1:02:14, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.961900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.611400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.583300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.469600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.473500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.450400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.364800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.378400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.363800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.285100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.268500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.293700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.242000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.242300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.165100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.160800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.185200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.159500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.139400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.113800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.046000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.095600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.153000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.055000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.067600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.133400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.058000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.091100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.062200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.027200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.005000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.035200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.872500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.863500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.884000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.831300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.851100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.867600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.863800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.865400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.875200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.889600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.846900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.846100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.816500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.850600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.831800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.820700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.822700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.825900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.838500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.816100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.833000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.766500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.805700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.787700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.795900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.782500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.817500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.861200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9000, training_loss=1.0219143998887803, metrics={'train_runtime': 3736.2941, 'train_samples_per_second': 4.818, 'train_steps_per_second': 2.409, 'total_flos': 1.6716612501504e+16, 'train_loss': 1.0219143998887803, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c31a21",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "56c31a21",
    "outputId": "367dedcf-e22d-45b9-8064-12e9cff44980"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7891965508460999,\n",
       " 'eval_runtime': 49.0426,\n",
       " 'eval_samples_per_second': 20.39,\n",
       " 'eval_steps_per_second': 10.195,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fafc714",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fafc714",
    "outputId": "854348fc-9928-4bbe-c2d0-4a9f572feee1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./gpt2-medium-finetuned-10k/tokenizer_config.json',\n",
       " './gpt2-medium-finetuned-10k/special_tokens_map.json',\n",
       " './gpt2-medium-finetuned-10k/vocab.json',\n",
       " './gpt2-medium-finetuned-10k/merges.txt',\n",
       " './gpt2-medium-finetuned-10k/added_tokens.json',\n",
       " './gpt2-medium-finetuned-10k/tokenizer.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./gpt2-medium-finetuned-10k\")\n",
    "tokenizer.save_pretrained(\"./gpt2-medium-finetuned-10k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764642d3",
   "metadata": {
    "id": "764642d3"
   },
   "source": [
    "Now we will run some metrics on the test set using 30 samples. First we will generate the output with the finetuned version for these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1769bd20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1769bd20",
    "outputId": "eaeab30f-245d-465c-c42e-0f89edf15e3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [12:44<00:00, 25.49s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = \"./gpt2-medium-finetuned-10k\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device= -1\n",
    ")\n",
    "\n",
    "test_dataset = dataset_10k[\"test\"]\n",
    "\n",
    "expected_codes = []\n",
    "generated_codes = []\n",
    "\n",
    "for example in tqdm(test_dataset.select(range(30))):\n",
    "    full_text = example[\"text\"]\n",
    "\n",
    "    try:\n",
    "        parts = full_text.split(\"### Problem:\\n\")[1].split(\"### Solution:\\n\")\n",
    "        description = parts[0].strip()\n",
    "        real_code = parts[1].strip()\n",
    "    except (IndexError, AttributeError):\n",
    "        continue\n",
    "\n",
    "    prompt = f\"### Problem:\\n{description}\\n### Solution:\\n\"\n",
    "\n",
    "    if not prompt.strip():\n",
    "        continue\n",
    "\n",
    "    max_prompt_tokens = 768\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_prompt_tokens)\n",
    "    prompt = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        output = generator(prompt, max_new_tokens=256, num_return_sequences=1, do_sample=False)[0][\"generated_text\"]\n",
    "    except RuntimeError as e:\n",
    "        print(\"GPU error:\", e)\n",
    "        continue\n",
    "\n",
    "    generated_code = output.split(\"### Solution:\\n\")[-1].strip()\n",
    "\n",
    "    expected_codes.append(real_code)\n",
    "    generated_codes.append(generated_code)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34a0f85",
   "metadata": {
    "id": "e34a0f85"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_codes = pd.DataFrame({\n",
    "    \"reference_code\": expected_codes,\n",
    "    \"generated_code\": generated_codes\n",
    "})\n",
    "\n",
    "df_codes.to_csv(\"codes_comparison.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737bc0c0",
   "metadata": {
    "id": "737bc0c0"
   },
   "source": [
    "Let's also run the model on the previous example to see the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25503504",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25503504",
    "outputId": "ce6408e9-5ba7-49a7-81b5-4113de3a4925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Problem:\n",
      "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
      "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
      "\n",
      "#### Examples:\n",
      "Input: 8\n",
      "Output: YES\n",
      "\n",
      "Input: 3\n",
      "Output: NO\n",
      "\n",
      "### Solution:\n",
      "w=int(input())\n",
      "for i in range(len(str(w))):\n",
      "    if str(i)==str(w-1):\n",
      "        print(\"YES\")\n",
      "    else:\n",
      "        print(\"NO\")\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"### Problem:\n",
    "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
    "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
    "\n",
    "#### Examples:\n",
    "Input: 8\n",
    "Output: YES\n",
    "\n",
    "Input: 3\n",
    "Output: NO\n",
    "\n",
    "### Solution:\n",
    "\"\"\"\n",
    "\n",
    "output = generator(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233676c5",
   "metadata": {
    "id": "233676c5"
   },
   "source": [
    "We can see that it does not generate the correct code for the problem, but it improves in comparison with the generated output from the very first model, since now we have well structured python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f68231",
   "metadata": {
    "id": "b2f68231"
   },
   "source": [
    "We will also get the outputs from the original model for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e07987",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 955,
     "referenced_widgets": [
      "ed83e1ed41c748cf9a47e17913801469",
      "57b1f6fabd5b4b518b941f3c60203b08",
      "d9e03181a2c14344b5ebc50893ebc1c8",
      "2c89b9b25f5a4f79a4e89d8e89d8e1f0",
      "42da2de819d246918718d843b964306c",
      "604b7132b3e14f449ac099f9eb702798",
      "12d30fbab7494586ab5dd13a315aa86f",
      "73c8499c66cc4d8c8dd4d812fbd52f55",
      "a6d716189c484723a3ae1e060eee5e32",
      "0db20d82d41548c6a2563a8d875f8558",
      "446efa47f7ca4cb39f6116eacc1a4331",
      "d270a55583344839b338243c1caa6888",
      "b8c495000d464cbbbcade3c90739df91",
      "6b2ef1938db646b5869d784fa6d31748",
      "d45348643b20451dbe032970df11a8e0",
      "8f308e82fdf141ef9b435a0ee11b9a84",
      "fe86d91d75b34bc88cce078847f83278",
      "77988a7cf1ef4d2e83c6645589c39466",
      "9ec4f85ee7eb40379876d3b9478ad5d8",
      "e467dc6317714b9cae8858b0475bf2f3",
      "ea4867c91b5b4c998064f56f694d4b20",
      "12747700a26a49d181e61fbcca7c3acd",
      "3d16f215d75b42ef900541e750227291",
      "236d7bb973354196bc18f348d8e21a06",
      "76ec335ddb584005ab2e63b844e71455",
      "497a3a3b31a94f3eb33fd51c3dd6dfb2",
      "0ddc362b4c29466e9de2da9d9325467f",
      "3aeb2172b5774caf8d5a54338928761f",
      "171b64cf4d624b6ea7990471d8a5b605",
      "4713b9fa2da3434e908c131399e435f0",
      "bb15fd914e404ebb82d203b6874a324c",
      "38d1f9e6d1104186a7c2bd129a49ea42",
      "4ec7e8282801455ea519c7151c4ec722",
      "bef7b99bcb2445f6bb0976f2d997aaa4",
      "7b93c339a2bc40b2b018f16b1adc662a",
      "064799fa1f754a799bf27a485cecdaa5",
      "201d2989f8194baeb2511e23d2453689",
      "84c49ee2542c4a45a8b6caf742bc825b",
      "d509dce7e84c41208599d742e8070628",
      "f2a5be1decc948a9852244562f98a223",
      "03e0668bb4b844ac89af7ef227313414",
      "b10e1bd73cfd401b92d25f7fb96dbe6e",
      "617ff60d327a4a15af686465280ea255",
      "e07b85eddffb44919f887b2c8e0fe83b",
      "f5f8b2ad2aef4617be213f663ddde0e1",
      "f2f1cc2598444dcf8ebd0d9638b1f9b3",
      "dc2ca14b44ff48bcb707c36b1f0a4738",
      "b18d849fcdbf462facf1d188d7ee8bc9",
      "01b3b4f794934673907d2a99a7aafcf2",
      "c36619dd9aea4a45895e8382d45940df",
      "e070d91c40654089b4daa5851fbb73db",
      "3b71790773194746a357ba9d2c25d04e",
      "05029e9ac5d64343b021e15aedcc6a58",
      "fa5b986a8c9b4221828a8e0d5d74bcf9",
      "730b2b769c1c4489891fdac6a977c33f",
      "45ad4d74c30e472eb5d9d9299083ec3f",
      "ed5e0d104b02488bba9bcd101cf1a8ce",
      "694e375fb7b64b979cfd049aa984d15e",
      "72026cb44fbe42658e7c3565f0e36501",
      "949265c8282c4f8d818b30aed3e6f0d8",
      "f9beb81bdb024d9dab64f6bb5b1edc48",
      "77023adc14c044bfb3e79190dc34a4f3",
      "adfbb988f3a5495bb09f07f83f44646b",
      "b77d1f6505e74d80a9293b9b134e3122",
      "b36f6dbb2c5e4c3d95b849535e8ce08e",
      "b841db9bfab74197a497995e9897e4d5",
      "7618b92b1bf94b50a99b5615a6f611af",
      "760b85ae61d34ce2883b57951024c695",
      "1fec0f43ea034c40b8ce8433db2b8896",
      "0900c9aa90174baa99097eff8e91dadd",
      "6c16b80fd19f486ab0b0d250d3b98b43",
      "5127f9475dcc476f9408cfc6eae8127d",
      "945b6a154d4d4d47a8e918c42d68200b",
      "1b4a938dc9004f4a877b3619776a0350",
      "0d37f63bc1084cdbb4a5329d617e1347",
      "4b0fda11cbf34daa8e9d15c3ddd5d433",
      "6752e27532df4a9093bffa35d4f3949d"
     ]
    },
    "id": "99e07987",
    "outputId": "0e6aa6b8-3b4c-447f-8c8b-f3036ea0a7a0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed83e1ed41c748cf9a47e17913801469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d270a55583344839b338243c1caa6888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d16f215d75b42ef900541e750227291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef7b99bcb2445f6bb0976f2d997aaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f8b2ad2aef4617be213f663ddde0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45ad4d74c30e472eb5d9d9299083ec3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7618b92b1bf94b50a99b5615a6f611af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  3%|â–Ž         | 1/30 [00:58<28:19, 58.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "  7%|â–‹         | 2/30 [02:03<29:06, 62.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 10%|â–ˆ         | 3/30 [03:03<27:31, 61.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 13%|â–ˆâ–Ž        | 4/30 [04:00<25:48, 59.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 17%|â–ˆâ–‹        | 5/30 [04:57<24:25, 58.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 20%|â–ˆâ–ˆ        | 6/30 [05:54<23:14, 58.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 23%|â–ˆâ–ˆâ–Ž       | 7/30 [06:51<22:07, 57.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [07:45<20:47, 56.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 30%|â–ˆâ–ˆâ–ˆ       | 9/30 [08:53<21:02, 60.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [09:48<19:31, 58.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 11/30 [10:51<18:56, 59.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [11:44<17:19, 57.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 13/30 [12:47<16:50, 59.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 14/30 [13:39<15:15, 57.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 15/30 [14:32<13:57, 55.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [15:21<12:34, 53.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 17/30 [16:19<11:54, 54.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 18/30 [17:15<11:02, 55.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 19/30 [18:07<09:58, 54.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [19:06<09:17, 55.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 21/30 [20:11<08:45, 58.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 22/30 [21:12<07:55, 59.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 23/30 [22:05<06:42, 57.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [23:02<05:43, 57.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 25/30 [24:01<04:49, 57.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 26/30 [25:07<04:01, 60.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 27/30 [25:56<02:50, 56.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [26:54<01:54, 57.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 29/30 [27:52<00:57, 57.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [28:46<00:00, 57.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "model_name = \"gpt2-medium\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=-1\n",
    ")\n",
    "\n",
    "test_dataset = dataset_10k[\"test\"]\n",
    "\n",
    "expected_codes_gpt2 = []\n",
    "generated_codes_gpt2 = []\n",
    "\n",
    "for example in tqdm(test_dataset.select(range(30))):\n",
    "    full_text = example[\"text\"]\n",
    "\n",
    "    try:\n",
    "        parts = full_text.split(\"### Problem:\\n\")[1].split(\"### Solution:\\n\")\n",
    "        description = parts[0].strip()\n",
    "        real_code = parts[1].strip()\n",
    "    except (IndexError, AttributeError):\n",
    "        continue\n",
    "\n",
    "    prompt = f\"### Problem:\\n{description}\\n### Solution:\\n\"\n",
    "\n",
    "    if not prompt.strip():\n",
    "        continue\n",
    "\n",
    "    max_prompt_tokens = 768\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_prompt_tokens)\n",
    "    prompt = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "    try:\n",
    "        output = generator(prompt, max_new_tokens=256, num_return_sequences=1, do_sample=False)[0][\"generated_text\"]\n",
    "    except RuntimeError as e:\n",
    "        print(\"GPU error:\", e)\n",
    "        continue\n",
    "\n",
    "    generated_code = output.split(\"### Solution:\\n\")[-1].strip()\n",
    "\n",
    "    expected_codes_gpt2.append(real_code)\n",
    "    generated_codes_gpt2.append(generated_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b3c1ef",
   "metadata": {
    "id": "c7b3c1ef"
   },
   "outputs": [],
   "source": [
    "df_codes = pd.DataFrame({\n",
    "    \"reference_code\": expected_codes_gpt2,\n",
    "    \"generated_code\": generated_codes_gpt2\n",
    "})\n",
    "\n",
    "df_codes.to_csv(\"codes_comparison_gpt2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ab7adf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "52ab7adf",
    "outputId": "937ca749-617d-4135-c43c-0adc7b287d71"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=eef5c3cff69b0071032aab654ac2062804a828d7fa5b82cf4afb07be1ae8d809\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0e0de6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fe0e0de6",
    "outputId": "eb2bba63-a988-442f-b496-f7e4818f4a1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f3a92e",
   "metadata": {
    "id": "74f3a92e"
   },
   "source": [
    "Here's the function to calculate the metrics. Exact Match checks if the generated code is exactly the same as the reference. Levenshtein Similarity measures how similar two strings are based on the number of edit operations needed to match them. BLEU Score evaluates n-gram overlap between generated and reference texts, commonly used in machine translation. ROUGE-L captures the longest common subsequence, focusing on the structural similarity between outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41381556",
   "metadata": {
    "id": "41381556"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "def exact_match(pred, ref):\n",
    "    return pred.strip() == ref.strip()\n",
    "\n",
    "def levenshtein_ratio(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def bleu_score(pred, ref):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(pred), smoothing_function=smoothie)\n",
    "\n",
    "def rouge_l_score(pred, ref):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    return scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "\n",
    "def compare_all(expected_codes, generated, label=\"\"):\n",
    "    print(f\"\\n=== Metrics for: {label} ===\")\n",
    "\n",
    "    em_list = []\n",
    "    lev_list = []\n",
    "    bleu_list = []\n",
    "    rouge_list = []\n",
    "\n",
    "    for pred, ref in zip(generated, expected_codes):\n",
    "        em_list.append(exact_match(pred, ref))\n",
    "        lev_list.append(levenshtein_ratio(pred, ref))\n",
    "        bleu_list.append(bleu_score(pred, ref))\n",
    "        rouge_list.append(rouge_l_score(pred, ref))\n",
    "\n",
    "    print(f\"Exact Match: {np.mean(em_list):.3f}\")\n",
    "    print(f\"Levenshtein Similarity: {np.mean(lev_list):.3f}\")\n",
    "    print(f\"BLEU Score: {np.mean(bleu_list):.3f}\")\n",
    "    print(f\"ROUGE-L Score: {np.mean(rouge_list):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfa697",
   "metadata": {
    "id": "b8dfa697"
   },
   "outputs": [],
   "source": [
    "codes_finetuned = pd.read_csv('codes_comparison.csv')\n",
    "#codes_gpt2 = pd.read_csv('code_comparison_gpt2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce98833",
   "metadata": {
    "id": "0ce98833"
   },
   "outputs": [],
   "source": [
    "expected_codes = codes_finetuned[\"reference_code\"].tolist()\n",
    "generated_codes_finetuned = codes_finetuned[\"generated_code\"].tolist()\n",
    "\n",
    "#expected_codes_gpt2 = codes_gpt2[\"reference_code\"].tolist()\n",
    "#generated_codes_gpt2 = codes_gpt2[\"generated_code\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1a720",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6dd1a720",
    "outputId": "239f30ba-ad21-481f-e767-a648ecc0e06d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics for: GPT-2 Original ===\n",
      "Exact Match: 0.000\n",
      "Levenshtein Similarity: 0.026\n",
      "BLEU Score: 0.004\n",
      "ROUGE-L Score: 0.057\n",
      "\n",
      "=== Metrics for: GPT-2 Fine-tuned ===\n",
      "Exact Match: 0.000\n",
      "Levenshtein Similarity: 0.282\n",
      "BLEU Score: 0.157\n",
      "ROUGE-L Score: 0.326\n"
     ]
    }
   ],
   "source": [
    "compare_all(expected_codes_gpt2, generated_codes_gpt2, label=\"GPT-2 Original\")\n",
    "compare_all(expected_codes, generated_codes_finetuned, label=\"GPT-2 Fine-tuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d30f5",
   "metadata": {
    "id": "695d30f5"
   },
   "source": [
    "The metrics show that the fine-tuned GPT-2 model significantly outperforms the original GPT-2 in generating code closer to the expected solutions. While both models have an Exact Match score of 0, indicating no perfect matches, the fine-tuned model achieves much higher similarity scores across Levenshtein, BLEU, and ROUGE-L metrics. This suggests that fine-tuning helps the model produce code that is structurally and lexically more similar to the reference, improving overall generation quality even if exact reproduction is rare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f87278",
   "metadata": {
    "id": "e2f87278"
   },
   "source": [
    "## Fine tuning TinyLlama\n",
    "\n",
    "TinyLlama-1.1B-Chat-v1.0 (https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) is a compact, chat-optimized language model with 1.1B parameters, based on the Llama 2 architecture. It was pretrained on 3T tokens and fine-tuned using UltraChat and UltraFeedback datasets to improve dialogue quality. Despite its small size, it performs well in chat tasks, especially on limited hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f00f211",
   "metadata": {
    "id": "3f00f211"
   },
   "source": [
    "### Imports\n",
    "\n",
    "We start by doing the necessary library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ef89c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "44ef89c7",
    "outputId": "05bb7773-4a4a-4860-d511-316e0bcf139c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q peft accelerate transformers datasets bitsandbytes trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0106ef94",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0106ef94",
    "outputId": "1013c7c3-12a0-41cb-f215-9e9222a3ede9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75af3f7",
   "metadata": {
    "id": "b75af3f7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import pandas as pd\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from google.colab import drive\n",
    "import os\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13e35ec",
   "metadata": {
    "id": "b13e35ec"
   },
   "source": [
    "### Load the model\n",
    "\n",
    "Before we do the fine-tuning, let's see how the pretrained model does on our task of code generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b6d49b",
   "metadata": {
    "id": "92b6d49b"
   },
   "source": [
    "In this next cell we will create a text-generation pipeline with the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bab61d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369,
     "referenced_widgets": [
      "eecadc090b1b42b3a4c1703da53df287",
      "2534366b3e404c26b921f399b08a2aee",
      "dafc40c78c744fd7935987a9af215a7a",
      "82f4d81d218747c09acba954f056250e",
      "fb1a2ab23c4e4d69827d3e65bf572981",
      "82f7dabcc825434a8373a15b4df33dfa",
      "ef0aa683430f43e1a36c9139c2b469eb",
      "65d1ce86bb354c5894dba73f87047506",
      "51fa7908ac1649f7833b414297a3c2c1",
      "78cd0618810e43969b1802f5619084b8",
      "91a44d595650442bbca3a6ea74f62db6",
      "b8cf4592769247a38ee657c26507dc6a",
      "c1c467c195ee4ccfb7cdb54a17ae684d",
      "7492c046e02a46a1b440afddefcb023f",
      "04ddc4aaab1e4019bdf6eec9288dbaa7",
      "b8368c2229e84f0c91eaed240d8cd4dd",
      "cb88cb6396e74c3ebfc21b7bc1d04626",
      "2e53664623c34ef0ae49f7cb33d1b93e",
      "a0b09fbad3074af0a3c59c789f131f60",
      "79cf95a9af5e43ed9d9efb5aab70d762",
      "e51dd3c892aa49188fb37c6f080fc280",
      "52043f8d440842f7a2580757ec37cd04",
      "417c909e6e3e49bb926dcedbc326b90e",
      "d3d05551a89f48cfaeab36f2a34cc63d",
      "aa9d4d747a284edabd190bb63de461d7",
      "da82c898776c4ad6b9650c4d38ca2d2c",
      "cc47b67c54be417f8ec1067461083102",
      "1724304ac1dc41cebc900a1d16894f98",
      "80e623612ddb40b4adce28b2540ed2ff",
      "7a02668878c8459c8872637802b0acda",
      "0f6bd9a9348e41e0be86630a7d4be91b",
      "2637c7ee64b640d2a24513e68068082c",
      "684463e481144d43bdd6301dad753f2f",
      "063c83b9041f4dda9ed5d79e05a856b2",
      "5c1ab7f232b54620b789df0ad3c2c94c",
      "f37ad61971eb46b7be79b846ecb8d746",
      "d7561a3abd404f23b7ee3eedd09b22b8",
      "73e2d80bbe6a4b79b7bb8f0d165afcf2",
      "ff099077bb0f4fadb9ec06932189f02a",
      "02011c41ea9844f1a998f1ded293929d",
      "9a141e7fb5424e81b468e405781954c5",
      "0d51d00a04d74820937ea625ee7d5ada",
      "c87f92f79aee436fa5ce85b0f467ab01",
      "54f3a36990b446a598b16c55c6b3a942",
      "2f3324642780460c841c49c8124a8511",
      "1a3db02b40314fad9afc138199d61ed8",
      "975136e433944a70a8f13052a8ae207e",
      "596e29de49cf484c8ab4c3a1b6aca745",
      "b98f38cc1ff345258560520e4a1db594",
      "7d41929dbf244966879d1663ed1112a8",
      "d9a1e5308e2e4be3a1a19ebcee181a60",
      "25657f59f2d44e2195cea51ecb3d9e57",
      "1173fa03d3b94a408b456b54ac8c5f4a",
      "af49bb0c9dd4454b9674d3824b8210dd",
      "8fc428194c514719839d00971354f295",
      "ca8dfa0593724bcabc310538271d39e3",
      "7493a7ca2da54771873e8eebe246ca89",
      "1938c68d0dd14ebb873456fb9d08d48c",
      "2b89f76b31aa48e8be8e8a5a83c71ede",
      "3b8f307172da478d8a8fc13c6035c2d9",
      "27066d146b524ca9a54fe9994bd120cf",
      "9adc15331c49484d9665aebbbc18bada",
      "c041367bb6ec4cef870b02a4eb95cce8",
      "7cb75d156923435c9645be61cb61ad31",
      "482351c57f28447ea4d9af250c5b3f28",
      "e89cf61617274ebd8fe4ee7072bb32dc",
      "d6429252274d48d28d46a857e7dfcf0f",
      "c2da9cf8384d4411ac18839e8cd4cc8b",
      "98d428ee68114cd98a228d0af5c8e999",
      "b6053e8a17924275bc0ba3a095230c34",
      "3b7f1096eb224f4ba3c6d18d687cf846",
      "aad9ba88663d43d4b12e95ca3a0ac761",
      "65fc8e82a3404afdbfe1d27b45badbeb",
      "27f63169528949f0b27c4d7f9c28dbf6",
      "9a1513c407284eadb4ba3594d38ee344",
      "bb174bc3afde4ae2b4104ab83e6d6f6c",
      "d1e1b86eb0804ca0ba7196945b90a47d"
     ]
    },
    "id": "a4bab61d",
    "outputId": "0d6b62d0-96fe-4ffb-d724-c46953d6106f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eecadc090b1b42b3a4c1703da53df287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8cf4592769247a38ee657c26507dc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "417c909e6e3e49bb926dcedbc326b90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063c83b9041f4dda9ed5d79e05a856b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3324642780460c841c49c8124a8511",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca8dfa0593724bcabc310538271d39e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6429252274d48d28d46a857e7dfcf0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37534b58",
   "metadata": {
    "id": "37534b58"
   },
   "source": [
    "And we will pass as the messages a problem that is not in our bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0729a62e",
   "metadata": {
    "id": "0729a62e"
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c9c817",
   "metadata": {
    "id": "b4c9c817"
   },
   "source": [
    "Now we will generate the output using the messages above as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd250cf4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cd250cf4",
    "outputId": "46b03182-1aff-44ed-b28d-6fe454ed5227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful Python assistant.</s>\n",
      "<|user|>\n",
      "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.</s>\n",
      "<|assistant|>\n",
      "Here's a Python function that returns 'YES' if it's possible to split a watermelon of weight `w` into two parts, each of even positive weight, and 'NO' otherwise:\n",
      "\n",
      "```python\n",
      "def is_splitable(w):\n",
      "    \"\"\"\n",
      "    Takes a weight `w` and returns 'YES' if it's possible to split the watermelon into two parts,\n",
      "    each of even positive weight, or 'NO' otherwise.\n",
      "    \"\"\"\n",
      "    if w <= 0:\n",
      "        return \"Watermelon is not splitable!\"\n",
      "    return \"YES\" if (w % 2) == 0 else \"NO\"\n",
      "```\n",
      "\n",
      "Here's an example usage:\n",
      "\n",
      "```python\n",
      "watermelon_wgt = 100\n",
      "is_splitable = is_splitable(watermelon_wgt)\n",
      "print(is_splitable)  # Output: YES\n",
      "```\n",
      "\n",
      "This function takes an integer `w` (1 <= w <= 100) as input and returns either 'YES' (if it's possible to split the watermelon into two\n"
     ]
    }
   ],
   "source": [
    "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "print(outputs[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359baa7d",
   "metadata": {
    "id": "359baa7d"
   },
   "source": [
    "We can see that the model already produces python code for the task, as request in the system prompt. It also tries to explain the code generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0c933b",
   "metadata": {
    "id": "0e0c933b"
   },
   "source": [
    "### Load the dataset\n",
    "\n",
    "Now we will load out dataset to start the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b4fb7b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "67b4fb7b",
    "outputId": "19eca7c0-db8c-438f-89d8-e2e9323cc854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/.shortcut-targets-by-id/17WgJO1gfIBADpYX2jVdb41q7HCbwWcOU/NLP_Project'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drive.mount('/content/drive')\n",
    "\n",
    "path = 'Colab Notebooks/NLP/NLP_Project'\n",
    "\n",
    "os.chdir(f'/content/drive/MyDrive/{path}')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04310a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bb04310a",
    "outputId": "d5c379a1-6eba-4dd0-8624-b12093d09ec0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 problem_description solution_id  \\\n",
      "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
      "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
      "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
      "3  Xenia has a set of weights and pan scales. Eac...         0_6   \n",
      "4  Xenia has a set of weights and pan scales. Eac...         0_8   \n",
      "\n",
      "                                       solution_code  \\\n",
      "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
      "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
      "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
      "3  MOD = 10**9 + 7\\nI = lambda:list(map(int,input...   \n",
      "4  to_print = []\\ndef dfs(d, ini, s, depth, m):\\n...   \n",
      "\n",
      "               problem_name time_complexity_inferred space_complexity_inferred  \n",
      "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
      "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "3  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
      "4  339_C. Xenia and Weights                     O(1)                      O(1)  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('final_ds.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52365ea",
   "metadata": {
    "id": "a52365ea"
   },
   "source": [
    "### Fine tuning with 1000 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a30fe27",
   "metadata": {
    "id": "8a30fe27"
   },
   "source": [
    "First we will start with a small example, doing the fine-tuning with 1000 samples of the dataset. In this next cell we prepare the text that we will give to the model in the training putting together the descriptions and the code solutions in the messages format, and splitting it into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a24cff",
   "metadata": {
    "id": "f5a24cff"
   },
   "outputs": [],
   "source": [
    "df_small = df[[\"problem_description\", \"solution_code\"]].dropna().sample(1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf5040",
   "metadata": {
    "id": "ebaf5040"
   },
   "outputs": [],
   "source": [
    "def format_messages(row):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": row[\"problem_description\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"solution_code\"]}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7be7617",
   "metadata": {
    "id": "c7be7617"
   },
   "outputs": [],
   "source": [
    "df_small[\"messages\"] = df_small.apply(format_messages, axis=1)\n",
    "dataset = Dataset.from_pandas(df_small[[\"messages\"]])\n",
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322cc1a",
   "metadata": {
    "id": "3322cc1a"
   },
   "source": [
    "Here we define the original model name and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0c5a9",
   "metadata": {
    "id": "4fa0c5a9"
   },
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bb00a",
   "metadata": {
    "id": "dd0bb00a"
   },
   "source": [
    "This next function is used to tokenize the prepared dataset with the model's tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedbd6ba",
   "metadata": {
    "id": "bedbd6ba"
   },
   "outputs": [],
   "source": [
    "def apply_chat_template(example):\n",
    "    prompt = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d8159",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "25b465778d464fce93eb26ddb1c84454",
      "30d47bdbcf4f4c799f37914feeab92e5",
      "cfa606017dc04af3925a8f17101a61fb",
      "20fdb380f149478bba6e8a3ccda23abd",
      "dc3738ae56f34e1c9839282fc92d789e",
      "5245bbe0dd974d7da5cdbf394f1e041c",
      "b1eb89b0688d40128f9a3756a677cd31",
      "4e1ea2daef0f4d0299ed98dc66e62ed3",
      "a47978c61a964d12adb7779a5cf8fc81",
      "57bad2c2e87a482aad3895e7e2f48a7c",
      "8dfb51fb55fa483ca3c46c024b1062dc",
      "056990113e454b8a841fd46191bb6d53",
      "00eb3bf4cee6476e90c9e9ad5e502c91",
      "484d6da962824dbbb7a7d48c3059cfc2",
      "1a9eba7a81c44941b55866139630b8a5",
      "12b0b34ab220484dbf0aaa24b5f6358e",
      "fc176e05515649ceae590f6fb57620d5",
      "912a5349a92f41d08e796106b44bc467",
      "e135619ee45c4165a74c1302e310a641",
      "585e6c6adf304eb2b064efeb03fd9683",
      "4705379ccc0d4f50a421b6b9a3e12532",
      "3bbc6c038d1b42f7b54e07babf59109c"
     ]
    },
    "id": "642d8159",
    "outputId": "ddd55527-a730-4cac-98e8-f0545d9a163a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b465778d464fce93eb26ddb1c84454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056990113e454b8a841fd46191bb6d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(apply_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f11ef43",
   "metadata": {
    "id": "5f11ef43"
   },
   "source": [
    "This next cell loads the base model in 4-bit precision and prepares it for k-bit training. LoRA adapters are configured to fine-tune only the q_proj and v_proj layers in a memory-efficient way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a5cf6a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1a5cf6a",
    "outputId": "92217805-2721-42f0-8331-171a4751ca01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b016f0f",
   "metadata": {
    "id": "0b016f0f"
   },
   "source": [
    "In this next step we create the Trainer, defining the training arguments and the model to be trained. We will train 2 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf7c82",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5edf7c82",
    "outputId": "d318af0c-d467-4744-940c-42cb8390f3e8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-d8dc1c25d451>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-chat-finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=100,\n",
    "    eval_steps=250,\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5121254",
   "metadata": {
    "id": "e5121254"
   },
   "source": [
    "Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a0589",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "cc6a0589",
    "outputId": "502669c9-97e2-44c8-c999-329fe05011d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 11:28, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.176600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.178200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.132600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.160100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.147200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.150500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.059700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=1.1935513051350912, metrics={'train_runtime': 689.5834, 'train_samples_per_second': 2.61, 'train_steps_per_second': 1.305, 'total_flos': 5726668220006400.0, 'train_loss': 1.1935513051350912, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821cb03a",
   "metadata": {
    "id": "821cb03a"
   },
   "source": [
    "After training we save the model generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d087ae",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "88d087ae",
    "outputId": "b9854122-0b74-43f6-e1d5-cab747bada96"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tinyllama-lora-finetuned/tokenizer_config.json',\n",
       " 'tinyllama-lora-finetuned/special_tokens_map.json',\n",
       " 'tinyllama-lora-finetuned/tokenizer.model',\n",
       " 'tinyllama-lora-finetuned/added_tokens.json',\n",
       " 'tinyllama-lora-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"tinyllama-lora-finetuned\")\n",
    "tokenizer.save_pretrained(\"tinyllama-lora-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1815d",
   "metadata": {
    "id": "6de1815d"
   },
   "source": [
    "Now we will generate an output with the same prompt from the beggining to see how the finetuned model behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39b33f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7c39b33f",
    "outputId": "36b9ba35-aae2-4789-9254-08772de46db6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful Python assistant.</s>\n",
      "<|user|>\n",
      "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.</s>\n",
      "<|assistant|>\n",
      "w = int(input())\n",
      "if w % 2 == 0:\n",
      "    if w == 0:\n",
      "        print('NO')\n",
      "        return\n",
      "    print('YES')\n",
      "else:\n",
      "    print('NO')\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, \"tinyllama-lora-finetuned\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "output = pipe(prompt, max_new_tokens=256, do_sample=True)\n",
    "print(output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00600a6",
   "metadata": {
    "id": "a00600a6"
   },
   "source": [
    "We can see that the model generated the correct code, but now without the explanations, which is more aligned with the examples from the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60f0f80",
   "metadata": {
    "id": "f60f0f80"
   },
   "source": [
    "### Fine tuning with 5000 samples\n",
    "\n",
    "Now we will try the finetuning with a little more samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe62ab",
   "metadata": {
    "id": "a2fe62ab"
   },
   "source": [
    "We take 5000 samples of the dataset, divide again into train and test set, apply the messages format and tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161475c8",
   "metadata": {
    "id": "161475c8"
   },
   "outputs": [],
   "source": [
    "df_medium = df[[\"problem_description\", \"solution_code\"]].dropna().sample(5000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998723b",
   "metadata": {
    "id": "4998723b"
   },
   "outputs": [],
   "source": [
    "df_medium[\"messages\"] = df_medium.apply(format_messages, axis=1)\n",
    "dataset_medium = Dataset.from_pandas(df_medium[[\"messages\"]])\n",
    "dataset_medium = dataset_medium.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643de157",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "7f6aa18a31844dc78a811c9c6e30a067",
      "fde2d5539d844dd097e6a307a015d3f2",
      "89b678fd2c3546d08c4cee74301dd593",
      "c6bbefd0d5634d13a23a261ef1dd881c",
      "8172a4e9be5a40fd9c6a8a0579697ecc",
      "560277464c1345dd969ccc26e4086f96",
      "681001fe9b264bd28114502ad0e1955f",
      "6286ae9e5b684d39ba630b73e246794c",
      "906a2f374581433b9e66a992a01ddd33",
      "4896f62801a1441d853faa6145218568",
      "6e1d592f886a4e25a5e4db978eaaf1de",
      "f2ba02c66ebb429daf8d34b5b2dbb0d9",
      "31aac9b09562456b9a100da237add872",
      "d2399b0894094f4aabf3f69021611597",
      "953e0883e53d4784ad59146a80c9ef56",
      "8341ecf8a0534159a89e0c418701b600",
      "45798f3b912c4f9aa68751b590acef96",
      "2b8a95e2ca5b441a9658ce1678a432c5",
      "2b63ced9bf0e4df3ac1d7f6332a588d9",
      "81b922f35dd64c91ae41c4bf1aa0e6c1",
      "4e2b5cfd695746368c89d0ad5c324660",
      "cae07ae9320c4e9fa1203a75de62bc2f"
     ]
    },
    "id": "643de157",
    "outputId": "aac7e59d-fbea-46d2-a914-96df6f74efdf"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f6aa18a31844dc78a811c9c6e30a067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2ba02c66ebb429daf8d34b5b2dbb0d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset_medium = dataset_medium.map(apply_chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b02a742",
   "metadata": {
    "id": "3b02a742"
   },
   "source": [
    "Here we define again the training arguments and the Trainer, taking the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964c6e18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "964c6e18",
    "outputId": "2f39423a-0f84-41a0-913d-adcabbdb353d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-15-a8e69201f326>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-chat-finetuned-mediumds\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=100,\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset_medium[\"train\"],\n",
    "    eval_dataset=tokenized_dataset_medium[\"test\"],\n",
    "    tokenizer=tokenizer\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df34dda0",
   "metadata": {
    "id": "df34dda0"
   },
   "source": [
    "And start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21454e44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "21454e44",
    "outputId": "9a6c47b8-bc65-4fdd-867a-2c9f28744f1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 55:00, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.138800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.161600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.112300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.087900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.139900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.139700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.082400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.120700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.086400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.066400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.082600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.088600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.093000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.082700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.091400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.042900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.069700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.055500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.044900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.047300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.019000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.051400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.076600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.055900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.044200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.048700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.013700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.015800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4500, training_loss=1.0915350663926866, metrics={'train_runtime': 3303.0539, 'train_samples_per_second': 2.725, 'train_steps_per_second': 1.362, 'total_flos': 2.8633341100032e+16, 'train_loss': 1.0915350663926866, 'epoch': 2.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fac32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "734fac32",
    "outputId": "e75cb216-7ae1-4ace-fe83-9696270fcdd6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 00:58]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0485718250274658,\n",
       " 'eval_runtime': 58.3072,\n",
       " 'eval_samples_per_second': 8.575,\n",
       " 'eval_steps_per_second': 4.288,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b7c71",
   "metadata": {
    "id": "900b7c71"
   },
   "source": [
    "Now we will generate again an output for the first prompt to see how the model behaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34c025e",
   "metadata": {
    "id": "c34c025e"
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "                                bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "adapter_path = \"./tinyllama-chat-finetuned-mediumds/checkpoint-4500\"\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc2ddb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cc2ddb7",
    "outputId": "fe670740-c532-4a80-eb82-e75f9313a96f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|system|>\n",
      "You are a helpful Python assistant.</s>\n",
      "<|user|>\n",
      "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.</s>\n",
      "<|assistant|>\n",
      "def is_positive_even(w):\n",
      "    if 1000/w == 54:\n",
      "        return True\n",
      "    if w % 2 == 0:\n",
      "        return True\n",
      "    \n",
      "    \n",
      "print(\"YES\", (is_positive_even(w) == True))\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.\"}\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "output = pipe(prompt, max_new_tokens=256, do_sample=True)\n",
    "\n",
    "print(output[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf596b10",
   "metadata": {
    "id": "bf596b10"
   },
   "source": [
    "Now we will run some metrics on the test set using 20 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcea66fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bcea66fd",
    "outputId": "04cf0c2c-153c-4057-89ba-8dd4ea577fae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=bc8f39e186df29ce86ecfae01a2a7b7de1c6a5340fdcbcaa23126f28de3a6836\n",
      "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: rouge_score\n",
      "Successfully installed rouge_score-0.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42595d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8f42595d",
    "outputId": "76530d32-2235-4aa7-d675-91ed39e831b1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf0eba",
   "metadata": {
    "id": "59cf0eba"
   },
   "source": [
    "Here's the function to calculate the metrics. Exact Match checks if the generated code is exactly the same as the reference. Levenshtein Similarity measures how similar two strings are based on the number of edit operations needed to match them. BLEU Score evaluates n-gram overlap between generated and reference texts, commonly used in machine translation. ROUGE-L captures the longest common subsequence, focusing on the structural similarity between outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f926664a",
   "metadata": {
    "id": "f926664a"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "import numpy as np\n",
    "import difflib\n",
    "\n",
    "def exact_match(pred, ref):\n",
    "    return pred.strip() == ref.strip()\n",
    "\n",
    "def levenshtein_ratio(a, b):\n",
    "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "def bleu_score(pred, ref):\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    return sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(pred), smoothing_function=smoothie)\n",
    "\n",
    "def rouge_l_score(pred, ref):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    return scorer.score(ref, pred)['rougeL'].fmeasure\n",
    "\n",
    "def compare_all(expected_codes, generated, label=\"\"):\n",
    "    print(f\"\\n=== Metrics for: {label} ===\")\n",
    "\n",
    "    em_list = []\n",
    "    lev_list = []\n",
    "    bleu_list = []\n",
    "    rouge_list = []\n",
    "\n",
    "    for pred, ref in zip(generated, expected_codes):\n",
    "        em_list.append(exact_match(pred, ref))\n",
    "        lev_list.append(levenshtein_ratio(pred, ref))\n",
    "        bleu_list.append(bleu_score(pred, ref))\n",
    "        rouge_list.append(rouge_l_score(pred, ref))\n",
    "\n",
    "    print(f\"Exact Match: {np.mean(em_list):.3f}\")\n",
    "    print(f\"Levenshtein Similarity: {np.mean(lev_list):.3f}\")\n",
    "    print(f\"BLEU Score: {np.mean(bleu_list):.3f}\")\n",
    "    print(f\"ROUGE-L Score: {np.mean(rouge_list):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9775d4d",
   "metadata": {
    "id": "a9775d4d"
   },
   "source": [
    "Now we will generate the outputs for the original model and for the finetuned model, and run the metrics function on the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe5527e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0fe5527e",
    "outputId": "e101a5a9-9833-46ff-c08c-97437ac83224"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [02:25<02:00, 12.01s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [05:01<00:00, 15.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Metrics for: TinyLlama Original ===\n",
      "Exact Match: 0.000\n",
      "Levenshtein Similarity: 0.178\n",
      "BLEU Score: 0.105\n",
      "ROUGE-L Score: 0.239\n",
      "\n",
      "=== Metrics for: TinyLlama Fine-tuned ===\n",
      "Exact Match: 0.000\n",
      "Levenshtein Similarity: 0.202\n",
      "BLEU Score: 0.148\n",
      "ROUGE-L Score: 0.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "def extract_code_from_output(output_text):\n",
    "    if \"<|assistant|>\" in output_text:\n",
    "        return output_text.split(\"<|assistant|>\")[-1].strip()\n",
    "    else:\n",
    "        return output_text.strip()\n",
    "\n",
    "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
    "                               bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "adapter_path = \"./tinyllama-chat-finetuned-mediumds/checkpoint-4500\"\n",
    "finetuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "pipe_orig = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "pipe_ft = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "test_dataset = dataset_medium[\"test\"].select(range(20))\n",
    "\n",
    "expected_codes = []\n",
    "generated_orig = []\n",
    "generated_ft = []\n",
    "\n",
    "for example in tqdm(test_dataset):\n",
    "    messages = example[\"messages\"]\n",
    "    description = \"\"\n",
    "    real_code = \"\"\n",
    "\n",
    "    for m in messages:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            description = m[\"content\"].strip()\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            real_code = m[\"content\"].strip()\n",
    "\n",
    "    if not description or not real_code:\n",
    "        continue\n",
    "\n",
    "    prompt_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": description}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    output_orig = pipe_orig(prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
    "    code_orig = extract_code_from_output(output_orig)\n",
    "\n",
    "    output_ft = pipe_ft(prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
    "    code_ft = extract_code_from_output(output_ft)\n",
    "\n",
    "    expected_codes.append(real_code)\n",
    "    generated_orig.append(code_orig)\n",
    "    generated_ft.append(code_ft)\n",
    "\n",
    "df_orig = pd.DataFrame({\n",
    "    \"reference_code\": expected_codes,\n",
    "    \"generated_code_original\": generated_orig\n",
    "})\n",
    "df_orig.to_csv(\"codes_comparison_tinyllama_original.csv\", index=False)\n",
    "\n",
    "df_ft = pd.DataFrame({\n",
    "    \"reference_code\": expected_codes,\n",
    "    \"generated_code_finetuned\": generated_ft\n",
    "})\n",
    "df_ft.to_csv(\"codes_comparison_tinyllama_finetuned.csv\", index=False)\n",
    "\n",
    "compare_all(expected_codes, generated_orig, label=\"TinyLlama Original\")\n",
    "compare_all(expected_codes, generated_ft, label=\"TinyLlama Fine-tuned\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a872c9",
   "metadata": {
    "id": "e3a872c9"
   },
   "source": [
    "The evaluation metrics show that the fine-tuned TinyLlama model outperforms the original in all measured aspects: Levenshtein similarity increased from 0.178 to 0.202, BLEU score improved from 0.105 to 0.148, and ROUGE-L rose from 0.239 to 0.300. Although these improvements are moderate, they reflect consistent gains in code generation quality. The relatively small increase is expected because the base model was already capable of generating Python code to some extent before fine-tuning."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
