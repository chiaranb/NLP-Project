{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24a66d6c"
   },
   "source": [
    "# Task 2: Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning of the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VF8q5DVSP65"
   },
   "source": [
    "Mount the google drive folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1900,
     "status": "ok",
     "timestamp": 1746728417226,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "XeIabhZ8STiN",
    "outputId": "3f7464be-9d25-4d8f-80a9-5ce4ff7254ec"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e0979823"
   },
   "source": [
    "### Dataset cleaning process\n",
    "\n",
    "The aim of this section is to create a single dataset using the two datasets at our disposal to simplify the retrieval of the information. Specifically we are interested in the descriptions of the problems, the various examples for the solutions and the respective time and space complexities. The aim is to simplify the retrieval of the information contained in the datasets. We then proceed to eliminate the na values, fix the index, and eliminate the comments from the code solutions.\n",
    "\n",
    "\n",
    "First, the datasets problem_and_human_solutions_list and complexity_labels_light are downloaded and turned into DataFrames.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "executionInfo": {
     "elapsed": 4305,
     "status": "ok",
     "timestamp": 1746728421953,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "670417a4"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15508,
     "status": "ok",
     "timestamp": 1746728437457,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "6afa05bb",
    "outputId": "f07160d1-bc41-457d-b3f8-0e1bbdcc993a"
   },
   "outputs": [],
   "source": [
    "# Download the dataset from Hugging Face Hub\n",
    "file_path_1 = hf_hub_download(\n",
    "    repo_id=\"facebook/BigOBench\",\n",
    "    filename=\"data/problem_and_human_solutions_list.jsonl\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Read the JSONL file and convert it to a DataFrame\n",
    "df_problem_and_human_solutions_list = []\n",
    "with open(file_path_1, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            df_problem_and_human_solutions_list.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Erorr JSON:\", e)\n",
    "\n",
    "df_problem_and_human_solutions_list = pd.DataFrame(df_problem_and_human_solutions_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "executionInfo": {
     "elapsed": 7253,
     "status": "ok",
     "timestamp": 1746728444705,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "44287494"
   },
   "outputs": [],
   "source": [
    "# Download the second dataset from Hugging Face Hub\n",
    "file_path_2 = hf_hub_download(\n",
    "    repo_id=\"facebook/BigOBench\",\n",
    "    filename=\"data/complexity_labels_light.jsonl\",\n",
    "    repo_type=\"dataset\"\n",
    ")\n",
    "\n",
    "# Read the JSONL file and convert it to a DataFrame\n",
    "df_complexity_labels_light = []\n",
    "with open(file_path_2, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            df_complexity_labels_light.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Erorr JSON:\", e)\n",
    "\n",
    "df_complexity_labels_light = pd.DataFrame(df_complexity_labels_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 774,
     "status": "ok",
     "timestamp": 1746728445484,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "3f96c98f",
    "outputId": "2ff9110b-cdf2-441c-81ad-230da8180e59"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>description</th>\n",
       "      <th>correct_solution_list</th>\n",
       "      <th>data_source</th>\n",
       "      <th>source_specific_limits</th>\n",
       "      <th>codeforces_specific_metadata</th>\n",
       "      <th>tests</th>\n",
       "      <th>human_accuracy_rate</th>\n",
       "      <th>dataclass</th>\n",
       "      <th>complexity_framework</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>{'text': 'Xenia has a set of weights and pan s...</td>\n",
       "      <td>[{'solution_id': '0_0', 'solution_code': '__au...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 2, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 339, 'cf_index': 'C', 'cf_po...</td>\n",
       "      <td>{'public_tests': [{'input': '0000000101\n",
       "3\n",
       "', '...</td>\n",
       "      <td>0.281633</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.40579710144927...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1547_E. Air Conditioners</td>\n",
       "      <td>{'text': 'On a strip of land of length n there...</td>\n",
       "      <td>[{'solution_id': '1_0', 'solution_code': 'def ...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 2, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 1547, 'cf_index': 'E', 'cf_p...</td>\n",
       "      <td>{'public_tests': [{'input': '5\n",
       "\n",
       "6 2\n",
       "2 5\n",
       "14 16\n",
       "...</td>\n",
       "      <td>0.620833</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.11409395973154...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>268_C. Beautiful Sets of Points</td>\n",
       "      <td>{'text': 'Manao has invented a new mathematica...</td>\n",
       "      <td>[{'solution_id': '2_0', 'solution_code': 'if _...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 1, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 268, 'cf_index': 'C', 'cf_po...</td>\n",
       "      <td>{'public_tests': [{'input': '2 2\n",
       "', 'output': ...</td>\n",
       "      <td>0.525066</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.05527638190954...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>478_C. Table Decorations</td>\n",
       "      <td>{'text': 'You have r red, g green and b blue b...</td>\n",
       "      <td>[{'solution_id': '3_0', 'solution_code': 'a = ...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 1, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 478, 'cf_index': 'C', 'cf_po...</td>\n",
       "      <td>{'public_tests': [{'input': '1 1 1\n",
       "', 'output'...</td>\n",
       "      <td>0.562264</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.41610738255033...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5_C. Longest Regular Bracket Sequence</td>\n",
       "      <td>{'text': 'This is yet another problem dealing ...</td>\n",
       "      <td>[{'solution_id': '4_0', 'solution_code': 'stri...</td>\n",
       "      <td>CODEFORCES</td>\n",
       "      <td>{'time_limit': {'seconds': 2, 'nanos': 0}, 'me...</td>\n",
       "      <td>{'cf_contest_id': 5, 'cf_index': 'C', 'cf_poin...</td>\n",
       "      <td>{'public_tests': [{'input': ')((())))(()())\n",
       "',...</td>\n",
       "      <td>0.395939</td>\n",
       "      <td>{'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...</td>\n",
       "      <td>{'time_complexity_fail_rate': 0.03846153846153...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  problem_id                           problem_name  \\\n",
       "0          0               339_C. Xenia and Weights   \n",
       "1          1               1547_E. Air Conditioners   \n",
       "2          2        268_C. Beautiful Sets of Points   \n",
       "3          3               478_C. Table Decorations   \n",
       "4          4  5_C. Longest Regular Bracket Sequence   \n",
       "\n",
       "                                         description  \\\n",
       "0  {'text': 'Xenia has a set of weights and pan s...   \n",
       "1  {'text': 'On a strip of land of length n there...   \n",
       "2  {'text': 'Manao has invented a new mathematica...   \n",
       "3  {'text': 'You have r red, g green and b blue b...   \n",
       "4  {'text': 'This is yet another problem dealing ...   \n",
       "\n",
       "                               correct_solution_list data_source  \\\n",
       "0  [{'solution_id': '0_0', 'solution_code': '__au...  CODEFORCES   \n",
       "1  [{'solution_id': '1_0', 'solution_code': 'def ...  CODEFORCES   \n",
       "2  [{'solution_id': '2_0', 'solution_code': 'if _...  CODEFORCES   \n",
       "3  [{'solution_id': '3_0', 'solution_code': 'a = ...  CODEFORCES   \n",
       "4  [{'solution_id': '4_0', 'solution_code': 'stri...  CODEFORCES   \n",
       "\n",
       "                              source_specific_limits  \\\n",
       "0  {'time_limit': {'seconds': 2, 'nanos': 0}, 'me...   \n",
       "1  {'time_limit': {'seconds': 2, 'nanos': 0}, 'me...   \n",
       "2  {'time_limit': {'seconds': 1, 'nanos': 0}, 'me...   \n",
       "3  {'time_limit': {'seconds': 1, 'nanos': 0}, 'me...   \n",
       "4  {'time_limit': {'seconds': 2, 'nanos': 0}, 'me...   \n",
       "\n",
       "                        codeforces_specific_metadata  \\\n",
       "0  {'cf_contest_id': 339, 'cf_index': 'C', 'cf_po...   \n",
       "1  {'cf_contest_id': 1547, 'cf_index': 'E', 'cf_p...   \n",
       "2  {'cf_contest_id': 268, 'cf_index': 'C', 'cf_po...   \n",
       "3  {'cf_contest_id': 478, 'cf_index': 'C', 'cf_po...   \n",
       "4  {'cf_contest_id': 5, 'cf_index': 'C', 'cf_poin...   \n",
       "\n",
       "                                               tests  human_accuracy_rate  \\\n",
       "0  {'public_tests': [{'input': '0000000101\n",
       "3\n",
       "', '...             0.281633   \n",
       "1  {'public_tests': [{'input': '5\n",
       "\n",
       "6 2\n",
       "2 5\n",
       "14 16\n",
       "...             0.620833   \n",
       "2  {'public_tests': [{'input': '2 2\n",
       "', 'output': ...             0.525066   \n",
       "3  {'public_tests': [{'input': '1 1 1\n",
       "', 'output'...             0.562264   \n",
       "4  {'public_tests': [{'input': ')((())))(()())\n",
       "',...             0.395939   \n",
       "\n",
       "                                           dataclass  \\\n",
       "0  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "1  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "2  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "3  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "4  {'dataclass_code': 'import sys\n",
       "import time\n",
       "imp...   \n",
       "\n",
       "                                complexity_framework  \n",
       "0  {'time_complexity_fail_rate': 0.40579710144927...  \n",
       "1  {'time_complexity_fail_rate': 0.11409395973154...  \n",
       "2  {'time_complexity_fail_rate': 0.05527638190954...  \n",
       "3  {'time_complexity_fail_rate': 0.41610738255033...  \n",
       "4  {'time_complexity_fail_rate': 0.03846153846153...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_problem_and_human_solutions_list.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746728445837,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "2fa53c30",
    "outputId": "44760977-2764-4ea3-bd80-f2ae87756a80"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "      <th>time_curve_coefficient</th>\n",
       "      <th>space_curve_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_0</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.012433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_2</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_4</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  problem_id              problem_name solution_id time_complexity_inferred  \\\n",
       "0          0  339_C. Xenia and Weights         0_0                     O(1)   \n",
       "1          0  339_C. Xenia and Weights         0_1                     None   \n",
       "2          0  339_C. Xenia and Weights         0_2                     O(1)   \n",
       "3          0  339_C. Xenia and Weights         0_3                     None   \n",
       "4          0  339_C. Xenia and Weights         0_4                     O(1)   \n",
       "\n",
       "  space_complexity_inferred  time_curve_coefficient  space_curve_coefficient  \n",
       "0                   O(n**2)                0.000019                 0.012433  \n",
       "1                      None                     NaN                      NaN  \n",
       "2                      O(1)                0.000004               584.000000  \n",
       "3                      None                     NaN                      NaN  \n",
       "4                      O(1)                0.000013               323.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity_labels_light.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADwtm79WOsZL"
   },
   "source": [
    "The attribute 'correst_solution_list' of the first dataset contains a list of code solutions for each sample. These solutions are stored as dictionaries containing as keys 'solution_id' and 'solution_code'. The aim is to obtain two new columns (attributes) in the final dataset one containing the solution id, and the other containing one solution code per sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBMnCngnQPc6"
   },
   "source": [
    "The Datasets results too big to be managed with the resources at our disposal, so it was decided to reduce the size of the datasets reducing the number of solutions for each description. After many considerations, it was choosen 300000 as the maximum amount of total samples for the final dataset. The following code reduces to 100 the maximum number of solutions for each description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "executionInfo": {
     "elapsed": 712,
     "status": "ok",
     "timestamp": 1746728446551,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "OyMNA4YRueWr",
    "outputId": "99f13d0d-7669-4ea4-d553-400aa880d979"
   },
   "outputs": [],
   "source": [
    "# reduce num of possible correct solutions in correct_solution_list\n",
    "max = 100  # num of corresct solutions kept\n",
    "\n",
    "for problem in df_problem_and_human_solutions_list.correct_solution_list:\n",
    "    if len(problem) > max:\n",
    "        problem[:] = problem[:max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 707,
     "status": "ok",
     "timestamp": 1746728446553,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "nglRhQOR7HP7",
    "outputId": "90685974-cbbe-4b2b-b36e-c73559fbde2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 69\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 64\n",
      "Number of samples: 75\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 54\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 85\n",
      "Number of samples: 100\n",
      "Number of samples: 75\n",
      "Number of samples: 86\n",
      "Number of samples: 100\n",
      "Number of samples: 90\n",
      "Number of samples: 78\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 100\n",
      "Number of samples: 98\n",
      "Number of samples: 100\n",
      "Number of samples: 91\n"
     ]
    }
   ],
   "source": [
    "# check num of solutions:\n",
    "for problem in df_problem_and_human_solutions_list.correct_solution_list[:30]:\n",
    "    print('Number of samples:', len(problem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgJTWN44RB3i"
   },
   "source": [
    "Now we create a new DataFrame with columns 'solution_id' and 'solution_code' from the attribute 'correct_solution_list', and 'problem_description' from the atribute 'description' of the dataset df_problem_and_human_solutions_list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746728446564,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "gH6WnYt4-ZcW",
    "outputId": "7d967719-13d0-4adf-9381-9af2a6f2d055"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_65</td>\n",
       "      <td>p, m = [i for i, x in enumerate(input(), 1) if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_66</td>\n",
       "      <td>s=input()\\nA=[]\\nfor i in range(1,11):\\n    if...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_67</td>\n",
       "      <td>def go(i, d, last):\\n    if i&gt;0 and d&lt;=0 or d&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_68</td>\n",
       "      <td>from sys import setrecursionlimit\\nsetrecursio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>On a strip of land of length n there are k air...</td>\n",
       "      <td>1_0</td>\n",
       "      <td>def ii(): return int(input())\\ndef si(): retur...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>70 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  problem_description solution_id  \\\n",
       "0   Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1   Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2   Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3   Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4   Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "..                                                ...         ...   \n",
       "65  Xenia has a set of weights and pan scales. Eac...        0_65   \n",
       "66  Xenia has a set of weights and pan scales. Eac...        0_66   \n",
       "67  Xenia has a set of weights and pan scales. Eac...        0_67   \n",
       "68  Xenia has a set of weights and pan scales. Eac...        0_68   \n",
       "69  On a strip of land of length n there are k air...         1_0   \n",
       "\n",
       "                                        solution_code  \n",
       "0   __author__ = 'ratnesh.mishra'\\n\\nweights = map...  \n",
       "1   s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...  \n",
       "2   import sys\\nsys.setrecursionlimit (1000000)\\n\\...  \n",
       "3   from __future__ import division, print_functio...  \n",
       "4   # Target - Expert on CF\\n# Be Humblefool\\n\\nim...  \n",
       "..                                                ...  \n",
       "65  p, m = [i for i, x in enumerate(input(), 1) if...  \n",
       "66  s=input()\\nA=[]\\nfor i in range(1,11):\\n    if...  \n",
       "67  def go(i, d, last):\\n    if i>0 and d<=0 or d>...  \n",
       "68  from sys import setrecursionlimit\\nsetrecursio...  \n",
       "69  def ii(): return int(input())\\ndef si(): retur...  \n",
       "\n",
       "[70 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modify correct_solutions_list to have two different attributes (solution_id and solution_code) insted of a list of them\n",
    "solutions_list = df_problem_and_human_solutions_list.correct_solution_list.copy()\n",
    "descriptions = df_problem_and_human_solutions_list.description.copy()\n",
    "\n",
    "rows = []\n",
    "for desc, solutions in zip(descriptions, solutions_list):\n",
    "    for solution in solutions:\n",
    "        rows.append({\n",
    "            'problem_description': desc['text'],\n",
    "            'solution_id': solution['solution_id'],\n",
    "            'solution_code': solution['solution_code']\n",
    "        })\n",
    "new_matrix = pd.DataFrame(rows)\n",
    "\n",
    "new_matrix.head(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746728446568,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "pOQnh-1n8RVF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293586, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746728446575,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "p7lup8GP98nd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>293586</td>\n",
       "      <td>293586</td>\n",
       "      <td>293586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3031</td>\n",
       "      <td>293586</td>\n",
       "      <td>289745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hilbert's Hotel is a very unusual hotel since ...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>from sys import stdin, stdout\\ninput = stdin.b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>200</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      problem_description solution_id  \\\n",
       "count                                              293586      293586   \n",
       "unique                                               3031      293586   \n",
       "top     Hilbert's Hotel is a very unusual hotel since ...         0_0   \n",
       "freq                                                  200           1   \n",
       "\n",
       "                                            solution_code  \n",
       "count                                              293586  \n",
       "unique                                             289745  \n",
       "top     from sys import stdin, stdout\\ninput = stdin.b...  \n",
       "freq                                                    2  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRatAW5IZBSG"
   },
   "source": [
    "Now the Dataset new_matrix and the second dataset are merged to abtain one line for each solution and keep only the solutions present in the new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1746728454460,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "lVBcqT8kapAI",
    "outputId": "ff4e3617-2cca-4d45-faa8-a0d43672be8e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3  Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...  \n",
       "1  s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...  \n",
       "2  import sys\\nsys.setrecursionlimit (1000000)\\n\\...  \n",
       "3  from __future__ import division, print_functio...  \n",
       "4  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "      <th>time_curve_coefficient</th>\n",
       "      <th>space_curve_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_0</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.012433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_1</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_2</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_3</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>0_4</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  problem_id              problem_name solution_id time_complexity_inferred  \\\n",
       "0          0  339_C. Xenia and Weights         0_0                     O(1)   \n",
       "1          0  339_C. Xenia and Weights         0_1                     None   \n",
       "2          0  339_C. Xenia and Weights         0_2                     O(1)   \n",
       "3          0  339_C. Xenia and Weights         0_3                     None   \n",
       "4          0  339_C. Xenia and Weights         0_4                     O(1)   \n",
       "\n",
       "  space_complexity_inferred  time_curve_coefficient  space_curve_coefficient  \n",
       "0                   O(n**2)                0.000019                 0.012433  \n",
       "1                      None                     NaN                      NaN  \n",
       "2                      O(1)                0.000004               584.000000  \n",
       "3                      None                     NaN                      NaN  \n",
       "4                      O(1)                0.000013               323.000000  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_complexity_labels_light.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 771,
     "status": "ok",
     "timestamp": 1746728455521,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "OjxHgVN-ZbRc"
   },
   "outputs": [],
   "source": [
    "df = new_matrix.merge(df_complexity_labels_light, on='solution_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1746728455901,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "McbzBbQKZ2o8",
    "outputId": "d86e6ad6-2344-45dd-a666-c098bb22227c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_id</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "      <th>time_curve_coefficient</th>\n",
       "      <th>space_curve_coefficient</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.012433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "      <td>0</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>323.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3  Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code problem_id  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...          0   \n",
       "1  s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...          0   \n",
       "2  import sys\\nsys.setrecursionlimit (1000000)\\n\\...          0   \n",
       "3  from __future__ import division, print_functio...          0   \n",
       "4  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...          0   \n",
       "\n",
       "               problem_name time_complexity_inferred  \\\n",
       "0  339_C. Xenia and Weights                     O(1)   \n",
       "1  339_C. Xenia and Weights                     None   \n",
       "2  339_C. Xenia and Weights                     O(1)   \n",
       "3  339_C. Xenia and Weights                     None   \n",
       "4  339_C. Xenia and Weights                     O(1)   \n",
       "\n",
       "  space_complexity_inferred  time_curve_coefficient  space_curve_coefficient  \n",
       "0                   O(n**2)                0.000019                 0.012433  \n",
       "1                      None                     NaN                      NaN  \n",
       "2                      O(1)                0.000004               584.000000  \n",
       "3                      None                     NaN                      NaN  \n",
       "4                      O(1)                0.000013               323.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746728455903,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "7336xkAPbXNU",
    "outputId": "35118156-2cec-4858-8995-6d0242ecf079"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(293586, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRk8ieRhcRM0"
   },
   "source": [
    "Remove from df the atributes that we aren't going to use during the modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746728455904,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "l9qLS7aucP2D"
   },
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['problem_id', 'time_curve_coefficient', 'space_curve_coefficient'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 400
    },
    "executionInfo": {
     "elapsed": 504,
     "status": "ok",
     "timestamp": 1746728456411,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "JYK5acYyc5Gd",
    "outputId": "846d97ab-b5a3-4d33-9bf2-67381082254b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "3  Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "4  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
       "1  s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...   \n",
       "2  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
       "3  from __future__ import division, print_functio...   \n",
       "4  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...   \n",
       "\n",
       "               problem_name time_complexity_inferred space_complexity_inferred  \n",
       "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
       "1  339_C. Xenia and Weights                     None                      None  \n",
       "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
       "3  339_C. Xenia and Weights                     None                      None  \n",
       "4  339_C. Xenia and Weights                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9RmgbYXZbcaU"
   },
   "source": [
    "From the preliminary analysis we know that only the second dataset has na values. The na values are now eliminated form df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 965
    },
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1746728456958,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "hpmLQVcFde7X",
    "outputId": "e74ffb16-4539-441d-efa8-01dc91780154"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_1</td>\n",
       "      <td>s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_3</td>\n",
       "      <td>from __future__ import division, print_functio...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_5</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit(100000)\\n\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_7</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit(1500)\\ndef d...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_9</td>\n",
       "      <td>s=input()\\nm=int(input())\\na=[]\\nfor j in rang...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293369</th>\n",
       "      <td>You are given an integer L. Construct a direct...</td>\n",
       "      <td>3102_83</td>\n",
       "      <td>import sys\\ninput = sys.stdin.readline\\n\\nL = ...</td>\n",
       "      <td>p03267 AtCoder Beginner Contest 108 - All Your...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293422</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_36</td>\n",
       "      <td>from itertools import accumulate\\nn, *A = map(...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293431</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_45</td>\n",
       "      <td>n, *A = map(int, open(0).read().split())\\ndef ...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293446</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_60</td>\n",
       "      <td>n = int(input())\\na = list(map(int,input().spl...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293453</th>\n",
       "      <td>You are given an integer sequence of length N....</td>\n",
       "      <td>3103_67</td>\n",
       "      <td>_,*l=map(int,open(0).read().split())\\ndef f(s)...</td>\n",
       "      <td>p03739 AtCoder Beginner Contest 059 - Sequence</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48709 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      problem_description solution_id  \\\n",
       "1       Xenia has a set of weights and pan scales. Eac...         0_1   \n",
       "3       Xenia has a set of weights and pan scales. Eac...         0_3   \n",
       "5       Xenia has a set of weights and pan scales. Eac...         0_5   \n",
       "7       Xenia has a set of weights and pan scales. Eac...         0_7   \n",
       "9       Xenia has a set of weights and pan scales. Eac...         0_9   \n",
       "...                                                   ...         ...   \n",
       "293369  You are given an integer L. Construct a direct...     3102_83   \n",
       "293422  You are given an integer sequence of length N....     3103_36   \n",
       "293431  You are given an integer sequence of length N....     3103_45   \n",
       "293446  You are given an integer sequence of length N....     3103_60   \n",
       "293453  You are given an integer sequence of length N....     3103_67   \n",
       "\n",
       "                                            solution_code  \\\n",
       "1       s = input()\\nm = int(input())\\nk1 = k2 = 0\\nz ...   \n",
       "3       from __future__ import division, print_functio...   \n",
       "5       import sys\\nsys.setrecursionlimit(100000)\\n\\n\\...   \n",
       "7       import sys\\nsys.setrecursionlimit(1500)\\ndef d...   \n",
       "9       s=input()\\nm=int(input())\\na=[]\\nfor j in rang...   \n",
       "...                                                   ...   \n",
       "293369  import sys\\ninput = sys.stdin.readline\\n\\nL = ...   \n",
       "293422  from itertools import accumulate\\nn, *A = map(...   \n",
       "293431  n, *A = map(int, open(0).read().split())\\ndef ...   \n",
       "293446  n = int(input())\\na = list(map(int,input().spl...   \n",
       "293453  _,*l=map(int,open(0).read().split())\\ndef f(s)...   \n",
       "\n",
       "                                             problem_name  \\\n",
       "1                                339_C. Xenia and Weights   \n",
       "3                                339_C. Xenia and Weights   \n",
       "5                                339_C. Xenia and Weights   \n",
       "7                                339_C. Xenia and Weights   \n",
       "9                                339_C. Xenia and Weights   \n",
       "...                                                   ...   \n",
       "293369  p03267 AtCoder Beginner Contest 108 - All Your...   \n",
       "293422     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "293431     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "293446     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "293453     p03739 AtCoder Beginner Contest 059 - Sequence   \n",
       "\n",
       "       time_complexity_inferred space_complexity_inferred  \n",
       "1                          None                      None  \n",
       "3                          None                      None  \n",
       "5                          None                      None  \n",
       "7                          None                      None  \n",
       "9                          None                      None  \n",
       "...                         ...                       ...  \n",
       "293369                     None                      None  \n",
       "293422                     None                      None  \n",
       "293431                     None                      None  \n",
       "293446                     None                      None  \n",
       "293453                     None                      None  \n",
       "\n",
       "[48709 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how many values of df are nan\n",
    "df[df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 366,
     "status": "ok",
     "timestamp": 1746728456975,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "-uzFuKf5bqxx"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 342,
     "status": "ok",
     "timestamp": 1746728457331,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "I16SN3XDb2Y7",
    "outputId": "d0677b4c-bdc5-4ef9-e05e-fd505266acb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244877, 6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# size of the temp_df after removal of na values\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPFydkG2hzRD"
   },
   "source": [
    "Rename the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1746728457351,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "Pht6z9RUh7q8"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746728457358,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "yHAsW8P4iGCO",
    "outputId": "19c2a4c8-6767-49b6-84ba-a07fbab2184f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td># Target - Expert on CF\\n# Be Humblefool\\n\\nim...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
       "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
       "2  # Target - Expert on CF\\n# Be Humblefool\\n\\nim...   \n",
       "\n",
       "               problem_name time_complexity_inferred space_complexity_inferred  \n",
       "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
       "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
       "2  339_C. Xenia and Weights                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qf1sm6uFsjNE"
   },
   "source": [
    "### Clean the code solutions\n",
    "In the code of the solutions are present many elements that would render the training of models harder, such as comments (both inline and multiple lines)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_CvdyoEw-dNO"
   },
   "source": [
    "Apon inspection of the code solutions, one code solution resulted not correct sintatically, so it was decided to drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746728457370,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "qbXoaeiG-qCN",
    "outputId": "1de61e61-1bb5-4c3d-db3d-a96753c86f8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A,B = map(int,input().split(\" \"))\\n\\ns1 = \"####################################################################################################\\\\n\\\\\\n.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#\\\\n\"\\ns2 = \"....................................................................................................\\\\n\\\\\\n#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.#.\\\\n\"\\nblack = \\'#\\' * 100 + \\'\\\\n\\'\\nwhite = \\'.\\' * 100 + \\'\\\\n\\'\\n\\ncnt = (A-1) // 50\\nmer = (A - 1) % 50\\nif mer == 0:\\n\\tup = s1 * cnt + black\\nelse:\\n\\tup = s1 * cnt + black + (\\'.#\\' * mer) + (\\'#\\' * (100 - 2*mer)) + \\'\\\\n\\' + black\\n\\ncnt = (B-1) // 50\\nmer = (B - 1) % 50\\n\\nif mer == 0:\\n\\tdown = s2 * cnt + white\\nelse:\\n\\tdown = s2 * cnt + white + (\\'#.\\' * mer) + (\\'.\\' * (100 - 2*mer)) + \\'\\\\n\\' + white\\n\\nprint(str(len((up+down)) // 100) + \" 100\")\\nprint(up + down)\\n\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.solution_code[114411]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 526,
     "status": "ok",
     "timestamp": 1746728457898,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "0EL4n2sy-woN"
   },
   "outputs": [],
   "source": [
    "df = df.drop(df.index[114411], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1746728457910,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "JBG_3KJw_2Kd",
    "outputId": "5787ecff-a13c-4560-b6b2-4d6a420ee781"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114409</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_0</td>\n",
       "      <td>\\nimport sys\\nsys.setrecursionlimit(1 &lt;&lt; 25)\\n...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114410</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_1</td>\n",
       "      <td>from math import ceil\\n\\nA, B = map(int, input...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(n+m)</td>\n",
       "      <td>O(n+m)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114412</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_3</td>\n",
       "      <td>a, b = map(int, input().split())\\na -= 1\\nb -=...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(nlogn)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114413</th>\n",
       "      <td>You are given two integers A and B.\\n\\nPrint a...</td>\n",
       "      <td>1447_4</td>\n",
       "      <td>ab = list(map(int, input().split()))\\n\\ng = [[...</td>\n",
       "      <td>p03404 AtCoder Regular Contest 093 - Grid Comp...</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      problem_description solution_id  \\\n",
       "114409  You are given two integers A and B.\\n\\nPrint a...      1447_0   \n",
       "114410  You are given two integers A and B.\\n\\nPrint a...      1447_1   \n",
       "114412  You are given two integers A and B.\\n\\nPrint a...      1447_3   \n",
       "114413  You are given two integers A and B.\\n\\nPrint a...      1447_4   \n",
       "\n",
       "                                            solution_code  \\\n",
       "114409  \\nimport sys\\nsys.setrecursionlimit(1 << 25)\\n...   \n",
       "114410  from math import ceil\\n\\nA, B = map(int, input...   \n",
       "114412  a, b = map(int, input().split())\\na -= 1\\nb -=...   \n",
       "114413  ab = list(map(int, input().split()))\\n\\ng = [[...   \n",
       "\n",
       "                                             problem_name  \\\n",
       "114409  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "114410  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "114412  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "114413  p03404 AtCoder Regular Contest 093 - Grid Comp...   \n",
       "\n",
       "       time_complexity_inferred space_complexity_inferred  \n",
       "114409                     O(1)                      O(n)  \n",
       "114410                   O(n+m)                    O(n+m)  \n",
       "114412                 O(nlogn)                   O(n**2)  \n",
       "114413                     O(1)                      O(1)  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[114409:114413]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1746728457916,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "EDbKGIGh_FqQ"
   },
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1746728458627,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "KbGimJKf_Oa9",
    "outputId": "133b239f-3a26-403c-b18b-4e5079a41e4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(244876, 6)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1AyCnxNtLhe"
   },
   "source": [
    "Visualize inline comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373,
     "status": "ok",
     "timestamp": 1746728461656,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "sBRj8TvBs_il",
    "outputId": "ae81e075-b41a-4389-84f2-8308a9713a14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'\\n#\\n': 2906, ' #!/usr/bin/env python3\\n': 1645, ' # -*- coding: utf-8 -*-\\n': 1553, '\\n# region fastio\\n': 1209, ' #\\n': 1012, ' #!/usr/bin/env python\\n': 693, '\\n# endregion\\n': 683, '\\n#     else:\\n': 545, ' # cook your dish here\\n': 540, '\\n# ------------------------------\\n': 436, ...})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import FreqDist\n",
    "\n",
    "inline_comment_regex = '([^\\\"|^\\']\\#[^\\'|^\\\"|^\\n]*\\n{1})'\n",
    "concatenated_code = ' '.join(df.solution_code)\n",
    "inline_comments_in_code = re.findall(inline_comment_regex, concatenated_code)\n",
    "counts = nltk.FreqDist(inline_comments_in_code)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11647,
     "status": "ok",
     "timestamp": 1746728473304,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "5y_9rYhJv-1C",
    "outputId": "7ed47d4f-8512-4ce4-cfb2-01aab46ce857"
   },
   "outputs": [],
   "source": [
    "for solution in df.solution_code:\n",
    "    comment_code = re.findall(inline_comment_regex, solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1746728473311,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "XoQO6ylU3TiN",
    "outputId": "c7c3c976-7248-49ad-9879-31bc3193e3c5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n = int(input())\\ncont = 0\\nmatriz = []\\nfor i in range(n):\\n    linha = list(input())\\n    matriz.append(linha)\\n    for i in range(n):\\n        if linha[i] == \\'#\\':\\n            cont+=1\\nif cont%5 != 0:\\n    print(\"NO\")\\nelse:\\n    for i in range(0,n-1):\\n        for j in range(1,n):\\n            if matriz[i][j] == \\'#\\' and i < n-2 and j < n-1: # topo da cruz\\n                if matriz[i+1][j] != \\'#\\' or matriz[i+2][j] != \\'#\\' or matriz[i+1][j-1] != \\'#\\' or matriz[i+1][j+1]!= \\'#\\':\\n                    continue\\n    \\n                else:\\n                    matriz[i][j] = \\'.\\'\\n                    matriz[i+1][j] = \\'.\\'\\n                    matriz[i+2][j] = \\'.\\'\\n                    matriz[i+1][j-1] = \\'.\\'\\n                    matriz[i+1][j+1] = \\'.\\'\\n                    cont -= 5\\n    if cont == 0:\\n        print(\"YES\")\\n    else:\\n        print(\"NO\")\\n            \\n \\t   \\t\\t\\t  \\t\\t \\t \\t \\t \\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print an example of code solution that contains inline comments\n",
    "df.solution_code[65669]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtQIGhQbvJRb"
   },
   "source": [
    "Remove inline comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1746728473674,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "qnBXi_KKvMiO"
   },
   "outputs": [],
   "source": [
    "for solution in df.solution_code:\n",
    "    solution = re.sub(inline_comment_regex, '\\n', solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1746728473682,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "3s3zMH5svjaT",
    "outputId": "3289d7ab-54b4-45ce-ca00-3609c77479ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n = int(input())\\ncont = 0\\nmatriz = []\\nfor i in range(n):\\n    linha = list(input())\\n    matriz.append(linha)\\n    for i in range(n):\\n        if linha[i] == \\'#\\':\\n            cont+=1\\nif cont%5 != 0:\\n    print(\"NO\")\\nelse:\\n    for i in range(0,n-1):\\n        for j in range(1,n):\\n            if matriz[i][j] == \\'#\\' and i < n-2 and j < n-1: # topo da cruz\\n                if matriz[i+1][j] != \\'#\\' or matriz[i+2][j] != \\'#\\' or matriz[i+1][j-1] != \\'#\\' or matriz[i+1][j+1]!= \\'#\\':\\n                    continue\\n    \\n                else:\\n                    matriz[i][j] = \\'.\\'\\n                    matriz[i+1][j] = \\'.\\'\\n                    matriz[i+2][j] = \\'.\\'\\n                    matriz[i+1][j-1] = \\'.\\'\\n                    matriz[i+1][j+1] = \\'.\\'\\n                    cont -= 5\\n    if cont == 0:\\n        print(\"YES\")\\n    else:\\n        print(\"NO\")\\n            \\n \\t   \\t\\t\\t  \\t\\t \\t \\t \\t \\t\\t\\t\\t\\t\\t\\t'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the solution code now without inline comments\n",
    "df.solution_code[65669]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yoew1xSyDPmc"
   },
   "source": [
    "Consider now multi-line comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 687,
     "status": "ok",
     "timestamp": 1746728474371,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "286JCDc5DkTZ",
    "outputId": "85003e66-6ee8-4ef1-e405-b9f514a2bd58"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_comment_regex = '([\\'\\\"])\\1\\1[\\d\\D]*?\\1{3}'\n",
    "concatenated_code = ' '.join(df.solution_code)\n",
    "multi_comments_in_code = re.findall(multi_comment_regex, concatenated_code)\n",
    "counts = nltk.FreqDist(multi_comments_in_code)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1746728474396,
     "user": {
      "displayName": "Sofia Perini",
      "userId": "07138673496505551525"
     },
     "user_tz": -120
    },
    "id": "sUOOJeqkDkTc"
   },
   "outputs": [],
   "source": [
    "for solution in df.solution_code:\n",
    "    comment_code = re.findall(multi_comment_regex, solution)\n",
    "    if comment_code:\n",
    "        print(df.loc[df['solution_code'] == solution].index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8AcTklgyF9Eg"
   },
   "source": [
    "There are no multiline comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6te2LxHiGgLW"
   },
   "source": [
    "### Save final Dataset in storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saving_path = path_to_project + '/final_ds.csv' if IN_COLAB else './final_ds.csv'\n",
    "df.to_csv(saving_path, sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers for Text Generation, Training and Fine-Tuning\n",
    "In this section we are going to train and evaluate Transformers models to generate text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Goodle Drive (if needed), add testing variables to the environment, and upload the dataset from storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_project + '/final_ds.csv' if IN_COLAB else './final_ds.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-c0cfe19d-52f1-429d-a535-4edaf277c72d\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>problem_description</th>\n",
       "      <th>solution_id</th>\n",
       "      <th>solution_code</th>\n",
       "      <th>problem_name</th>\n",
       "      <th>time_complexity_inferred</th>\n",
       "      <th>space_complexity_inferred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_0</td>\n",
       "      <td>__author__ = 'ratnesh.mishra'\\n\\nweights = map...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(n**2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_2</td>\n",
       "      <td>import sys\\nsys.setrecursionlimit (1000000)\\n\\...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Xenia has a set of weights and pan scales. Eac...</td>\n",
       "      <td>0_4</td>\n",
       "      <td>import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...</td>\n",
       "      <td>339_C. Xenia and Weights</td>\n",
       "      <td>O(1)</td>\n",
       "      <td>O(1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0cfe19d-52f1-429d-a535-4edaf277c72d')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-c0cfe19d-52f1-429d-a535-4edaf277c72d button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-c0cfe19d-52f1-429d-a535-4edaf277c72d');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-afe982b2-bb0b-45f9-a6b5-03823a606083\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-afe982b2-bb0b-45f9-a6b5-03823a606083')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-afe982b2-bb0b-45f9-a6b5-03823a606083 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                 problem_description solution_id  \\\n",
       "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
       "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
       "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
       "\n",
       "                                       solution_code  \\\n",
       "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
       "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
       "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
       "\n",
       "               problem_name time_complexity_inferred space_complexity_inferred  \n",
       "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
       "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
       "2  339_C. Xenia and Weights                     O(1)                      O(1)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train T5-Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The developers of the Text-To-Text Transfer Transformer (T5) write:\n",
    "\n",
    "'With T5, we propose reframing all NLP tasks into a unified text-to-text-format where the input and output are always text strings, in contrast to BERT-style models that can only output either a class label or a span of the input. Our text-to-text framework allows us to use the same model, loss function, and hyperparameters on any NLP task.'\n",
    "\n",
    "The model has 223M parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries that are going to be used in the following subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Model T5-Base only 8000 samples are kept from the dataset due to the limitation of the resources availabe to us. We decided to keep the training (and fine-tuning) time of the models at approximately 2 hours for this section, and the amount of samples allowed us to maintain this contraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## only in initial part: reduce ds to allow faster testing of the code\n",
    "df = df.head(8000).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data into training dataset, validation dataset and test dataset. We decided to use 10% of the dataset for tesing, and the remaining for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1)\n",
    "train, val = train_test_split(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train instances:  5760\n",
      "# test instances:   800\n",
      "# val instances:    1440\n"
     ]
    }
   ],
   "source": [
    "print('# train instances: ', train.shape[0])\n",
    "print('# test instances:  ', test.shape[0])\n",
    "print('# val instances:   ', val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google-t5/t5-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37f3852a115a496bbb4cfddba48272e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76da375e3df04483a63e1cbe95fa551f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6658b732794510bcd7fbdc762adc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print some useful information on the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  32000\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('â–big', 600),\n",
       " ('â–God', 601),\n",
       " ('â–dass', 602),\n",
       " ('im', 603),\n",
       " ('â–30', 604),\n",
       " ('â–event', 605),\n",
       " ('â–development', 606),\n",
       " ('â–form', 607),\n",
       " ('â–read', 608),\n",
       " ('â–hand', 609)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer.get_vocab().items())[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['â–You', 'â–have', 'â–an', 'â–array', 'â–in', 'â–input', ',', 'â–order', 'â–the', 'â–elements', 'â–in', 'â–it', 'â–in', 'â–O', '(', 'n', ')', 'â–time', 'â–complexity', '.', 'â–Add', 'â–', 'a', 'â–wrong', 'â–word', 'd']\n"
     ]
    }
   ],
   "source": [
    "text = \"You have an array in input, order the elements in it in O(n) time complexity. Add a wrong wordd\"\n",
    "encoded_input = tokenizer._tokenize(text)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[148, 43, 46, 5590, 16, 3785, 6, 455, 8, 2479, 16, 34, 16, 411, 599, 29, 61, 97, 11641, 5, 2334, 3, 9, 1786, 1448, 26, 1]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = tokenizer(text)['input_ids']\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is uploaded and connected to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d287ceaf052947a58fff893483a947f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ad9095acdb14d4cb6d5414d658447db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
    "t5.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32100, 768)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 768)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 12)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-11): 11 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
      "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
      "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222882048"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_params = sum(param.numel() for param in t5.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: shared.weight\n",
      "Parameter shape: torch.Size([32100, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter shape: torch.Size([32, 12])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.8.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.9.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.10.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.1.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.1.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.block.11.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: encoder.final_layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "Parameter shape: torch.Size([32, 12])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.0.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.1.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.2.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.3.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.4.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.5.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.6.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.7.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.8.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.9.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.10.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.SelfAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.0.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.q.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.k.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.v.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.EncDecAttention.o.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.1.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.2.DenseReluDense.wi.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.2.DenseReluDense.wo.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.block.11.layer.2.layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: decoder.final_layer_norm.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in t5.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.size()}\")\n",
    "    print(f\"Is trainable: {param.requires_grad}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both Tokenizer and Model we can tokenize the dataset and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a single string for each conversation of problem description and solution, where the user input and the system response are separated by eos tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eos_token(idx, df, eos_token):\n",
    "    # build the user input including the problem description, the time complexity and the space complexity\n",
    "    chat_string = 'User:' + df.loc[idx, 'problem_description'] + ' Time complexity: ' + df.loc[idx, 'time_complexity_inferred'] + '; Space complexity: ' + df.loc[idx, 'space_complexity_inferred']\n",
    "    # now add the eos token and the response from the assistant, the code solution for the problem\n",
    "    chat_string = chat_string + eos_token + 'Assistant: ' + df.loc[idx, 'solution_code'] + eos_token\n",
    "    return chat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = [apply_eos_token(idx, train, tokenizer.eos_token) for idx in train.index]\n",
    "test_str = [apply_eos_token(idx, test, tokenizer.eos_token) for idx in test.index]\n",
    "val_str = [apply_eos_token(idx, val, tokenizer.eos_token) for idx in val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'User:It is known that the area of a regular dodecagon inscribed in a circle of radius a is 3a^2.\\n\\nGiven an integer r, find the area of a regular dodecagon inscribed in a circle of radius r.\\n\\nConstraints\\n\\n* 1 \\\\leq r \\\\leq 100\\n* r is an integer.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nr\\n\\n\\nOutput\\n\\nPrint an integer representing the area of the regular dodecagon.\\n\\nExamples\\n\\nInput\\n\\n4\\n\\n\\nOutput\\n\\n48\\n\\n\\nInput\\n\\n15\\n\\n\\nOutput\\n\\n675\\n\\n\\nInput\\n\\n80\\n\\n\\nOutput\\n\\n19200 Time complexity: O(1); Space complexity: O(1)</s>Assistant: N = int(input())\\n\\nprint(3*(N**2))</s>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings where the eos token was applied, are now put in a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'chat': train_str})\n",
    "test_data = Dataset.from_dict({'chat': test_str})\n",
    "val_data = Dataset.from_dict({'chat': val_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['val'] = val_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d6361bf11624c1c9932038818e8297c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/t5/tokenization_t5.py:289: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8809b40f9154d6a87ca645c1c6b004a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e8dc975dcd495f88121e62c44fed39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer(examples[\"chat\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids,\n",
    "        'attention_mask': input_encodings.attention_mask,\n",
    "        'labels': input_encodings.input_ids.copy()\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sequences in same batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"t5_trainer\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=1e-4,       # t5 needs a higher lr than other models\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,\n",
    "    report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= t5,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_data['train'],\n",
    "    eval_dataset= tokenized_data['val'],\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time taken by the model to finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1080' max='1080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1080/1080 2:08:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.004200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1080, training_loss=0.07308270597347506, metrics={'train_runtime': 7709.3416, 'train_samples_per_second': 2.241, 'train_steps_per_second': 0.14, 'total_flos': 1.05227923488768e+16, 'train_loss': 0.07308270597347506, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  7709.833685159683\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Training time: \", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The T5 model has 257 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "shared.weight                                           (32100, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight            (768, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight            (768, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight            (768, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight            (768, 768)\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight     (32, 12)\n",
      "encoder.block.0.layer.0.layer_norm.weight                     (768,)\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight         (3072, 768)\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight         (768, 3072)\n",
      "encoder.block.0.layer.1.layer_norm.weight                     (768,)\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight            (768, 768)\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight            (768, 768)\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight            (768, 768)\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight            (768, 768)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "decoder.block.11.layer.2.layer_norm.weight                    (768,)\n",
      "decoder.final_layer_norm.weight                               (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(t5.named_parameters())\n",
    "\n",
    "print('The T5 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 768)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 768)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 12)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-11): 11 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
       "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/t5_train_2025_05_18_12_35_46'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "t5_training_path = path_to_project + '/Transformer-trained-models/' + f\"t5_train_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer.save_pretrained(t5_training_path)\n",
    "t5.save_pretrained(t5_training_path)\n",
    "print(f\"Checkpoint saved at: \\'{t5_training_path}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "#t5_training_path = '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/t5_train_2025_05_18_12_35_46'\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_training_path)\n",
    "t5 = T5ForConditionalGeneration.from_pretrained(t5_training_path, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we first extract one conversation from the test data randomly, give it as input to the model, and we see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "User: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research.\n",
      "\n",
      "First of all, Vlad assigned individual indices to all cafes. Then, he wrote down indices of cafes he visited in a row, in order of visiting them. Now, Vlad wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that he hasn't been there for as long as possible. Help Vlad to find that cafe.\n",
      "\n",
      "Input\n",
      "\n",
      "In first line there is one integer n (1 â‰¤ n â‰¤ 2Â·105) â€” number of cafes indices written by Vlad.\n",
      "\n",
      "In second line, n numbers a1, a2, ..., an (0 â‰¤ ai â‰¤ 2Â·105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted.\n",
      "\n",
      "Output\n",
      "\n",
      "Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "5\n",
      "1 3 2 1 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "6\n",
      "2 1 2 2 4 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "2\n",
      "\n",
      "Note\n",
      "\n",
      "In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. \n",
      "\n",
      "In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some numbers while numerating the cafes. Time complexity: O(n); Space complexity: O(n)</s>Assistant: from collections import defaultdict\n",
      "n = int(input())\n",
      "a = list(map(int,input().split()))\n",
      "\n",
      "c = defaultdict(lambda: 0)\n",
      "\n",
      "for i in range(len(a)):\n",
      "    c[a[i]] = i\n",
      "\n",
      "maxi = 0\n",
      "k = len(a)\n",
      "ne = 0\n",
      "for key in c:\n",
      "    if(k - c[key] > maxi):\n",
      "        ne = key\n",
      "    maxi = max(maxi, k - c[key])\n",
      "\n",
      "print(ne)</s>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      " User: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research.\n",
      "\n",
      "First of all, Vlad assigned individual indices to all cafes. Then, he wrote down indices of cafes he visited in a row, in order of visiting them. Now, Vlad wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that he hasn't been there for as long as possible. Help Vlad to find that cafe.\n",
      "\n",
      "Input\n",
      "\n",
      "In first line there is one integer n (1 â‰¤ n â‰¤ 2Â·105) â€” number of cafes indices written by Vlad.\n",
      "\n",
      "In second line, n numbers a1, a2, ..., an (0 â‰¤ ai â‰¤ 2Â·105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted.\n",
      "\n",
      "Output\n",
      "\n",
      "Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "5\n",
      "1 3 2 1 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "6\n",
      "2 1 2 2 4 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "2\n",
      "\n",
      "Note\n",
      "\n",
      "In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. \n",
      "\n",
      "In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some numbers while numerating the cafes. Time complexity: O(n); Space complexity: O(n)</s> Assistant: \n",
      "User input lenght:  1601\n",
      "####\n",
      "Correct solution output: \n",
      " Assistant: from collections import defaultdict\n",
      "n = int(input())\n",
      "a = list(map(int,input().split()))\n",
      "\n",
      "c = defaultdict(lambda: 0)\n",
      "\n",
      "for i in range(len(a)):\n",
      "    c[a[i]] = i\n",
      "\n",
      "maxi = 0\n",
      "k = len(a)\n",
      "ne = 0\n",
      "for key in c:\n",
      "    if(k - c[key] > maxi):\n",
      "        ne = key\n",
      "    maxi = max(maxi, k - c[key])\n",
      "\n",
      "print(ne)</s>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, en = dialogue.split('</s>')\n",
    "\n",
    "# add eos token at the end of test_input and test_output\n",
    "test_input = test_input + tokenizer.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer.eos_token\n",
    "\n",
    "print('User input: \\n', test_input)\n",
    "print('User input lenght: ', len(test_input))\n",
    "print('####')\n",
    "print('Correct solution output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> User: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research. First of all, Vlad assigned individual indices to all cafes. Then, he wrote down indices of cafes he visited in a row, in order of visiting them. Now, Vlad wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that he hasn't been there for as long as possible. Help Vlad to find that cafe. Input In first line there is one integer n (1 <unk> n <unk> 2<unk> 105) â€” number of cafes indices written by Vlad. In second line, n numbers a1, a2, ..., an (0 <unk> ai <unk> 2<unk> 105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted. Output Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible. Examples Input 5 1 3 2 1 2 Output 3 Input 6 2 1 2 2 4 1 Output 2 Note In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some numbers while numerating the cafes. Time complexity: O(n); Space complexity: O(n) Assistant: Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research. First of all, Vlad assigned individual indices to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. Input In first line there is one integer n (1 <unk> n <unk> 2<unk> 105) â€” number of cafes indices written by Vlad. In second line, n numbers a1, a2, ..., an (0 <unk> ai <unk> 2<unk>105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted. Output Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible. Examples Input 5 1 3 2 1 2 Output 3 Input 6 2 1 2 2 4 1 Output 2 Note In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some\n",
      " Vlad likes to eat in cafes very much. During his life, he has visited cafes n times. Unfortunately, Vlad started to feel that his last visits are not any different from each other. To fix that Vlad had a small research. First of all, Vlad assigned individual indices to that cafe was before his last visits to every other cafe. In other words, he wants to find such a cafe that his last visit to that cafe was before his last visits to every other cafe. Input In first line there is one integer n (1 <unk> n <unk> 2<unk> 105) â€” number of cafes indices written by Vlad. In second line, n numbers a1, a2, ..., an (0 <unk> ai <unk> 2<unk>105) are written â€” indices of cafes in order of being visited by Vlad. Vlad could visit some cafes more than once. Note that in numeration, some indices could be omitted. Output Print one integer â€” index of the cafe that Vlad hasn't visited for as long as possible. Examples Input 5 1 3 2 1 2 Output 3 Input 6 2 1 2 2 4 1 Output 2 Note In first test, there are three cafes, and the last visits to cafes with indices 1 and 2 were after the last visit to cafe with index 3; so this cafe is the answer. In second test case, there are also three cafes, but with indices 1, 2 and 4. Cafes with indices 1 and 4 were visited after the last visit of cafe with index 2, so the answer is 2. Note that Vlad could omit some\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(test_input, return_tensors=\"pt\", max_length = 2000).to(device)\n",
    "output = t5.generate(**input_ids, max_new_tokens=800)\n",
    "gen_text = tokenizer.decode(output[0])\n",
    "\n",
    "print(gen_text)\n",
    "print(gen_text.split('Assistant:')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model does not perform very well and struggles to produce code in output and to maintain the conversation format given to it. What it tries to do is to describe the solution process without explicitly generating code. \n",
    "\n",
    "We expected this type of reults, the T5 model has very good performance on translation and sumarization, while it wasn't trained as a chatbot to produce code initially. Moreover, due to our limited resources, 8000 samples are too little to train such a big model (223M parameters) in an efective way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to consider the metrics Perplexity, BLEU and F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select only the first three samples of the test set due to limitations in the RAM available to us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('</s>')[0] + tokenizer.eos_token + 'Assistant: ' for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('</s>')[1] + tokenizer.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lmppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py:898: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/configuration_auto.py:1099: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:4056: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23387.28944204467, 1260.3922689555861, 22569.483405842497]\n",
      "average perplexity: 15739.055038947583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import lmppl\n",
    "\n",
    "scorer = lmppl.EncoderDecoderLM(t5_training_path)\n",
    "\n",
    "ppl = scorer.get_perplexity(input_texts= test_input, output_texts=test_output)\n",
    "print(list( ppl))\n",
    "print(f\"average perplexity: {sum(ppl)/len(ppl)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user input: \n",
      "User: Gleb ordered pizza home. When the courier delivered the pizza, he was very upset, because several pieces of sausage lay on the crust, and he does not really like the crust.\n",
      "\n",
      "The pizza is a circle of radius r and center at the origin. Pizza consists of the main part â€” circle of radius r - d with center at the origin, and crust around the main part of the width d. Pieces of sausage are also circles. The radius of the i -th piece of the sausage is ri, and the center is given as a pair (xi, yi).\n",
      "\n",
      "Gleb asks you to help determine the number of pieces of sausage caught on the crust. A piece of sausage got on the crust, if it completely lies on the crust.\n",
      "\n",
      "Input\n",
      "\n",
      "First string contains two integer numbers r and d (0 â‰¤ d < r â‰¤ 500) â€” the radius of pizza and the width of crust.\n",
      "\n",
      "Next line contains one integer number n â€” the number of pieces of sausage (1 â‰¤ n â‰¤ 105).\n",
      "\n",
      "Each of next n lines contains three integer numbers xi, yi and ri ( - 500 â‰¤ xi, yi â‰¤ 500, 0 â‰¤ ri â‰¤ 500), where xi and yi are coordinates of the center of i-th peace of sausage, ri â€” radius of i-th peace of sausage.\n",
      "\n",
      "Output\n",
      "\n",
      "Output the number of pieces of sausage that lay on the crust.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "8 4\n",
      "7\n",
      "7 8 1\n",
      "-7 3 2\n",
      "0 2 1\n",
      "0 -2 2\n",
      "-3 -3 1\n",
      "0 6 2\n",
      "5 3 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "2\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "10 8\n",
      "4\n",
      "0 0 9\n",
      "0 0 10\n",
      "1 0 1\n",
      "1 0 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "0\n",
      "\n",
      "Note\n",
      "\n",
      "Below is a picture explaining the first example. Circles of green color denote pieces of sausage lying on the crust.\n",
      "\n",
      "<image> Time complexity: O(n**2); Space complexity: O(1)</s>Assistant: \n",
      "#################\n",
      "prediction: \n",
      "Assistant: import math\n",
      "from sys import stdin, stdout\n",
      "def main():\n",
      "    x = stdin.readline().split()\n",
      "    r = int(x[0])\n",
      "    d = int(x[1])\n",
      "    r -= d\n",
      "    n = int(stdin.readline())\n",
      "    ans = 0\n",
      "    for i in range(n):\n",
      "        x = stdin.readline().split()\n",
      "        a = int(x[0])\n",
      "        b = int(x[1])\n",
      "        ri = int(x[2])\n",
      "        D = math.sqrt((a*a) + (b*b))\n",
      "        if D-ri >= r and d >= 2*ri and D+ri <= r+d:\n",
      "            ans += 1\n",
      "    print(ans)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()</s>\n"
     ]
    }
   ],
   "source": [
    "print(f\"user input: \\n{test_input[ppl.index(min(ppl))]}\")\n",
    "print('#################')\n",
    "print(f\"prediction: \\n{test_output[ppl.index(min(ppl))]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**\n",
    "\n",
    "BLEU focuses on precision by counting matching n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU: 3.804e-09\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import BleuMetric\n",
    "\n",
    "input_ids = tokenizer(test_input[0], return_tensors=\"pt\", max_length = 2000).to(device)\n",
    "output = t5.generate(**input_ids, max_new_tokens=800)\n",
    "gen_text = tokenizer.decode(output[0])\n",
    "\n",
    "bleu = BleuMetric.compute(gen_text, [test_output[0]])\n",
    "print(f\"BLEU: {bleu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bleu metric has a very low value, indicating very little overlap between the generated text and the solution of the test sample "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1 Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.03226\n"
     ]
    }
   ],
   "source": [
    "from parlai.core.metrics import F1Metric\n",
    "\n",
    "f1_score = F1Metric.compute(gen_text, [test_output[0]])\n",
    "print(f\"F1: {f1_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning T5-small\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discaimer:** We decided to not include the whole code for this section: the code was very similar to the one used to train the T5-base model and, given the already very long notebook and the not-excellent performance after the fine-tuning, we considered best to not include it.\n",
    "\n",
    "The T5-small model has 60M parameters, and is part of the family of the T5 text-to-text models. It was trained on Common Crawl.\n",
    "For the fine-tuning of T5-small we decided to keep 5000 samples. The model is relatively small, with only 60M parameters and already trained: a higher amount of samples led to worse performance because it caused the training to overwrite parameters unecessarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer\n",
    "We include the model specifics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google-t5/t5-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84ac14146d2642fba5c2e5cb2a815caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c81648f36f44b088194190ddf8e428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593dc97b085d415e82f8695676687a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b646753724640f7bc734376016da4db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbff42bad7d14a1482bace3887551b29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "561f0690a14c4ef1b13965c76ca1d97e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 512)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t5 = T5ForConditionalGeneration.from_pretrained(model_name, device_map=device)\n",
    "t5.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32100, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32100, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(t5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity calculated before the fine-tuning was: [66.54613901709006, 105.79112153990572, 35432.329456935135] with\n",
    "average perplexity: 11868.222239164044.\n",
    "\n",
    "The training lasted 1314.5914108753204 seconds.\n",
    "\n",
    "We then tested the text generation of the fine-tuned model on a random sample from the test set: also in this case, the model does not perform well. It doesn't generate the code, mainly copies the input, and struggles to keep the separation between 'User' and 'Assistant'. Overall the generated text from T5-small is less clear than the text generated by the training of T5-base: the bigger amount of parameters of T5-base guarantees better text quality.\n",
    "\n",
    "We considered the evaluation metrics Perplexity, Bleu and F1.\n",
    "\n",
    "**Perplexity:** it was calculated considering three samples form the test set, the results are: [439.4691293912123, 720.7729464902942, 1981.2242516858746] with average perplexity: 1047.1554425224604. We can see that the fine-tuning did not increase considerably the performance of the model. The average of the perplexity decreased though, indicating a better understanding of the new functionality with respect to the model prior the fine-tuning.\n",
    "\n",
    "**Bleu:** 7.417e-06\n",
    "\n",
    "**F1-score:** 0.0744\n",
    "\n",
    "Also Bleu and F1 scores are extremely low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning gpt2-small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is a transformers model pretrained on a very large corpus of English data in a self-supervised fashion. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts. More precisely, it was trained to guess the next word in sentences.\n",
    "\n",
    "Moreover, inputs are sequences of continuous text of a certain length and the targets are the same sequence, shifted one token (word or piece of word) to the right. The model uses internally a mask-mechanism to make sure the predictions for the token i only uses the inputs from 1 to i but not the future tokens.\n",
    "\n",
    "This is the smallest version of GPT-2, with 124M parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Goodle Drive and upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_project + '/final_ds.csv' if IN_COLAB else '/final_ds.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries that are going to be used in the following subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fine-tuning of gpt2-small we decided to keep 15000 samples. The training lasts approximately 2 hours, and, given the size and the pre-training of the model, gpt2 had better performance with a bigger amount of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(15000).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data into training dataset, validation dataset and test dataset. We decided to use 10% of the dataset for tesing, and the remaining for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1)\n",
    "train, val = train_test_split(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train instances:  10800\n",
      "# test instances:   1500\n",
      "# val instances:    2700\n"
     ]
    }
   ],
   "source": [
    "print('# train instances: ', train.shape[0])\n",
    "print('# test instances:  ', test.shape[0])\n",
    "print('# val instances:   ', val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  50257\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", tokenizer_gpt2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('int', 600),\n",
       " ('ress', 601),\n",
       " ('ations', 602),\n",
       " ('ail', 603),\n",
       " ('Ä 4', 604),\n",
       " ('ical', 605),\n",
       " ('Ä them', 606),\n",
       " ('Ä her', 607),\n",
       " ('ount', 608),\n",
       " ('Ä Ch', 609)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer_gpt2.get_vocab().items())[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'Ä have', 'Ä an', 'Ä array', 'Ä in', 'Ä input', ',', 'Ä order', 'Ä the', 'Ä elements', 'Ä in', 'Ä it', 'Ä in', 'Ä O', '(', 'n', ')', 'Ä time', 'Ä complexity', '.', 'Ä Add', 'Ä a', 'Ä wrong', 'Ä word', 'd']\n"
     ]
    }
   ],
   "source": [
    "text = \"You have an array in input, order the elements in it in O(n) time complexity. Add a wrong wordd\"\n",
    "encoded_input = tokenizer_gpt2._tokenize(text)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1639, 423, 281, 7177, 287, 5128, 11, 1502, 262, 4847, 287, 340, 287, 440, 7, 77, 8, 640, 13357, 13, 3060, 257, 2642, 1573, 67]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = tokenizer_gpt2(text)['input_ids']\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was necessary to use GPT2LMHeadModel instead of GPT2MOdel, beacuse the latter is not compatible with the Trainer due to a mismatch of the arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50257, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = GPT2LMHeadModel.from_pretrained(model_name, device_map=device)\n",
    "gpt2.resize_token_embeddings(len(tokenizer_gpt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124439808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_params = sum(param.numel() for param in gpt2.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: transformer.wte.weight\n",
      "Parameter shape: torch.Size([50257, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.wpe.weight\n",
      "Parameter shape: torch.Size([1024, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in gpt2.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.size()}\")\n",
    "    print(f\"Is trainable: {param.requires_grad}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both Tokenizer and Model we can tokenize the dataset and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eos tokens have to be added by hand to obtain a single string for each 'conversation'. In a single string we include the user input, that includes the description of the problem, and the time and space complexities, and the espected answer from the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eos_token(idx, df, eos_token):\n",
    "    # build the user input including the problem description, the time complexity and the space complexity\n",
    "    chat_string = 'User:' + df.loc[idx, 'problem_description'] + ' Time complexity: ' + df.loc[idx, 'time_complexity_inferred'] + '; Space complexity: ' + df.loc[idx, 'space_complexity_inferred']\n",
    "    # now add the eos token and the response from the assistant, the code solution for the problem\n",
    "    chat_string = chat_string + eos_token + 'Assistant: ' + df.loc[idx, 'solution_code'] + eos_token\n",
    "    return chat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = [apply_eos_token(idx, train, tokenizer_gpt2.eos_token) for idx in train.index]\n",
    "test_str = [apply_eos_token(idx, test, tokenizer_gpt2.eos_token) for idx in test.index]\n",
    "val_str = [apply_eos_token(idx, val, tokenizer_gpt2.eos_token) for idx in val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"User: Naruto has sneaked into the Orochimaru's lair and is now looking for Sasuke. There are T rooms there. Every room has a door into it, each door can be described by the number n of seals on it and their integer energies a_1, a_2, ..., a_n. All energies a_i are nonzero and do not exceed 100 by absolute value. Also, n is even.\\n\\nIn order to open a door, Naruto must find such n seals with integer energies b_1, b_2, ..., b_n that the following equality holds: a_{1} â‹… b_{1} + a_{2} â‹… b_{2} + ... + a_{n} â‹… b_{n} = 0. All b_i must be nonzero as well as a_i are, and also must not exceed 100 by absolute value. Please find required seals for every room there.\\n\\nInput\\n\\nThe first line contains the only integer T (1 â‰¤ T â‰¤ 1000) standing for the number of rooms in the Orochimaru's lair. The other lines contain descriptions of the doors.\\n\\nEach description starts with the line containing the only even integer n (2 â‰¤ n â‰¤ 100) denoting the number of seals.\\n\\nThe following line contains the space separated sequence of nonzero integers a_1, a_2, ..., a_n (|a_{i}| â‰¤ 100, a_{i} â‰  0) denoting the energies of seals.\\n\\nOutput\\n\\nFor each door print a space separated sequence of nonzero integers b_1, b_2, ..., b_n (|b_{i}| â‰¤ 100, b_{i} â‰  0) denoting the seals that can open the door. If there are multiple valid answers, print any. It can be proven that at least one answer always exists.\\n\\nExample\\n\\nInput\\n\\n\\n2\\n2\\n1 100\\n4\\n1 2 3 6\\n\\n\\nOutput\\n\\n\\n-100 1\\n1 1 1 -1\\n\\nNote\\n\\nFor the first door Naruto can use energies [-100, 1]. The required equality does indeed hold: 1 â‹… (-100) + 100 â‹… 1 = 0.\\n\\nFor the second door Naruto can use, for example, energies [1, 1, 1, -1]. The required equality also holds: 1 â‹… 1 + 2 â‹… 1 + 3 â‹… 1 + 6 â‹… (-1) = 0. Time complexity: O(n*m); Space complexity: O(n*m)<|endoftext|>Assistant: a = int(input())\\nfor i in range(a):\\n    n = int(input())\\n    f = list(map(int, input().split()))\\n    ans = []\\n    for i in range(0, n, 2):\\n        ans.append(-1 * f[i + 1])\\n        ans.append(f[i])\\n    print(*ans)<|endoftext|>\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings where the eos token was applied, are now put in a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'chat': train_str})\n",
    "test_data = Dataset.from_dict({'chat': test_str})\n",
    "val_data = Dataset.from_dict({'chat': val_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['val'] = val_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c52e170969a44158db480a827a0d495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0d2f7a6ffdb427aaa79275b497db212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f439e6d7985d4d3aa00fe5fcb5f80f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer_gpt2(examples[\"chat\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids,\n",
    "        'attention_mask': input_encodings.attention_mask,\n",
    "        'labels': input_encodings.input_ids.copy()\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sequences in same batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer_gpt2, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Perplexity before fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random conversation from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|>Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input:  User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant: \n",
      "####\n",
      "Correct solution output:  Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "test_input = test_input + tokenizer_gpt2.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: ', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: ', test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use pipeline from the transformers library to generate the output of the model, indicating the task 'text-generation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipeline transformers evaluate\n",
    "from transformers import pipeline\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant: __________________\n",
      "\n",
      "\n",
      "\"Don't ever say it aloud or ask it aloud before you start saying it, you might end up being a fool! It is not a good look for a fool! If you have no idea what you need you can always just tell someone off.\"\n",
      "\n",
      "-John Dillinger, 1963\n",
      "\n",
      "__________________\n",
      "\n",
      "\"I know that there is one thing that every and all man must do when he sees and hears men and he takes them to be men without even touching them except for the most slight and the smallest. But in the man's world all is the same; he is the same for himself. But now he has a whole world to go to which he has no control; and therefore the man must go to his own home in the greatest number of places.\"\n",
      "\n",
      "-Frank Sinatra, A Song of Ice and Fire by Paul McCartney\n",
      "\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe(test_input, max_new_tokens=256)\n",
    "print(\"Generated text:\\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text has nothing to do with python code and includes references to pop culture, probably included in the pre-training of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa8cc511ba0e4cb5a3e23041a662e9ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da6489352864ee3ba66ca0f0a9d92f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity before fine-tuning: {'perplexities': [28.182695388793945, 14.845183372497559, 14.715679168701172], 'mean_perplexity': np.float64(19.24785264333089)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity_before_finetuning = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    add_start_token=False,\n",
    "    model_id= 'gpt2'\n",
    ")\n",
    "print(\"Perplexity before fine-tuning:\", perplexity_before_finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning\n",
    "Now we can start the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"gpt2_trainer\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8, \n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,  \n",
    "    report_to=None,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= gpt2,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_data['train'],\n",
    "    eval_dataset= tokenized_data['val'],\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time taken by the model to finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2025' max='2025' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2025/2025 2:01:52, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.133900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.341100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.236900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.211400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2025, training_loss=0.4775619372615108, metrics={'train_runtime': 7315.882, 'train_samples_per_second': 4.429, 'train_steps_per_second': 0.277, 'total_flos': 8465861836800000.0, 'train_loss': 0.4775619372615108, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  7316.4667019844055\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Training time: \", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPT-2 model has 148 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (50257, 768)\n",
      "transformer.wpe.weight                                   (1024, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "transformer.h.0.ln_1.weight                                   (768,)\n",
      "transformer.h.0.ln_1.bias                                     (768,)\n",
      "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
      "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias                              (768,)\n",
      "transformer.h.0.ln_2.weight                                   (768,)\n",
      "transformer.h.0.ln_2.bias                                     (768,)\n",
      "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.ln_f.weight                                       (768,)\n",
      "transformer.ln_f.bias                                         (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(gpt2.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: '/content/drive/MyDrive/UnB/NLP_Project/Transformer-trained-models/gpt2_fine_tuning_(chat+code)_2025_05_15_13_50_58'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "gpt2_finetune_path = path_to_project + '/Transformer-trained-models/' + f\"gpt2_fine_tuning_(chat+code)_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer_gpt2.save_pretrained(gpt2_finetune_path)\n",
    "gpt2.save_pretrained(gpt2_finetune_path)\n",
    "print(f\"Checkpoint saved at: \\'{gpt2_finetune_path}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "gpt2_finetune_path = '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/gpt2_fine_tuning_(chat+code)_2025_05_15_13_50_58'\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(gpt2_finetune_path)\n",
    "gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_finetune_path, device_map=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we first extract one chat from the test data randomly, give it as input to the model, and we see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n",
      "User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|>Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      " User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant: \n",
      "####\n",
      "Correct solution output: \n",
      " Assistant: n = int(input())\n",
      "\n",
      "\n",
      "\n",
      "ans = 0\n",
      "kol = 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "for c in list(map(int, input().split())):\n",
      "\tx = c // 2\n",
      "\t\n",
      "\tif x <= kol:\n",
      "\n",
      "\t\tans += x\n",
      "\t\tkol -= x\n",
      "\t\t\n",
      "\t\tif c % 2 == 1:\n",
      "\n",
      "\t\t\tkol += 1\n",
      "\telse:\n",
      "\t\tans += kol\n",
      "\t\tans += (c - kol * 2) // 3\n",
      "\t\tkol = (c - kol * 2) % 3\n",
      "\n",
      "\n",
      "\n",
      "print(ans)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "# add eos token at the end of test_input and test_output\n",
    "test_input = test_input + tokenizer_gpt2.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: \\n', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipeline transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe_finetuned = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after the fine-tuning: \n",
      " User: Pavel has several sticks with lengths equal to powers of two.\n",
      "\n",
      "He has a_0 sticks of length 2^0 = 1, a_1 sticks of length 2^1 = 2, ..., a_{n-1} sticks of length 2^{n-1}. \n",
      "\n",
      "Pavel wants to make the maximum possible number of triangles using these sticks. The triangles should have strictly positive area, each stick can be used in at most one triangle.\n",
      "\n",
      "It is forbidden to break sticks, and each triangle should consist of exactly three sticks.\n",
      "\n",
      "Find the maximum possible number of triangles.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer n (1 â‰¤ n â‰¤ 300 000) â€” the number of different lengths of sticks.\n",
      "\n",
      "The second line contains n integers a_0, a_1, ..., a_{n-1} (1 â‰¤ a_i â‰¤ 10^9), where a_i is the number of sticks with the length equal to 2^i.\n",
      "\n",
      "Output\n",
      "\n",
      "Print a single integer â€” the maximum possible number of non-degenerate triangles that Pavel can make.\n",
      "\n",
      "Examples\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "5\n",
      "1 2 2 2 2\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "1 1 1\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "0\n",
      "\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "3\n",
      "3 3 3\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "3\n",
      "\n",
      "Note\n",
      "\n",
      "In the first example, Pavel can, for example, make this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^4, 2^4), (2^1, 2^3, 2^3), (2^1, 2^2, 2^2).\n",
      "\n",
      "In the second example, Pavel cannot make a single triangle.\n",
      "\n",
      "In the third example, Pavel can, for example, create this set of triangles (the lengths of the sides of the triangles are listed): (2^0, 2^0, 2^0), (2^1, 2^1, 2^1), (2^2, 2^2, 2^2). Time complexity: O(n); Space complexity: O(n)<|endoftext|> Assistant:    n = int(input())\n",
      "    arr = list(map(int,input().split()))\n",
      "arr[0] = 1\n",
      "for i in range(n):\n",
      "    arr[i]+=1\n",
      "print(n-1)\n",
      "\n",
      "for i in range(0,n):\n",
      "    if arr[i]!=n:\n",
      "        print(2-1)\n",
      "    else:\n",
      "        print(2 - 1)\n",
      "    elif arr[i]==2 or arr[i+1]==2:\n",
      "         print(2-1)\n",
      "          print(2)\n",
      "    print(sum(arr[-1]-1-arr[-2]).append(2))\n",
      "else:\n",
      "    print(math.log(lambda x: int(input().strip('\\n') + 1,i)))\n",
      "\n",
      "print(f(length(arr),0))\n",
      "print(max_length(arr))))\n",
      "\n",
      "count = 0\n",
      "for i in range(n):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe_finetuned(test_input, max_new_tokens=256)\n",
    "print(\"Generated text after the fine-tuning: \\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be noticed, after the fine-tuning the model is able to generate python code. The generated text starts with a reference to the user input, but it able to go forward and include an answer from the 'Assistant'. The model is not yet able to generate a correct solution for the problem, but given the smaller size of gpt2-small, it was expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to consider the metrics Perplexity and BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797fa7049fb64a96aaa28f8052347743",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity post fine-tuning: {'perplexities': [1.0604777336120605, 1.047598958015442, 1.0766164064407349], 'mean_perplexity': np.float64(1.0615643660227458)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    add_start_token=False,\n",
    "    model_id= gpt2_finetune_path\n",
    ")\n",
    "print(\"Perplexity post fine-tuning:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity calculated before the fine-tuning was [28.182695388793945, 14.845183372497559, 14.715679168701172] with mean_perplexity: np.float64(19.24785264333089)\n",
    "\n",
    "We can see, now, that the perplexity values have sistematically decreased, indicating that the fine-tuning  process was able to improve the performance of the model on this specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**\n",
    "\n",
    "BLEU focuses on precision by counting matching n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "660ef625dbc3478fba406e861e625092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c3b04b12ae741c2a880a3ded62f815d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8247ea07a842c9a28b6deae512e772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.12839728594651886, 'precisions': [0.3142857142857143, 0.14537444933920704, 0.09271523178807947, 0.06415929203539823], 'brevity_penalty': 1.0, 'length_ratio': 1.6666666666666667, 'translation_length': 455, 'reference_length': 273}\n"
     ]
    }
   ],
   "source": [
    "# BLEU Score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu_metric.compute(\n",
    "    predictions= [pipe_finetuned(test_input[0], max_new_tokens=256)[0]['generated_text']], \n",
    "    references= [test_output[0]]  \n",
    ")\n",
    "print(\"BLEU Score:\", bleu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bleu scores are still low overall, but are much higher than the Bleu scores for the T5 models, further indicating how the better performance among similar sized model was achieved by the gpt2 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning gpt2-CodeParrot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is a base GPT-2 architecture with 124M parameters. It was trained on the huggingface-course/codeparrot-ds-valid dataset, which is a small subset of the original WebText dataset used to train GPT-2, and includes python code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to Goodle Drive and upload the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  import sys\n",
    "  path_to_project = '/content/drive/MyDrive/NLP_Project'\n",
    "  sys.path.append(path_to_project)\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = path_to_project + '/final_ds.csv' if IN_COLAB else '/final_ds.csv'\n",
    "df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries that are going to be used in the following subsections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Pre-processing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the small size of the model and the limited data used during its training, we decided to reduce the number of samples used for the fine-tuning to avoid overwriting parameters unecessarily and degrade the performance of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(5000).copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset the data into training dataset, validation dataset and test dataset. We decided to use 10% of the dataset for tesing, and the remaining for train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.1)\n",
    "train, val = train_test_split(train_val, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# train instances:  3600\n",
      "# test instances:   500\n",
      "# val instances:    900\n"
     ]
    }
   ],
   "source": [
    "print('# train instances: ', train.shape[0])\n",
    "print('# test instances:  ', test.shape[0])\n",
    "print('# val instances:   ', val.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we check the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model and Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retreiev the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'kailasps/GPT2-codeparrot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12718fbcdc654652895919e860807dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e1087eb2ffb48548b32667d2650db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/789k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a785399f3614ffbb250c866ee956712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/448k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f5930f72657487db293be04e2c0b33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc4f11775a7489ba5864f105da06db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2.pad_token = tokenizer_gpt2.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size:  50000\n"
     ]
    }
   ],
   "source": [
    "print(\"vocabulary size: \", tokenizer_gpt2.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ä Sources', 46176),\n",
       " ('Ä cac', 33927),\n",
       " ('Ä segm', 31074),\n",
       " ('electric', 30949),\n",
       " ('Ä untagged', 43874),\n",
       " ('ellipse', 22936),\n",
       " ('Ä Subscription', 26798),\n",
       " ('Ä looping', 24598),\n",
       " ('`', 64),\n",
       " ('HierarchySession', 26069)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(tokenizer_gpt2.get_vocab().items())[600:610]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'Ä have', 'Ä an', 'Ä array', 'Ä in', 'Ä input', ',', 'Ä order', 'Ä the', 'Ä elements', 'Ä in', 'Ä it', 'Ä in', 'Ä O', '(', 'n', ')', 'Ä time', 'Ä complexity', '.', 'Ä Add', 'Ä a', 'Ä wrong', 'Ä word', 'd']\n"
     ]
    }
   ],
   "source": [
    "text = \"You have an array in input, order the elements in it in O(n) time complexity. Add a wrong wordd\"\n",
    "encoded_input = tokenizer_gpt2.tokenize(text)\n",
    "print(encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6147, 1054, 309, 960, 253, 935, 12, 1444, 256, 2347, 253, 577, 253, 690, 8, 78, 9, 626, 16989, 14, 1822, 231, 6401, 1975, 68]\n"
     ]
    }
   ],
   "source": [
    "encoded_ids = tokenizer_gpt2(text)['input_ids']\n",
    "print(encoded_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is uploaded and connected to the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb0b85e73c4faf8f038fa5ed04e055",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/898 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abbfe5fed7f6443baae62f5c80d47151",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/497M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c351cea2915349d2995af5b77369bb14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50000, 768)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2 = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)\n",
    "gpt2.resize_token_embeddings(len(tokenizer_gpt2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50000, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(gpt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider now the number of parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124242432"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_params = sum(param.numel() for param in gpt2.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter name: transformer.wte.weight\n",
      "Parameter shape: torch.Size([50000, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.wpe.weight\n",
      "Parameter shape: torch.Size([1024, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.0.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.1.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.2.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.3.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.4.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.5.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.6.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.7.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.8.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.9.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.10.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_1.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.weight\n",
      "Parameter shape: torch.Size([768, 2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_attn.bias\n",
      "Parameter shape: torch.Size([2304])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.weight\n",
      "Parameter shape: torch.Size([768, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.attn.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.ln_2.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.weight\n",
      "Parameter shape: torch.Size([768, 3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_fc.bias\n",
      "Parameter shape: torch.Size([3072])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.weight\n",
      "Parameter shape: torch.Size([3072, 768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.h.11.mlp.c_proj.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.weight\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n",
      "Parameter name: transformer.ln_f.bias\n",
      "Parameter shape: torch.Size([768])\n",
      "Is trainable: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in gpt2.named_parameters():\n",
    "    print(f\"Parameter name: {name}\")\n",
    "    print(f\"Parameter shape: {param.size()}\")\n",
    "    print(f\"Is trainable: {param.requires_grad}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have both Tokenizer and Model we can tokenize the dataset and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_eos_token(idx, df, eos_token):\n",
    "    # build the user input including the problem description, the time complexity and the space complexity\n",
    "    chat_string = 'User:' + df.loc[idx, 'problem_description'] + ' Time complexity: ' + df.loc[idx, 'time_complexity_inferred'] + '; Space complexity: ' + df.loc[idx, 'space_complexity_inferred']\n",
    "    # now add the eos token and the response from the assistant, the code solution for the problem\n",
    "    chat_string = chat_string + eos_token + 'Assistant: ' + df.loc[idx, 'solution_code'] + eos_token\n",
    "    return chat_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str = [apply_eos_token(idx, train, tokenizer_gpt2.eos_token) for idx in train.index]\n",
    "test_str = [apply_eos_token(idx, test, tokenizer_gpt2.eos_token) for idx in test.index]\n",
    "val_str = [apply_eos_token(idx, val, tokenizer_gpt2.eos_token) for idx in val.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'User: Manao has invented a new mathematical term â€” a beautiful set of points. He calls a set of points on a plane beautiful if it meets the following conditions:\\n\\n  1. The coordinates of each point in the set are integers. \\n  2. For any two points from the set, the distance between them is a non-integer. \\n\\n\\n\\nConsider all points (x, y) which satisfy the inequations: 0 â‰¤ x â‰¤ n; 0 â‰¤ y â‰¤ m; x + y > 0. Choose their subset of maximum size such that it is also a beautiful set of points.\\n\\nInput\\n\\nThe single line contains two space-separated integers n and m (1 â‰¤ n, m â‰¤ 100).\\n\\nOutput\\n\\nIn the first line print a single integer â€” the size k of the found beautiful set. In each of the next k lines print a pair of space-separated integers â€” the x- and y- coordinates, respectively, of a point from the set.\\n\\nIf there are several optimal solutions, you may print any of them.\\n\\nExamples\\n\\nInput\\n\\n2 2\\n\\n\\nOutput\\n\\n3\\n0 1\\n1 2\\n2 0\\n\\n\\nInput\\n\\n4 3\\n\\n\\nOutput\\n\\n4\\n0 3\\n2 1\\n3 0\\n4 2\\n\\nNote\\n\\nConsider the first sample. The distance between points (0, 1) and (1, 2) equals <image>, between (0, 1) and (2, 0) â€” <image>, between (1, 2) and (2, 0) â€” <image>. Thus, these points form a beautiful set. You cannot form a beautiful set with more than three points out of the given points. Note that this is not the only solution. Time complexity: O(n+m); Space complexity: O(n+m)<|endoftext|>Assistant: n,m=map(int,input().split())\\nif n>m:\\n    print(m+1)\\n    for i in range(m+1):\\n        print(n-i,i)\\nelse:\\n    print(n+1)\\n    for i in range(n+1):\\n        print(i,m-i)<|endoftext|>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_str[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strings where the eos token was applied, are now put in a dictionary structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = Dataset.from_dict({'chat': train_str})\n",
    "test_data = Dataset.from_dict({'chat': test_str})\n",
    "val_data = Dataset.from_dict({'chat': val_str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DatasetDict()\n",
    "data['train'] = train_data\n",
    "data['val'] = val_data\n",
    "data['test'] = test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "096e3cf827674134aa59ab45adff3d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b9e0817954d40ab8d41990bde0b91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17543e30d4f0451d9c281a4dfdbfd7aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    input_encodings = tokenizer_gpt2(examples[\"chat\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512)\n",
    "    sample = {\n",
    "        'input_ids': input_encodings.input_ids,\n",
    "        'attention_mask': input_encodings.attention_mask,\n",
    "        'labels': input_encodings.input_ids.copy()\n",
    "    }\n",
    "    return sample\n",
    "\n",
    "tokenized_data = data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all sequences in same batch\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer_gpt2, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the training starts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate Perplexity before fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a random conversation from the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input:  User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|> Assistant: \n",
      "####\n",
      "Correct solution output:  Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "test_input = test_input + tokenizer_gpt2.eos_token + ' Assistant: '\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: ', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: ', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pipeline\n",
      "  Downloading pipeline-0.1.0-py3-none-any.whl.metadata (483 bytes)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Downloading pipeline-0.1.0-py3-none-any.whl (2.6 kB)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pipeline, evaluate\n",
      "Successfully installed evaluate-0.4.3 pipeline-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pipeline transformers evaluate\n",
    "from transformers import pipeline\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text:\n",
      " User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|> Assistant:  In 1-15 is the data.\n",
      "[2. 2D 2. # (d as the current row from the first\n",
      "    :math.0, in the same and 7.  J.\n",
      "\n",
      "N and *k (P-D array to be defined from the model) for the `s, Ral is no set by default value, *6f1.0 (or R.0, which that ``b\n",
      "\"\"\"  The first column are the full\n",
      "* are the following\n",
      "    G/6.0. In the input-4.18 (not \"L2.e.  The color the maximum label of the same number of the Si -6/E-2.\n",
      " *a with the Apache problem. the\n",
      "\n",
      "The value of the training size in the maximum input). If not to be a same data, a numpy and x.\n",
      "\n",
      "\n",
      "\n",
      "* the C-1.\n",
      "T-2. The data.0 *x, * np.\n",
      "/2.0 and the given time.0.\n",
      "\n",
      "The number of the \"o.0 in the same kernel of the same class, which. *Y are been two features file, - `r(a. -4.\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe(test_input, max_new_tokens=256)\n",
    "print(\"Generated text:\\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text does not include the solution code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84b152fff5d4f0299a8a844882ede08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/8.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e5631bc2a7047aaab49992ebb704e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity before fine-tuning: {'perplexities': [653.0138549804688, 570.7035522460938, 424.50201416015625], 'mean_perplexity': np.float64(549.4064737955729)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity_before_finetuning = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    model_id= model_name\n",
    ")\n",
    "print(\"Perplexity before fine-tuning:\", perplexity_before_finetuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fine-Tuning\n",
    "Now we can start the fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    \"gpt2-parrot_trainer\",\n",
    "    #evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8, \n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    learning_rate=6.25e-5,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    bf16=True,  \n",
    "    report_to=None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= gpt2,\n",
    "    args= training_args,\n",
    "    train_dataset= tokenized_data['train'],\n",
    "    eval_dataset= tokenized_data['val'],\n",
    "    data_collator= data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the time taken by the model to finish training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "begin = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 39:40, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.458700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=675, training_loss=1.1649107078269676, metrics={'train_runtime': 2385.8294, 'train_samples_per_second': 4.527, 'train_steps_per_second': 0.283, 'total_flos': 2821953945600000.0, 'train_loss': 1.1649107078269676, 'epoch': 3.0})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time:  2386.2620153427124\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Training time: \", end - begin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The GPT-2 model has 148 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "transformer.wte.weight                                  (50000, 768)\n",
      "transformer.wpe.weight                                   (1024, 768)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "transformer.h.0.ln_1.weight                                   (768,)\n",
      "transformer.h.0.ln_1.bias                                     (768,)\n",
      "transformer.h.0.attn.c_attn.weight                       (768, 2304)\n",
      "transformer.h.0.attn.c_attn.bias                             (2304,)\n",
      "transformer.h.0.attn.c_proj.weight                        (768, 768)\n",
      "transformer.h.0.attn.c_proj.bias                              (768,)\n",
      "transformer.h.0.ln_2.weight                                   (768,)\n",
      "transformer.h.0.ln_2.bias                                     (768,)\n",
      "transformer.h.0.mlp.c_fc.weight                          (768, 3072)\n",
      "transformer.h.0.mlp.c_fc.bias                                (3072,)\n",
      "transformer.h.0.mlp.c_proj.weight                        (3072, 768)\n",
      "transformer.h.0.mlp.c_proj.bias                               (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "transformer.ln_f.weight                                       (768,)\n",
      "transformer.ln_f.bias                                         (768,)\n"
     ]
    }
   ],
   "source": [
    "# Get all of the model's parameters as a list of tuples.\n",
    "params = list(gpt2.named_parameters())\n",
    "\n",
    "print('The GPT-2 model has {:} different named parameters.\\n'.format(len(params)))\n",
    "\n",
    "print('==== Embedding Layer ====\\n')\n",
    "\n",
    "for p in params[0:2]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "\n",
    "for p in params[2:14]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "\n",
    "for p in params[-2:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50000, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50000, bias=False)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gpt2.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model's parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved at: '/content/drive/MyDrive/SFX/NLP_Project/Transformer-trained-models/gpt2-parrot_fine_tuning_2025_05_16_14_58_31'\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "gpt2_finetune_path = path_to_project + '/Transformer-trained-models/' + f\"gpt2-parrot_fine_tuning_{datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}\"\n",
    "tokenizer_gpt2.save_pretrained(gpt2_finetune_path)\n",
    "gpt2.save_pretrained(gpt2_finetune_path)\n",
    "print(f\"Checkpoint saved at: \\'{gpt2_finetune_path}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "#gpt2_finetune_path = '/content/drive/MyDrive/NLP_Project/Transformer-trained-models/gpt2-parrot_fine_tuning_2025_05_16_14_58_31'\n",
    "tokenizer_gpt2 = AutoTokenizer.from_pretrained(gpt2_finetune_path)\n",
    "gpt2 = AutoModelForCausalLM.from_pretrained(gpt2_finetune_path, device_map = device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the model, we first extract one chat from the test data randomly, give it as input to the model, and we see the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(43)\n",
    "\n",
    "idx = random.choice(range(len(test_data))) # select a random conversation\n",
    "print(idx)\n",
    "dialogue = test_data['chat'][idx]\n",
    "print(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User input: \n",
      " User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>\n",
      "####\n",
      "Correct solution output: \n",
      " Assistant: from math import ceil\n",
      "n = int(input())\n",
      "X = list(map(int,input().split()))\n",
      "for x in X:\n",
      "\n",
      "\n",
      "\tres = \"NO\"\n",
      "\tl = ceil((x - 20) / 14)\n",
      "\tfor k in range(l+100, l-101, -1):\n",
      "\t\tif k >= 0 and 15+14*k<=x<=20+14*k:\n",
      "\t\t\tres = \"YES\"\n",
      "\t\t\tbreak\n",
      "\tprint(res)<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# now take only the 'input part' and the 'output part'\n",
    "# parse string\n",
    "test_input, test_output, end = dialogue.split('<|endoftext|>')\n",
    "\n",
    "# add eos token at the end of test_input and test_output\n",
    "test_input = test_input + tokenizer_gpt2.eos_token\n",
    "test_output = test_output + tokenizer_gpt2.eos_token\n",
    "\n",
    "print('User input: \\n', test_input)\n",
    "print('####')\n",
    "print('Correct solution output: \\n', test_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pipeline transformers evaluate\n",
    "from transformers import pipeline\n",
    "import evaluate\n",
    "pipe_finetuned = pipeline(\"text-generation\", model= gpt2, tokenizer=tokenizer_gpt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text after the fine-tuning: \n",
      " User: Bob is playing with 6-sided dice. A net of such standard cube is shown below.\n",
      "\n",
      "<image>\n",
      "\n",
      "He has an unlimited supply of these dice and wants to build a tower by stacking multiple dice on top of each other, while choosing the orientation of each dice. Then he counts the number of visible pips on the faces of the dice.\n",
      "\n",
      "For example, the number of visible pips on the tower below is 29 â€” the number visible on the top is 1, from the south 5 and 3, from the west 4 and 2, from the north 2 and 4 and from the east 3 and 5.\n",
      "\n",
      "<image>\n",
      "\n",
      "The one at the bottom and the two sixes by which the dice are touching are not visible, so they are not counted towards total.\n",
      "\n",
      "Bob also has t favourite integers x_i, and for every such integer his goal is to build such a tower that the number of visible pips is exactly x_i. For each of Bob's favourite integers determine whether it is possible to build a tower that has exactly that many visible pips.\n",
      "\n",
      "Input\n",
      "\n",
      "The first line contains a single integer t (1 â‰¤ t â‰¤ 1000) â€” the number of favourite integers of Bob. \n",
      "\n",
      "The second line contains t space-separated integers x_i (1 â‰¤ x_i â‰¤ 10^{18}) â€” Bob's favourite integers.\n",
      "\n",
      "Output\n",
      "\n",
      "For each of Bob's favourite integers, output \"YES\" if it is possible to build the tower, or \"NO\" otherwise (quotes for clarity).\n",
      "\n",
      "Example\n",
      "\n",
      "Input\n",
      "\n",
      "\n",
      "4\n",
      "29 34 19 38\n",
      "\n",
      "\n",
      "Output\n",
      "\n",
      "\n",
      "YES\n",
      "YES\n",
      "YES\n",
      "NO\n",
      "\n",
      "Note\n",
      "\n",
      "The first example is mentioned in the problem statement.\n",
      "\n",
      "In the second example, one can build the tower by flipping the top dice from the previous tower.\n",
      "\n",
      "In the third example, one can use a single die that has 5 on top.\n",
      "\n",
      "The fourth example is impossible. Time complexity: O(nlogn); Space complexity: O(n)<|endoftext|>Assistant: n=int(input())\n",
      "l=list(map(int,input().split()))\n",
      "    1=0})\n",
      "for i in n:\n",
      "  if j==1:\n",
      "    print('NO')\n",
      "  else:\n",
      "    print(\"NO\")\n",
      "else:\n",
      "    print(\"NO\")\n",
      "else:\n",
      "    print(\"NO\")\n",
      "else:\n",
      "    print(1%2)\n",
      "  print(\"NO\")\n",
      "else:\n",
      "    def __join(int(n):\n",
      "    print(int(n-n-0)\n",
      "    print('NO')\n",
      "    print([])\n",
      "    print(list(int(int,1))\n",
      "    print(\"0 or (n-1,k2))\n",
      "    print(\"NO\")\n",
      "    print(\"NO\")\n",
      "    print(\"YES\")\n",
      "else: A1!=0\")\n",
      "    print(3\")//2 // 2 % 2) +(1))\n",
      "    \n",
      "    for i in range(max(n-2):\n",
      "        print(\"NO\")\n",
      "    else:\n",
      "        print(\"NO\")\n",
      "\n",
      "\n",
      "if n*m):\n",
      "        print(\"YES\")\n",
      "       elif(s[0):\n",
      "        print(\"YES\")\n",
      "    else:\n",
      "        print(\"YES\")\n",
      "    else:\n",
      "        print(\"YES\")\n",
      "    print(\"NO\")\n",
      "    if(3[0]\n",
      "    else\n"
     ]
    }
   ],
   "source": [
    "gen_output = pipe_finetuned(test_input, max_new_tokens=256)\n",
    "print(\"Generated text after the fine-tuning: \\n\", gen_output[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated text after the fine-tuning includes now the python code. The code is still not correct, especially from a syntactical point of view, but the model is able to maintain the espected format of the response ('user:' and 'assistant:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was decided to consider the metrics Perplexity and BLEU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perplexity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get inputs from test_data\n",
    "test_input = [dialogue.split('<|endoftext|>')[0] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]\n",
    "\n",
    "# get outputs from test data\n",
    "test_output = [dialogue.split('<|endoftext|>')[1] + tokenizer_gpt2.eos_token for dialogue in test_data['chat'][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffd5be745124ab58514a9d00fd161c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity post fine-tuning: {'perplexities': [1.080601692199707, 1.1230411529541016, 1.0684187412261963], 'mean_perplexity': np.float64(1.0906871954600017)}\n"
     ]
    }
   ],
   "source": [
    "perplexity_metric = evaluate.load(\"perplexity\", module_type=\"metric\")\n",
    "perplexity = perplexity_metric.compute(\n",
    "    predictions= test_input,\n",
    "    add_start_token=False,\n",
    "    model_id= gpt2_finetune_path\n",
    ")\n",
    "print(\"Perplexity post fine-tuning:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perplexity calculated before the fine-tuning was [653.0138549804688, 570.7035522460938, 424.50201416015625] with mean_perplexity: np.float64(549.4064737955729)\n",
    "\n",
    "Now we can notice how the values for the perplexity are much lower, similar to those achieved by the gpt2-small model, indicating how the fine-tuning process managed to improve the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BLEU**\n",
    "\n",
    "BLEU focuses on precision by counting matching n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52870268b7134df49be0e7b83d29d57f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00fed6b270f4f83bca8953b54bde8bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafb1988b06b47609d38a246e30eb6f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.05315658078526119, 'precisions': [0.3118081180811808, 0.10351201478743069, 0.03333333333333333, 0.0074211502782931356], 'brevity_penalty': 1.0, 'length_ratio': 1.7828947368421053, 'translation_length': 542, 'reference_length': 304}\n"
     ]
    }
   ],
   "source": [
    "# BLEU Score\n",
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "bleu_results = bleu_metric.compute(\n",
    "    predictions= [pipe_finetuned(test_input[0], max_new_tokens=256)[0]['generated_text']], \n",
    "    references= [test_output[0]]  \n",
    ")\n",
    "print(\"BLEU Score:\", bleu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bleu score, even if higher than the scores achieved by the T5 models, are still low, indicating that the generated text is still far from the reference text. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
