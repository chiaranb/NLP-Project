{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f292e135",
      "metadata": {
        "id": "f292e135"
      },
      "source": [
        "# Try some LLMs on the task\n",
        "\n",
        "The notebook tests how different pre-trained language models perform code generation from a textual problem description. Three prompting techniques are explored:\n",
        "\n",
        "* Zero-shot: The model receives only the problem statement.\n",
        "\n",
        "* One-shot: One example of a solved problem is provided.\n",
        "\n",
        "* Few-shot: Multiple examples are given before asking the model to solve a new problem.\n",
        "\n",
        "Three different LLMs are compared across these settings:\n",
        "\n",
        "\n",
        "* TinyLlama-1.1B: 1.1B parameters (https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
        "* deepseek-coder-1.3b-base: 1.3B parameters (https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base)\n",
        "* Mistral-7B-Instruct-v0.1: 7B parameters (https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b3bb191",
      "metadata": {
        "id": "7b3bb191"
      },
      "source": [
        "## Imports\n",
        "\n",
        "This section installs and imports all necessary libraries for the notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea36cb1d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea36cb1d",
        "outputId": "bf708708-147a-46ff-b320-6a4502747a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2.0.2)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\n",
            "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-3.1.1 sacrebleu-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "944392b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "944392b9",
        "outputId": "b6a0e689-ba42-49bd-b54a-eb84ca9ba211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=cd3991a622edd739af4a9ae10905bdea7e64d0c09a9b9a6bc7ac3fc56813fa84\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ede3db4d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ede3db4d",
        "outputId": "868edb40-eb6d-4ab3-8813-e791bdb379d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51b00d7e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51b00d7e",
        "outputId": "52816d48-6940-4066-ec54-0c7ee38ef7c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.6.0\n",
            "    Uninstalling accelerate-1.6.0:\n",
            "      Successfully uninstalled accelerate-1.6.0\n",
            "Successfully installed accelerate-1.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cacfe386",
      "metadata": {
        "id": "cacfe386"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3905c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b3905c4",
        "outputId": "5b7ebee8-7f2d-4fd5-d59f-aca955b8610e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/50.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain-community langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb1836a0",
      "metadata": {
        "id": "bb1836a0"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import sacrebleu\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d41fe725",
      "metadata": {
        "id": "d41fe725"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a195cab4",
      "metadata": {
        "id": "a195cab4"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28db394b",
      "metadata": {
        "id": "28db394b"
      },
      "source": [
        "## Load Dataset\n",
        "\n",
        "Load the dataset that has already been preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4d9c1fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "a4d9c1fd",
        "outputId": "a38fa7c7-7342-4d77-d7aa-a4d38680ac90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/17WgJO1gfIBADpYX2jVdb41q7HCbwWcOU/NLP_Project'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "path = 'Colab Notebooks/NLP/NLP_Project'\n",
        "\n",
        "os.chdir(f'/content/drive/MyDrive/{path}')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed1f3c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eed1f3c0",
        "outputId": "88736781-cf2b-4387-879d-0be2eb807c7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 problem_description solution_id  \\\n",
            "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
            "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
            "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
            "3  Xenia has a set of weights and pan scales. Eac...         0_6   \n",
            "4  Xenia has a set of weights and pan scales. Eac...         0_8   \n",
            "\n",
            "                                       solution_code  \\\n",
            "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
            "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
            "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
            "3  MOD = 10**9 + 7\\nI = lambda:list(map(int,input...   \n",
            "4  to_print = []\\ndef dfs(d, ini, s, depth, m):\\n...   \n",
            "\n",
            "               problem_name time_complexity_inferred space_complexity_inferred  \n",
            "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
            "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "3  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "4  339_C. Xenia and Weights                     O(1)                      O(1)  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('final_ds.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc30840a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc30840a",
        "outputId": "d5073d1c-2617-4602-9234-522db84a51e9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(244876, 6)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5500776",
      "metadata": {
        "id": "d5500776"
      },
      "source": [
        "## Try TinyLlama on the task\n",
        "\n",
        "The TinyLlama-1.1B-Chat-v1.0 is a compact open-source language model with 1.1 billion parameters, trained for chat and code generation tasks. This version is fine-tuned for multi-turn conversations and instruction-following behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7118aa6",
      "metadata": {
        "id": "d7118aa6"
      },
      "source": [
        "Here we initializes a Hugging Face pipeline for text generation, the pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72306705",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363,
          "referenced_widgets": [
            "33e12e57a3a44609816c57fd9369313a",
            "1bbc6d7ada6740a98c1cd1011a8942d6",
            "08f19498b215477f9f66c3e255d4fc8d",
            "10363323aac14ee68a113a3023b688b7",
            "7de60d1676754d25992e0c56c36d4021",
            "e13ee6b3517641e4b9ce1bd0f3784b93",
            "b103b30abd5846cc833964fc5f223dcc",
            "52ee37f3e62443d59d7a93bd5aa5c4ef",
            "1e53ef3270f140ceb73da442f8e5b6f0",
            "3604917ccef14975ae759f2116ac9fef",
            "f729215829964eb9a0dfd112351810df",
            "4e71d4787c8f4ea58d82de252ca9d48c",
            "93d4b9a899b1433f9ada5cffd9a0beb5",
            "03de041c94704b248438d90eb161db31",
            "895994746ce14a52a30e05f593063c3f",
            "6c4754381ec54e13bafc0ba36de8f232",
            "a47a07b05fbf44e9a624fb43f38c690e",
            "fa73e7c00c784bc0b718090cc8361103",
            "790092184906487bb99ded5135278955",
            "8220f521b6d7402f93b247d426479c93",
            "08a843493e3d49338a55d681d816362b",
            "d13831c435ce40b5b1ef5680af69623d",
            "0cb38e67f2e14148b30cd8c40e88305b",
            "dc860a07db014cea97bc2cc289eceb7f",
            "14155752c9d64899b784e696f3286f59",
            "2856fdce52eb4da0ad5f6b5ccd39df73",
            "59e127b4ae7a4c5d88d040b15b58bcb7",
            "d802ddbd96214a36a46019c9b8470875",
            "df30d5558e19476cada8f71c9ae10b48",
            "e18fdfb0cf1f407daea988565ae021ff",
            "71e554f7b69f4441b84ca73bf1f051b9",
            "c85fad37b83b4ba089e88edf673e485a",
            "ad5079c77b3a40f9a7af88a016127f47",
            "551622d595c3410e9064e3d245dc9efb",
            "53064cf23c2c4780bc5ce4486fa2e745",
            "3c1741a1db5e4bcf86375bb7a38b570d",
            "3f2dbd4cfb89422581e674b1c014cc58",
            "53dc523deb3048c88b912d4b61bffcf3",
            "168ee20b8c784985aadc8c10eefe8282",
            "c9d36a6174614b1d9fc7f1ec6ca5d0b0",
            "74fdb5165d8f4576909df7f47c7d9dd3",
            "d7cd30e16f2e4a9fb6faba3ba88fa474",
            "f0252a42b8054dfba4ffb0b031474a9e",
            "1ccb444b5a784aaeb3713efbeb94978b",
            "4f4fbdd063ba46509eb3774e6e460dc9",
            "c9212337f663463fba892853a0fca04a",
            "d5d1075e9f734f5f89d6efa2c9a5c332",
            "850121d4cd0a45e293127f39db720d87",
            "f981f779839c4c9d9ab65f2971560e04",
            "aa4d89d66718446186db72914abf1ddf",
            "c8f75b9d3dfa4237871098c4b0817fa5",
            "e5eb3f0548ad4ed9a5a0139f775abb67",
            "075c8fe7d6274b9c8f40423902610a28",
            "e1486a16ea4f4ffca0d71adbbf4ff0d7",
            "4a82b93e9a1146d3a69c7697bd7adec2",
            "39d5156e58034ff69dde71c1ffa892ad",
            "428b2de4cfde490e9c5e8346c61ab433",
            "e31fd63683084f1f90a3dc8209cd7088",
            "80a8c525d6a041b79d34cac1d4d90a17",
            "945fa66824ea4cac93547a12c04771fb",
            "a3d204bea94f47c29c9f038f11facd7b",
            "ac4144836be24270b8fba210931e08f0",
            "57712c4f4f2d437381fc30d25d6607bc",
            "14d5f227e9ce444b98a51f3f7ac24bee",
            "463a242bb5cf405898d0ec48ed70fd16",
            "ab1b520d88254a939042ea4b978065c5",
            "1ef96045a0a442b996a71f1da51c954d",
            "9013fac9ba054f4ca7504684a5ff77b6",
            "10f8d7771d40468e8320b67736611dfd",
            "09a0fdb6fc6c44239b8f4385d65de7e5",
            "986980aa1f954f41a3cc2cc676ecf99a",
            "ee69b7f5b2cc49a69a6bfcb5a35c222e",
            "f3fed1da9ba948888081f8eef633c45f",
            "45e01c47dd83425fbd71c538ebd3f5de",
            "0adaae9c96ef4656941ffd38c559f93b",
            "bade19de629b44279788c83496fa4d65",
            "3d96a0c8054d4e2a89832b718aa632ab"
          ]
        },
        "id": "72306705",
        "outputId": "197409c6-99c2-4170-f5ef-4d15632a91e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33e12e57a3a44609816c57fd9369313a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e71d4787c8f4ea58d82de252ca9d48c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cb38e67f2e14148b30cd8c40e88305b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "551622d595c3410e9064e3d245dc9efb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4f4fbdd063ba46509eb3774e6e460dc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "39d5156e58034ff69dde71c1ffa892ad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ef96045a0a442b996a71f1da51c954d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f0957e2",
      "metadata": {
        "id": "2f0957e2"
      },
      "source": [
        "This function creates a list of chat-style messages to prompt a language model for code generation, using zero-shot, one-shot or few-shot prompting strategies.\n",
        "\n",
        "The returned messages list is formatted with \"system\", \"user\", and \"assistant\" roles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0f1fb3a",
      "metadata": {
        "id": "b0f1fb3a"
      },
      "outputs": [],
      "source": [
        "def build_messages(df, index, mode='zero-shot', num_few_shot=2, random_state=0):\n",
        "    problem = clean_problem_description(df.loc[index, 'problem_description'])\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    if mode == 'zero-shot':\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"Can you please solve the problem below by writing the corresponding Python code.\\n\\n\"\n",
        "                       f\"### Problem:\\n{problem}\\n### Your Solution:\"\n",
        "        })\n",
        "\n",
        "    elif mode == 'one-shot':\n",
        "        example_idx = df.drop(index).sample(1, random_state=random_state).index[0]\n",
        "        example_problem = clean_problem_description(df.loc[example_idx, 'problem_description'])\n",
        "        example_solution = df.loc[example_idx, 'solution_code']\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"### Problem:\\n{example_problem}\\n### Solution Expected:\"\n",
        "        })\n",
        "        messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": f\"{example_solution}\"\n",
        "        })\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Now please solve the problem below by writing the corresponding Python code.\\n\\n\"\n",
        "                f\"### Problem:\\n{problem}\\n### Your Solution:\"\n",
        "            )\n",
        "        })\n",
        "\n",
        "    elif mode == 'few-shot':\n",
        "        example_indices = df.drop(index).sample(num_few_shot, random_state=random_state).index\n",
        "\n",
        "        for i in example_indices:\n",
        "            example_problem = clean_problem_description(df.loc[i, 'problem_description'])\n",
        "            example_solution = df.loc[i, 'solution_code']\n",
        "            messages.append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"### Problem:\\n{example_problem}\\n### Solution Expected:\"\n",
        "            })\n",
        "            messages.append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": f\"{example_solution}\"\n",
        "            })\n",
        "\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": (\n",
        "                \"Now please solve the problem below by writing the corresponding Python code.\\n\\n\"\n",
        "                f\"### Problem:\\n{problem}\\n### Your Solution:\"\n",
        "            )\n",
        "        })\n",
        "\n",
        "    return messages\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ad451d",
      "metadata": {
        "id": "29ad451d"
      },
      "source": [
        "This function uses the chat-formatted messages to build a prompt and pass it to the language model for code generation. The following generation parameters are used:\n",
        "\n",
        "* max_new_tokens=256: limits the number of new tokens the model can generate\n",
        "* do_sample=True: enables sampling, which allows the model to produce more diverse and creative outputs instead of always choosing the most likely next token.\n",
        "* temperature=0.7: controls the randomness of the predictions.\n",
        "* top_k=50: restricts sampling to the top 50 most probable tokens at each step.\n",
        "* top_p=0.95: enables nucleus sampling, where the model considers only the smallest possible set of tokens whose cumulative probability is at least 95%.\n",
        "\n",
        "The model returns the generated Python solution as plain text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ba4005",
      "metadata": {
        "id": "48ba4005"
      },
      "outputs": [],
      "source": [
        "def generate_solution(messages, max_new_tokens=256):\n",
        "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "    return outputs[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f9f467",
      "metadata": {
        "id": "75f9f467"
      },
      "source": [
        "This function evaluates the similarity between a generated solution and a reference solution:\n",
        "* BLEU: Uses sacrebleu to compute n-gram precision.\n",
        "* ROUGE-1 / ROUGE-L: Measures recall-based overlap (unigrams and longest common subsequence).\n",
        "* Exact Match: Checks if the strings are exactly equal after stripping whitespace.\n",
        "\n",
        "Returns a dictionary with all four metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2902c6fd",
      "metadata": {
        "id": "2902c6fd"
      },
      "outputs": [],
      "source": [
        "def evaluate_metrics(reference, hypothesis):\n",
        "    bleu_score = sacrebleu.corpus_bleu([hypothesis], [[reference]]).score\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference, hypothesis)\n",
        "    exact = int(reference.strip() == hypothesis.strip())\n",
        "    return {\n",
        "        'bleu': bleu_score,\n",
        "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
        "        'rougeL': rouge_scores['rougeL'].fmeasure,\n",
        "        'exact_match': exact\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aec751d",
      "metadata": {
        "id": "0aec751d"
      },
      "source": [
        "This function is used to remove the examples inputs and outputs of the problem description, so the one and few shot prompts are less confusing for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41ca9af7",
      "metadata": {
        "id": "41ca9af7"
      },
      "outputs": [],
      "source": [
        "def clean_problem_description(text: str) -> str:\n",
        "    lines = text.strip().splitlines()\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        if line.strip().lower().startswith(\"examples\"):\n",
        "            break\n",
        "        result.append(line)\n",
        "    return \"\\n\".join(result).strip()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08f7e595",
      "metadata": {
        "id": "08f7e595"
      },
      "source": [
        "Here we get a random sample of the dataset so we can test the three prompt types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54929d7e",
      "metadata": {
        "id": "54929d7e"
      },
      "outputs": [],
      "source": [
        "example = df.sample(n=1, random_state=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02af2f2e",
      "metadata": {
        "id": "02af2f2e"
      },
      "outputs": [],
      "source": [
        "n = example.index[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8740a141",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8740a141",
        "outputId": "f6db42ff-02dd-4d10-9c39-23945aac2259"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(15399)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1899961",
      "metadata": {
        "id": "f1899961"
      },
      "outputs": [],
      "source": [
        "example_text = df.iloc[n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d54cefc",
      "metadata": {
        "id": "3d54cefc"
      },
      "outputs": [],
      "source": [
        "clean_text = clean_problem_description(df.loc[n, 'problem_description'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28362204",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "28362204",
        "outputId": "d7fa5350-03c4-43a4-bc5a-906c5fee3075"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clean_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0010fe0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0010fe0",
        "outputId": "cacc5837-af6b-4505-ddd0-6ab81eea99be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "\n",
            "Examples\n",
            "\n",
            "Input\n",
            "\n",
            "3 2\n",
            "2 1\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "1\n",
            "\n",
            "\n",
            "Input\n",
            "\n",
            "5 5\n",
            "2 3\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "6\n",
            "\n",
            "\n",
            "Input\n",
            "\n",
            "2 4\n",
            "2 4\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(example_text['problem_description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55ae18c2",
      "metadata": {
        "id": "55ae18c2"
      },
      "source": [
        "Here we can the the description of the problem we are going to use to test the prompts. It receives four numbers a, b, c and d as inputs and should output (a-c)*(b-d) as the number of remaining white cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fcb029d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "4fcb029d",
        "outputId": "1529aa81-36bf-4dc2-990b-a95257ef31fb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'a,b=map(int,input().split())\\nc,d=map(int,input().split())\\nprint((a-c)*(b-d))'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example_text['solution_code']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa2d85fb",
      "metadata": {
        "id": "aa2d85fb"
      },
      "source": [
        "First we need to build the zero shot prompt using the build_messages function and specifying the zero-shot approach"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9b0ea1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9b0ea1a",
        "outputId": "4dfd5c38-4910-4abc-a05e-6c51ad58f14a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.'}, {'role': 'user', 'content': 'Can you please solve the problem below by writing the corresponding Python code.\\n\\n### Problem:\\nThere are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n### Your Solution:'}]\n"
          ]
        }
      ],
      "source": [
        "zero_shot_messages = build_messages(df, example.index[0], mode='zero-shot')\n",
        "print(zero_shot_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b298450",
      "metadata": {
        "id": "9b298450"
      },
      "source": [
        "Now we pass this prompt to the system and print the generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3e032ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e032ba",
        "outputId": "77171511-1409-4d13-a217-1b28b2761236"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.</s>\n",
            "<|user|>\n",
            "Can you please solve the problem below by writing the corresponding Python code.\n",
            "\n",
            "### Problem:\n",
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "### Your Solution:</s>\n",
            "<|assistant|>\n",
            "Here's the Python code for the problem:\n",
            "\n",
            "```python\n",
            "import sys\n",
            "\n",
            "# Check input format\n",
            "if len(sys.argv)!= 2:\n",
            "    print(\"Usage: python3 solution.py <input_file_name>\")\n",
            "    sys.exit(1)\n",
            "\n",
            "try:\n",
            "    input_file = open(sys.argv[1], \"r\")\n",
            "except FileNotFoundError:\n",
            "    print(\"Input file not found!\")\n",
            "    sys.exit(1)\n",
            "\n",
            "try:\n",
            "    n = int(input_file.readline())\n",
            "    h = int(input_file.readline())\n",
            "    w = int(input_file.readline())\n",
            "    input_file.close()\n",
            "except ValueError:\n",
            "    print(\"Invalid input format!\")\n",
            "    sys.exit(1)\n",
            "\n",
            "# Check if all values are integers\n",
            "while h > 0 or w > 0:\n",
            "    try:\n",
            "        h = int(input_file.readline())\n",
            "    except ValueError:\n",
            "        print(\"Invalid input format!\")\n",
            "        sys.exit(1)\n",
            "    try:\n",
            "        w = int(input_file.read\n"
          ]
        }
      ],
      "source": [
        "zero_shot_generated = generate_solution(zero_shot_messages)\n",
        "print(zero_shot_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "366e7f7c",
      "metadata": {
        "id": "366e7f7c"
      },
      "source": [
        "The code generated by the model for this problem was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24b398de",
      "metadata": {
        "id": "24b398de"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Check input format\n",
        "if len(sys.argv)!= 2:\n",
        "    print(\"Usage: python3 solution.py <input_file_name>\")\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    input_file = open(sys.argv[1], \"r\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Input file not found!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "try:\n",
        "    n = int(input_file.readline())\n",
        "    h = int(input_file.readline())\n",
        "    w = int(input_file.readline())\n",
        "    input_file.close()\n",
        "except ValueError:\n",
        "    print(\"Invalid input format!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Check if all values are integers\n",
        "while h > 0 or w > 0:\n",
        "    try:\n",
        "        h = int(input_file.readline())\n",
        "    except ValueError:\n",
        "        print(\"Invalid input format!\")\n",
        "        sys.exit(1)\n",
        "    try:\n",
        "        w = int(input_file.read"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ff9b79d",
      "metadata": {
        "id": "9ff9b79d"
      },
      "source": [
        "The generated code attempts to read input from a file using sys.argv, but it is incorrect for the proposed problem. It closes the file before finishing reading, and contains incomplete and invalid chunks. And it does not solve the problem correctly or print the expected output.\n",
        "\n",
        "However, we can see that the model is already generating Python code with a reasonable structure, including basic input handling, error checking, and use of standard libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63fd62c7",
      "metadata": {
        "id": "63fd62c7"
      },
      "source": [
        "Now we see if the model performs better with the one-shot prompt approach. In this cell we generate the one shot prompt using also a random example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "914c751d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "914c751d",
        "outputId": "eff3cd4b-4267-4146-8b8d-2f7b10858442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.'}, {'role': 'user', 'content': '### Problem:\\nYou are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\\n\\nAt least how many operations are necessary to satisfy the following conditions?\\n\\n* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\\n* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\\n\\nConstraints\\n\\n* 2 ≤ n ≤ 10^5\\n* |a_i| ≤ 10^9\\n* Each a_i is an integer.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nn\\na_1 a_2 ... a_n\\n\\n\\nOutput\\n\\nPrint the minimum necessary count of operations.\\n### Solution Expected:'}, {'role': 'assistant', 'content': 'N = int(input())\\na = list(map(int, input().split()))\\n\\nans1 = 0\\ns = 0\\nflg = 1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans1 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nans2 = 0\\ns = 0\\nflg = -1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans2 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nprint(min(ans1, ans2))'}, {'role': 'user', 'content': 'Now please solve the problem below by writing the corresponding Python code.\\n\\n### Problem:\\nThere are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n### Your Solution:'}]\n"
          ]
        }
      ],
      "source": [
        "one_shot_messages = build_messages(df, example.index[0], mode='one-shot')\n",
        "print(one_shot_messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c966872",
      "metadata": {
        "id": "3c966872"
      },
      "source": [
        "And now we generate the output for this prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8a2503",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f8a2503",
        "outputId": "9194e906-c17b-4ac2-ac74-504607a92c84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.</s>\n",
            "<|user|>\n",
            "### Problem:\n",
            "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "### Solution Expected:</s>\n",
            "<|assistant|>\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))</s>\n",
            "<|user|>\n",
            "Now please solve the problem below by writing the corresponding Python code.\n",
            "\n",
            "### Problem:\n",
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "### Your Solution:</s>\n",
            "<|assistant|>\n",
            "N = int(input())\n",
            "h = int(input())\n",
            "w = int(input())\n",
            "\n",
            "if h * w <= 0:\n",
            "    print(1)\n",
            "else:\n",
            "    cells_remaining = 0\n",
            "    for row in range(h):\n",
            "        for col in range(w):\n",
            "            if row * w + col <= h * w:\n",
            "                cells_remaining += 1\n",
            "    print(cells_remaining)\n",
            "```\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The problem is to find the number of white cells that will remain in a grid with `h` rows and `w` columns, where each row or column contains cells of size `h`x`w`. The `h` and `w` values are obtained from input, and the cells_remaining variable is initialized to 0.\n",
            "\n",
            "The cells_remaining variable is iterated over each row and column. For each row, we check if the number of white cells is still less than or equal to the product of `h` and `w`. If this condition is met, we increment cells_remaining by 1. If this condition is not met, we do not increment cells_remaining and\n"
          ]
        }
      ],
      "source": [
        "one_shot_generated = generate_solution(one_shot_messages)\n",
        "print(one_shot_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c46f888",
      "metadata": {
        "id": "9c46f888"
      },
      "source": [
        "Now the code generated was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2939bf71",
      "metadata": {
        "id": "2939bf71"
      },
      "outputs": [],
      "source": [
        "N = int(input())\n",
        "h = int(input())\n",
        "w = int(input())\n",
        "\n",
        "if h * w <= 0:\n",
        "    print(1)\n",
        "else:\n",
        "    cells_remaining = 0\n",
        "    for row in range(h):\n",
        "        for col in range(w):\n",
        "            if row * w + col <= h * w:\n",
        "                cells_remaining += 1\n",
        "    print(cells_remaining)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11187750",
      "metadata": {
        "id": "11187750"
      },
      "source": [
        "Which still has a well structured python code but logically doesn't solve the proposed problem. Now we can also see that after the code the model tried to explain the logic behind it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06532c21",
      "metadata": {
        "id": "06532c21"
      },
      "source": [
        "The last approach if the few-shot, giving the model two examples instead of just one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c501d1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c501d1c",
        "outputId": "8398aa04-1739-4dec-9791-a6c305ca67f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': 'You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.'}, {'role': 'user', 'content': '### Problem:\\nYou are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\\n\\nAt least how many operations are necessary to satisfy the following conditions?\\n\\n* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\\n* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\\n\\nConstraints\\n\\n* 2 ≤ n ≤ 10^5\\n* |a_i| ≤ 10^9\\n* Each a_i is an integer.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nn\\na_1 a_2 ... a_n\\n\\n\\nOutput\\n\\nPrint the minimum necessary count of operations.\\n### Solution Expected:'}, {'role': 'assistant', 'content': 'N = int(input())\\na = list(map(int, input().split()))\\n\\nans1 = 0\\ns = 0\\nflg = 1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans1 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nans2 = 0\\ns = 0\\nflg = -1\\nfor ai in a:\\n    s += ai\\n    if s * flg <= 0:\\n        ans2 += abs(s) + 1\\n        s = flg\\n    flg *= -1\\n\\nprint(min(ans1, ans2))'}, {'role': 'user', 'content': '### Problem:\\nIn some other world, today is the day before Christmas Eve.\\n\\nMr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\\\leq i \\\\leq N) is p_i yen (the currency of Japan).\\n\\nHe has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\\n\\nConstraints\\n\\n* 2 \\\\leq N \\\\leq 10\\n* 100 \\\\leq p_i \\\\leq 10000\\n* p_i is an even number.\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nN\\np_1\\np_2\\n:\\np_N\\n\\n\\nOutput\\n\\nPrint the total amount Mr. Takaha will pay.\\n### Solution Expected:'}, {'role': 'assistant', 'content': 'N=int(input())\\nP=sorted([int(input()) for _ in range(N)])\\nprint(sum(P)-P[-1]//2)'}, {'role': 'user', 'content': 'Now please solve the problem below by writing the corresponding Python code.\\n\\n### Problem:\\nThere are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n### Your Solution:'}]\n"
          ]
        }
      ],
      "source": [
        "few_shot_messages = build_messages(df, example.index[0], mode='few-shot')\n",
        "print(few_shot_messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56f173f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56f173f6",
        "outputId": "793ce501-0a13-4c38-97cb-05b7ea108fe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are an assistent who helps solve programming problems by writing the corresponding Python code. Please respond with the code only, without explanation.</s>\n",
            "<|user|>\n",
            "### Problem:\n",
            "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "### Solution Expected:</s>\n",
            "<|assistant|>\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))</s>\n",
            "<|user|>\n",
            "### Problem:\n",
            "In some other world, today is the day before Christmas Eve.\n",
            "\n",
            "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
            "\n",
            "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 \\leq N \\leq 10\n",
            "* 100 \\leq p_i \\leq 10000\n",
            "* p_i is an even number.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "N\n",
            "p_1\n",
            "p_2\n",
            ":\n",
            "p_N\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the total amount Mr. Takaha will pay.\n",
            "### Solution Expected:</s>\n",
            "<|assistant|>\n",
            "N=int(input())\n",
            "P=sorted([int(input()) for _ in range(N)])\n",
            "print(sum(P)-P[-1]//2)</s>\n",
            "<|user|>\n",
            "Now please solve the problem below by writing the corresponding Python code.\n",
            "\n",
            "### Problem:\n",
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "### Your Solution:</s>\n",
            "<|assistant|>\n",
            "N=int(input())\n",
            "h=int(input()[0])\n",
            "w=int(input()[1])\n",
            "\n",
            "# Find the number of white cells in H rows and W columns\n",
            "rows_num=0\n",
            "for I in range(h):\n",
            "    for j in range(w):\n",
            "        if input()[i*w+j] == 'w':\n",
            "            rows_num+=1\n",
            "\n",
            "# Calculate the number of white cells in the remaining H rows and W columns\n",
            "remaining_rows_num=0\n",
            "for I in range(h-1, -1, -1):\n",
            "    for j in range(w):\n",
            "        if input()[i*w+j] == 'w':\n",
            "            remaining_rows_num+=1\n",
            "\n",
            "# Calculate the number of white cells remaining in H rows and W columns\n",
            "remaining_cells_num=rows_num-remaining_rows_num\n",
            "\n",
            "print(remaining_cells_num)\n",
            "\n",
            "# Example:\n",
            "# H = 3, W = 4\n",
            "# h w\n",
            "# 1 0 0 1 0 0 0\n",
            "# 0 1 1\n"
          ]
        }
      ],
      "source": [
        "few_shot_generated = generate_solution(few_shot_messages)\n",
        "print(few_shot_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28c339fb",
      "metadata": {
        "id": "28c339fb"
      },
      "source": [
        "The code generated this time was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0011fca4",
      "metadata": {
        "id": "0011fca4"
      },
      "outputs": [],
      "source": [
        "N=int(input())\n",
        "h=int(input()[0])\n",
        "w=int(input()[1])\n",
        "\n",
        "# Find the number of white cells in H rows and W columns\n",
        "rows_num=0\n",
        "for I in range(h):\n",
        "    for j in range(w):\n",
        "        if input()[i*w+j] == 'w':\n",
        "            rows_num+=1\n",
        "\n",
        "# Calculate the number of white cells in the remaining H rows and W columns\n",
        "remaining_rows_num=0\n",
        "for I in range(h-1, -1, -1):\n",
        "    for j in range(w):\n",
        "        if input()[i*w+j] == 'w':\n",
        "            remaining_rows_num+=1\n",
        "\n",
        "# Calculate the number of white cells remaining in H rows and W columns\n",
        "remaining_cells_num=rows_num-remaining_rows_num\n",
        "\n",
        "print(remaining_cells_num)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6bcfde",
      "metadata": {
        "id": "1c6bcfde"
      },
      "source": [
        "The few-shot output better aligns with the grid structure of the problem and introduces more descriptive variable names. However, it still misinterprets the input format and fails to implement the correct logic."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36d53768",
      "metadata": {
        "id": "36d53768"
      },
      "source": [
        "Now we will run the three prompting approaches with a few samples to get some metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dd12947",
      "metadata": {
        "id": "2dd12947"
      },
      "outputs": [],
      "source": [
        "results = {'zero-shot': [], 'one-shot': [], 'few-shot': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b49365a",
      "metadata": {
        "id": "9b49365a"
      },
      "outputs": [],
      "source": [
        "sample_indices = df.sample(5, random_state=0).index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f681776e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f681776e",
        "outputId": "d0743cb9-d299-4d39-bd2c-ecc7b9cbadcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode: zero-shot, Example 219235\n",
            "bleu: 1.1744\n",
            "rouge1: 0.0337\n",
            "rougeL: 0.0337\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 116625\n",
            "bleu: 4.5137\n",
            "rouge1: 0.1049\n",
            "rougeL: 0.0918\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 161440\n",
            "bleu: 1.8577\n",
            "rouge1: 0.1154\n",
            "rougeL: 0.0726\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 177012\n",
            "bleu: 8.3524\n",
            "rouge1: 0.2725\n",
            "rougeL: 0.1755\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 26857\n",
            "bleu: 7.2144\n",
            "rouge1: 0.2652\n",
            "rougeL: 0.1215\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 219235\n",
            "bleu: 1.2996\n",
            "rouge1: 0.0182\n",
            "rougeL: 0.0182\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode: one-shot, Example 116625\n",
            "bleu: 2.4819\n",
            "rouge1: 0.0784\n",
            "rougeL: 0.0610\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 161440\n",
            "bleu: 1.9589\n",
            "rouge1: 0.1223\n",
            "rougeL: 0.0699\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 177012\n",
            "bleu: 5.9250\n",
            "rouge1: 0.2358\n",
            "rougeL: 0.1440\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 26857\n",
            "bleu: 4.6816\n",
            "rouge1: 0.2055\n",
            "rougeL: 0.1054\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 219235\n",
            "bleu: 0.6755\n",
            "rouge1: 0.0095\n",
            "rougeL: 0.0095\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (2048). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode: few-shot, Example 116625\n",
            "bleu: 1.6039\n",
            "rouge1: 0.0285\n",
            "rougeL: 0.0268\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 161440\n",
            "bleu: 1.8570\n",
            "rouge1: 0.0734\n",
            "rougeL: 0.0500\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 177012\n",
            "bleu: 3.7599\n",
            "rouge1: 0.1353\n",
            "rougeL: 0.0844\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 26857\n",
            "bleu: 4.0815\n",
            "rouge1: 0.1560\n",
            "rougeL: 0.0705\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "saved_results = []\n",
        "\n",
        "for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
        "    for idx in sample_indices:\n",
        "        messages = build_messages(df, idx, mode=mode, num_few_shot=3)\n",
        "        generated = generate_solution(messages, max_new_tokens=256)\n",
        "\n",
        "        expected = df.loc[idx, 'solution_code']\n",
        "\n",
        "        metrics = evaluate_metrics(expected, generated)\n",
        "\n",
        "        print(f\"Mode: {mode}, Example {idx}\")\n",
        "        #print(\"Generated:\\n\", generated)\n",
        "        #print(\"Expected:\\n\", expected)\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        results[mode].append(metrics)\n",
        "\n",
        "        saved_results.append({\n",
        "            'mode': mode,\n",
        "            'index': idx,\n",
        "            'problem_description': df.loc[idx, 'problem_description'],\n",
        "            'expected_solution': expected,\n",
        "            'generated_solution': generated,\n",
        "            'bleu': metrics['bleu'],\n",
        "            'rouge1': metrics['rouge1'],\n",
        "            'rougeL': metrics['rougeL'],\n",
        "            'exact_match': metrics['exact_match']\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45a3bf17",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45a3bf17",
        "outputId": "7e53205f-6152-4fc7-b916-fab41ce80fba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average metrics for zero-shot:\n",
            "bleu: 4.6225\n",
            "rouge1: 0.1583\n",
            "rougeL: 0.0990\n",
            "exact_match: 0.0000\n",
            "\n",
            "Average metrics for one-shot:\n",
            "bleu: 3.2694\n",
            "rouge1: 0.1321\n",
            "rougeL: 0.0797\n",
            "exact_match: 0.0000\n",
            "\n",
            "Average metrics for few-shot:\n",
            "bleu: 2.3955\n",
            "rouge1: 0.0805\n",
            "rougeL: 0.0482\n",
            "exact_match: 0.0000\n"
          ]
        }
      ],
      "source": [
        "for mode in results:\n",
        "    avg_metrics = {}\n",
        "    for metric in results[mode][0].keys():\n",
        "        avg_metrics[metric] = sum([ex[metric] for ex in results[mode]]) / len(results[mode])\n",
        "\n",
        "    print(f\"\\nAverage metrics for {mode}:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2488fc1b",
      "metadata": {
        "id": "2488fc1b"
      },
      "source": [
        "These results show that the zero-shot setting outperforms one-shot and few-shot in all metrics, including BLEU, ROUGE-1, ROUGE-L, and exact match (which is zero across the board). This suggests that providing examples (one-shot or few-shot) did not help the model improve code generation quality for this task, and may have introduced noise or confusion, leading to worse performance.\n",
        "This can be caused by poorly formatted prompts or overly long or complex example prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4595bd38",
      "metadata": {
        "id": "4595bd38"
      },
      "outputs": [],
      "source": [
        "df_results = pd.DataFrame(saved_results)\n",
        "df_results.to_csv('generated_codes.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd949b07",
      "metadata": {
        "id": "dd949b07"
      },
      "outputs": [],
      "source": [
        "with open('generated_codes.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(saved_results, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "518546be",
      "metadata": {
        "id": "518546be"
      },
      "source": [
        "## Try deepseek-coder on the task\n",
        "\n",
        "DeepSeek-Coder-1.3B-Base is a 1.3 billion parameter language model specialized in code generation and understanding. It is trained on a diverse set of programming languages and designed to assist with tasks such as code completion, bug fixing, and problem-solving by generating syntactically correct and contextually relevant code snippets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38eb4f99",
      "metadata": {
        "id": "38eb4f99"
      },
      "source": [
        "We start by loading the model and the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b48098d7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "87599d7f2b264aab92680d59a0aa654b",
            "0f0b0db0eeac4212bc8a7b7fd395bbd3",
            "0ec7212dd1964ec4a2a6eec898fc18fc",
            "bae19b053bf3421ea2970d0137e66e3d",
            "000f135d3d5a451b9482e1f40a9a1e52",
            "3bfa3a0f137142b287ab40f49fbc7b60",
            "8ede59f33e4d4484b41929af233f01ab",
            "b24344d21f5f48b083d6caee1f3dd286",
            "1a4b1fe1c40c4e239ba870737d873f30",
            "d2f772e8736641819a04f57bd04dc1ef",
            "1582d0dc83cd4733b2124c7a9e640da6",
            "7d11f10d218c4b59a41d1db3d3450270",
            "5b475b0cf8264c21945726f6ee231a22",
            "886e9b0e92bc4bb49c6e7ab12aeec924",
            "aecad5b4c97b42f6b92beae8a5bbc446",
            "81168d91180348af95dcdd077d9ef227",
            "37fb19a974684cfa8f51186bccaac0e1",
            "75ae0cde7fce4d3c944f9a491d5e1652",
            "07cf273c8dd7466ea5d0dc25ad6c59ed",
            "79fad8a7322f472484c08f32af219993",
            "9637946ff9fd49a1a52aed5b1075f40e",
            "b1b750d4a12548a2861b7c5feb520512",
            "8d0e18d30d7e46bd81365c8a1680b97f",
            "3caecd3b9b6a4be7968b7ab6f2289332",
            "4f68886ba4804855ac349de3ee4ffd8d",
            "e192780ec4164bd6ae7d3273af88f510",
            "cd4d7cf77d554e0e81e2e09c75604fc0",
            "1d68cf4fb0a54f3f829430a7d1af2442",
            "667fa3ad502748f6bcff359bf473bc83",
            "1fde9bb4cda74c069522d3dca927720e",
            "ef51797d549445df82f5e37e1b33d25e",
            "c4173a0a08d14565b96943ee428c3c37",
            "0b34ed4c30a84f23821d4d6b099d3983",
            "0bbbee9109044bfdabfbc909fc3eb24c",
            "dca27add69b74eed92f435770a701cfd",
            "85561c08f85547d881fe636d916800e3",
            "2e6ca48621884e4283fc0e3a9b2d6293",
            "dda0304e45f14ed79fc72199fa3513b2",
            "14224e6531984e3fbe6482b7f65f98cf",
            "9c470bb938414ce18fb47c19feffc59f",
            "3eb2348ddcce4d459963f134ee308032",
            "25d6a1178ca0412198b6ae760226fe96",
            "0dd55454388a4852b757cb80917d39ba",
            "8edd819f814340b5a22cbb9d559f9896",
            "d5dad8717a5e492f8bcb542bac04723d",
            "5f073c292e3f48aab29dad48b4a293e3",
            "e2af7acb196243dc976de9146f7ac868",
            "1747779592844a848d7759664d88f3ce",
            "2247952a42874a8c8183d2d209b498a1",
            "d1126f14a72d42f8bda77b0fd19828a7",
            "6e6f0ccac09b492f891a960e7a3b33d0",
            "06d8d7e7f5734ef48e29fb2a9af1beb0",
            "1c8b100829bf4eda89b895e490688993",
            "ca575a4266d847198aff57b17d99bc26",
            "0af1b918f3524cc69b82717487dc7c7f",
            "16348bcb39e24f3bba1f0e4bdf8d9ffd",
            "1c25b326bc5d4ad5b0bf006c6096d805",
            "aacc7fe045924d1fadf1b513ca958420",
            "df458098f26c4567a0149bc975940726",
            "c13b0ad94dc64b01aa3a5bcc1365b3f5",
            "c43cd2c2221b4c07a3aeeabdc56a8b62",
            "507dc04548aa4286924b17ec4087a7f1",
            "4c2e17bd10584a388b13cd943ac3b173",
            "2645b7da0c0e47bfb18cc7f4dcc9ed53",
            "692ef209a3804b468a96ed62bdfa3351",
            "11b0868a202f40459a1241ecf983432b",
            "13c69e0e2076443ab5b3905d10bbf98b",
            "065ea327a9f540158fcddf2fc507491b",
            "7471d77221a84d818e26f61dd857402a",
            "df7fc469c3bf4e58b587f7cffa55dcea",
            "55c97d7ac752474bab9cda64b7347aa3",
            "acd8a01699b940dfa6f51428030e6d67",
            "1a09564f32c74f7480fb8e16871a78a7",
            "96bdd6e9d05f4e4e89bdf245695b052d",
            "1ac919024bbe44b5b7e3511e7f4f83b6",
            "1fd38e62726f43a1af4fc19b2fbeb8a0",
            "e2af7715a451412b9cceea20df674b16"
          ]
        },
        "id": "b48098d7",
        "outputId": "691ff322-69fc-40d9-c2ce-68d7ae23cecc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87599d7f2b264aab92680d59a0aa654b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d11f10d218c4b59a41d1db3d3450270",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d0e18d30d7e46bd81365c8a1680b97f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0bbbee9109044bfdabfbc909fc3eb24c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/631 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5dad8717a5e492f8bcb542bac04723d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16348bcb39e24f3bba1f0e4bdf8d9ffd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.69G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "13c69e0e2076443ab5b3905d10bbf98b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-1.3b-base\", trust_remote_code=True).cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2eabb76e",
      "metadata": {
        "id": "2eabb76e"
      },
      "source": [
        "This function is used to build the prompts in each of the approaches. It tries to give a well structured prompt with clearly separated examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6f794f4",
      "metadata": {
        "id": "c6f794f4"
      },
      "outputs": [],
      "source": [
        "def build_prompt_deepseek(df, index, mode='zero-shot', num_few_shot=3, random_state=0):\n",
        "    problem = clean_problem_description(df.loc[index, 'problem_description'])\n",
        "    if mode == 'zero-shot':\n",
        "        prompt = f\"# Task: {problem}\\n# Solution:\\n\"\n",
        "\n",
        "    elif mode == 'one-shot':\n",
        "        example_idx = df.drop(index).sample(1, random_state=random_state).index[0]\n",
        "        example_problem = clean_problem_description(df.loc[example_idx, 'problem_description'])\n",
        "        example_solution = df.loc[example_idx, 'solution_code']\n",
        "        prompt = (\n",
        "            f\"# === Example 1 ===\\n\"\n",
        "            f\"# Task: {example_problem}\\n# Solution:\\n{example_solution}\\n\\n\"\n",
        "            f\"# Task: {problem}\\n# Solution:\\n\"\n",
        "        )\n",
        "\n",
        "    elif mode == 'few-shot':\n",
        "        example_indices = df.drop(index).sample(num_few_shot, random_state=random_state).index\n",
        "        prompt = \"\"\n",
        "        for j, i in enumerate(example_indices):\n",
        "            example_problem = clean_problem_description(df.loc[i, 'problem_description'])\n",
        "            example_solution = df.loc[i, 'solution_code']\n",
        "            prompt += (\n",
        "                f\"# === Example {j+1} ===\\n\"\n",
        "                f\"# Task: {example_problem}\\n\"\n",
        "                f\"# Solution:\\n{example_solution}\\n\\n\"\n",
        "            )\n",
        "        prompt += f\"# Task: {problem}\\n# Solution:\\n\"\n",
        "\n",
        "    return prompt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61e5c64f",
      "metadata": {
        "id": "61e5c64f"
      },
      "source": [
        "This function is used to generate the model output using the prompt build before and returns the string containing the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c2d5981",
      "metadata": {
        "id": "7c2d5981"
      },
      "outputs": [],
      "source": [
        "def generate_solution_deepseek(prompt, max_new_tokens=512):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "410e2f54",
      "metadata": {
        "id": "410e2f54"
      },
      "source": [
        "We will use the same problem example as before for demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1119524",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1119524",
        "outputId": "e85e4f64-871b-4a46-ed39-299b59819b98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(14480)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a164ba12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "a164ba12",
        "outputId": "093ae257-3f0b-4bc5-cc7b-23acf6cc0fb2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"example\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"problem_description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"There are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n\\nExamples\\n\\nInput\\n\\n3 2\\n2 1\\n\\n\\nOutput\\n\\n1\\n\\n\\nInput\\n\\n5 5\\n2 3\\n\\n\\nOutput\\n\\n6\\n\\n\\nInput\\n\\n2 4\\n2 4\\n\\n\\nOutput\\n\\n0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"solution_id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"190_3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"solution_code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"a,b=map(int,input().split())\\nc,d=map(int,input().split())\\nprint((a-c)*(b-d))\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"problem_name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"p03101 AtCoder Beginner Contest 121 - White Cells\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"time_complexity_inferred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"O(1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"space_complexity_inferred\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"O(1)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "example"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-b40aa2f5-312a-480f-b2ab-2b28faa6576c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>problem_description</th>\n",
              "      <th>solution_id</th>\n",
              "      <th>solution_code</th>\n",
              "      <th>problem_name</th>\n",
              "      <th>time_complexity_inferred</th>\n",
              "      <th>space_complexity_inferred</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>14480</th>\n",
              "      <td>There are H rows and W columns of white square...</td>\n",
              "      <td>190_3</td>\n",
              "      <td>a,b=map(int,input().split())\\nc,d=map(int,inpu...</td>\n",
              "      <td>p03101 AtCoder Beginner Contest 121 - White Cells</td>\n",
              "      <td>O(1)</td>\n",
              "      <td>O(1)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b40aa2f5-312a-480f-b2ab-2b28faa6576c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b40aa2f5-312a-480f-b2ab-2b28faa6576c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b40aa2f5-312a-480f-b2ab-2b28faa6576c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_6302cffe-d1ae-4ec1-ac1e-4b2154f0db47\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('example')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6302cffe-d1ae-4ec1-ac1e-4b2154f0db47 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('example');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                     problem_description solution_id  \\\n",
              "14480  There are H rows and W columns of white square...       190_3   \n",
              "\n",
              "                                           solution_code  \\\n",
              "14480  a,b=map(int,input().split())\\nc,d=map(int,inpu...   \n",
              "\n",
              "                                            problem_name  \\\n",
              "14480  p03101 AtCoder Beginner Contest 121 - White Cells   \n",
              "\n",
              "      time_complexity_inferred space_complexity_inferred  \n",
              "14480                     O(1)                      O(1)  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab25342",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6ab25342",
        "outputId": "e6ca4146-f6a7-4913-ab18-8856ef8e7794"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Task: There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "# Solution:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "zero_shot_prompt = build_prompt_deepseek(df, example.index[0], mode='zero-shot')\n",
        "print(zero_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a310f686",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a310f686",
        "outputId": "6888c60c-9347-43b2-a95d-fbcaabe37bbc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Task: There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "# Solution:\n",
            "\n",
            "\n",
            "def white_cells(H, W, h, w):\n",
            "    # Write your code here\n",
            "    return (H - h) * (W - w)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    H, W = map(int, input().split())\n",
            "    h, w = map(int, input().split())\n",
            "    print(white_cells(H, W, h, w))\n"
          ]
        }
      ],
      "source": [
        "zero_shot_generated = generate_solution_deepseek(zero_shot_prompt)\n",
        "print(zero_shot_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e23bafb",
      "metadata": {
        "id": "2e23bafb"
      },
      "source": [
        "The code generated by the zero-shot approach was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72034771",
      "metadata": {
        "id": "72034771"
      },
      "outputs": [],
      "source": [
        "def white_cells(H, W, h, w):\n",
        "    # Write your code here\n",
        "    return (H - h) * (W - w)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    H, W = map(int, input().split())\n",
        "    h, w = map(int, input().split())\n",
        "    print(white_cells(H, W, h, w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "134fbb3e",
      "metadata": {
        "id": "134fbb3e"
      },
      "source": [
        "Which correctly solves the problem proposed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "704dbd2e",
      "metadata": {
        "id": "704dbd2e"
      },
      "source": [
        "TinyLlama, being a general-purpose language model with fewer parameters and less specialized training on code, may struggle with understanding problem constraints and generating correct, concise solutions. In contrast, DeepSeek-Coder is specifically trained on diverse programming tasks and code generation, enabling it to better grasp problem requirements and produce accurate, efficient code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a81901",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "e0a81901",
        "outputId": "7ee7878f-9c89-41ee-e9d1-fe7b142204b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# === Example 1 ===\n",
            "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "# Solution:\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))\n",
            "\n",
            "# Task: There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "# Solution:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "one_shot_prompt = build_prompt_deepseek(df, example.index[0], mode='one-shot')\n",
        "print(one_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d9f962c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d9f962c",
        "outputId": "4426d809-43c0-49f2-e46b-3bfe0e0c12f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# === Example 1 ===\n",
            "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "# Solution:\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))\n",
            "\n",
            "# Task: There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "# Solution:\n",
            "H, W = map(int, input().split())\n",
            "h, w = map(int, input().split())\n",
            "print((H - h + 1) * (W - w + 1) - len(set(input().split())))\n"
          ]
        }
      ],
      "source": [
        "one_shot_generated = generate_solution_deepseek(one_shot_prompt)\n",
        "print(one_shot_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7670377a",
      "metadata": {
        "id": "7670377a"
      },
      "source": [
        "The solution generated by the one-shot prompt was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6303e59",
      "metadata": {
        "id": "d6303e59"
      },
      "outputs": [],
      "source": [
        "# Solution:\n",
        "H, W = map(int, input().split())\n",
        "h, w = map(int, input().split())\n",
        "print((H - h + 1) * (W - w + 1) - len(set(input().split())))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a56dc6d5",
      "metadata": {
        "id": "a56dc6d5"
      },
      "source": [
        "The generated solution overcomplicates the problem by using an incorrect formula and expecting extra input that is not required. This likely happened because the one-shot example used in the prompt added noise or confusion, leading the model to misinterpret the problem’s requirements and produce a more complex, incorrect solution than necessary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a0dde48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1a0dde48",
        "outputId": "42585ff4-648c-40f7-8506-4b583d541c0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# === Example 1 ===\n",
            "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "# Solution:\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))\n",
            "\n",
            "# === Example 2 ===\n",
            "# Task: In some other world, today is the day before Christmas Eve.\n",
            "\n",
            "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
            "\n",
            "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 \\leq N \\leq 10\n",
            "* 100 \\leq p_i \\leq 10000\n",
            "* p_i is an even number.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "N\n",
            "p_1\n",
            "p_2\n",
            ":\n",
            "p_N\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the total amount Mr. Takaha will pay.\n",
            "# Solution:\n",
            "N=int(input())\n",
            "P=sorted([int(input()) for _ in range(N)])\n",
            "print(sum(P)-P[-1]//2)\n",
            "\n",
            "# === Example 3 ===\n",
            "# Task: Two polar bears Menshykov and Uslada from the St.Petersburg zoo and elephant Horace from the Kiev zoo got six sticks to play with and assess the animals' creativity. Menshykov, Uslada and Horace decided to make either an elephant or a bear from those sticks. They can make an animal from sticks in the following way: \n",
            "\n",
            "  * Four sticks represent the animal's legs, these sticks should have the same length. \n",
            "  * Two remaining sticks represent the animal's head and body. The bear's head stick must be shorter than the body stick. The elephant, however, has a long trunk, so his head stick must be as long as the body stick. Note that there are no limits on the relations between the leg sticks and the head and body sticks. \n",
            "\n",
            "\n",
            "\n",
            "Your task is to find out which animal can be made from the given stick set. The zoo keeper wants the sticks back after the game, so they must never be broken, even bears understand it.\n",
            "\n",
            "Input\n",
            "\n",
            "The single line contains six space-separated integers li (1 ≤ li ≤ 9) — the lengths of the six sticks. It is guaranteed that the input is such that you cannot make both animals from the sticks.\n",
            "\n",
            "Output\n",
            "\n",
            "If you can make a bear from the given set, print string \"Bear\" (without the quotes). If you can make an elephant, print string \"Elephant\" (wıthout the quotes). If you can make neither a bear nor an elephant, print string \"Alien\" (without the quotes).\n",
            "# Solution:\n",
            "import sys\n",
            "import math\n",
            "import itertools\n",
            "import collections\n",
            "\n",
            "\n",
            "def divs(n, start=1):\n",
            "    r = []\n",
            "    for i in range(start, int(math.sqrt(n) + 1)):\n",
            "        if (n % i == 0):\n",
            "            if (n / i == i):\n",
            "                r.append(i)\n",
            "            else:\n",
            "                r.extend([i, n // i])\n",
            "    return r\n",
            "def cdiv(n, k): return n // k + (n % k != 0)\n",
            "def ii(): return int(input())\n",
            "def mi(): return map(int, input().split())\n",
            "def li(): return list(map(int, input().split()))\n",
            "def lcm(a, b): return abs(a * b) // math.gcd(a, b)\n",
            "def wr(arr): return ' '.join(map(str, arr))\n",
            "def revn(n): return int(str(n)[::-1])\n",
            "def prime(n):\n",
            "    if n == 2: return True\n",
            "    if n % 2 == 0 or n <= 1: return False\n",
            "    sqr = int(math.sqrt(n)) + 1\n",
            "    for d in range(3, sqr, 2):\n",
            "        if n % d == 0: return False\n",
            "    return True\n",
            "def convn(number, base=3):\n",
            "    newnumber = ''\n",
            "    while number > 0:\n",
            "        newnumber = str(number % base) + newnumber\n",
            "        number //= base\n",
            "    return newnumber\n",
            "\n",
            "\n",
            "l = li()\n",
            "lc = collections.Counter(l)\n",
            "bear = [[4, 1, 1], [5, 1], [1, 5], [1, 4, 1], [1, 1, 4]]\n",
            "elephant = [[6], [4, 2], [2, 4]]\n",
            "if list(lc.values()) in bear:\n",
            "    print('Bear')\n",
            "elif list(lc.values()) in elephant:\n",
            "    print('Elephant')\n",
            "else:\n",
            "    print('Alien')\n",
            "\n",
            "# Task: There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "# Solution:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = build_prompt_deepseek(df, example.index[0], mode='few-shot')\n",
        "print(few_shot_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff00d936",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff00d936",
        "outputId": "2d5cb0ac-b28f-4e9e-efdb-2bef958f2bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# === Example 1 ===\n",
            "# Task: You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "# Solution:\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))\n",
            "\n",
            "# === Example 2 ===\n",
            "# Task: In some other world, today is the day before Christmas Eve.\n",
            "\n",
            "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
            "\n",
            "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 \\leq N \\leq 10\n",
            "* 100 \\leq p_i \\leq 10000\n",
            "* p_i is an even number.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "N\n",
            "p_1\n",
            "p_2\n",
            ":\n",
            "p_N\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the total amount Mr. Takaha will pay.\n",
            "# Solution:\n",
            "N=int(input())\n",
            "P=sorted([int(input()) for _ in range(N)])\n",
            "print(sum(P)-P[-1]//2)\n",
            "\n",
            "# === Example 3 ===\n",
            "# Task: Two polar bears Menshykov and Uslada from the St.Petersburg zoo and elephant Horace from the Kiev zoo got six sticks to play with and assess the animals' creativity. Menshykov, Uslada and Horace decided to make either an elephant or a bear from those sticks. They can make an animal from sticks in the following way: \n",
            "\n",
            "  * Four sticks represent the animal's legs, these sticks should have the same length. \n",
            "  * Two remaining sticks represent the animal's head and body. The bear's head stick must be shorter than the body stick. The elephant, however, has a long trunk, so his head stick must be as long as the body stick. Note that there are no limits on the relations between the leg sticks and the head and body sticks. \n",
            "\n",
            "\n",
            "\n",
            "Your task is to find out which animal can be made from the given stick set. The zoo keeper wants the sticks back after the game, so they must never be broken, even bears understand it.\n",
            "\n",
            "Input\n",
            "\n",
            "The single line contains six space-separated integers li (1 ≤ li ≤ 9) — the lengths of the six sticks. It is guaranteed that the input is such that you cannot make both animals from the sticks.\n",
            "\n",
            "Output\n",
            "\n",
            "If you can make a bear from the given set, print string \"Bear\" (without the quotes). If you can make an elephant, print string \"Elephant\" (wıthout the quotes). If you can make neither a bear nor an elephant, print string \"Alien\" (without the quotes).\n",
            "# Solution:\n",
            "import sys\n",
            "import math\n",
            "import itertools\n",
            "import collections\n",
            "\n",
            "\n",
            "def divs(n, start=1):\n",
            "    r = []\n",
            "    for i in range(start, int(math.sqrt(n) + 1)):\n",
            "        if (n % i == 0):\n",
            "            if (n / i == i):\n",
            "                r.append(i)\n",
            "            else:\n",
            "                r.extend([i, n // i])\n",
            "    return r\n",
            "def cdiv(n, k): return n // k + (n % k != 0)\n",
            "def ii(): return int(input())\n",
            "def mi(): return map(int, input().split())\n",
            "def li(): return list(map(int, input().split()))\n",
            "def lcm(a, b): return abs(a * b) // math.gcd(a, b)\n",
            "def wr(arr): return ' '.join(map(str, arr))\n",
            "def revn(n): return int(str(n)[::-1])\n",
            "def prime(n):\n",
            "    if n == 2: return True\n",
            "    if n % 2 == 0 or n <= 1: return False\n",
            "    sqr = int(math.sqrt(n)) + 1\n",
            "    for d in range(3, sqr, 2):\n",
            "        if n % d == 0: return False\n",
            "    return True\n",
            "def convn(number, base=3):\n",
            "    newnumber = ''\n",
            "    while number > 0:\n",
            "        newnumber = str(number % base) + newnumber\n",
            "        number //= base\n",
            "    return newnumber\n",
            "\n",
            "\n",
            "l = li()\n",
            "lc = collections.Counter(l)\n",
            "bear = [[4, 1, 1], [5, 1], [1, 5], [1, 4, 1], [1, 1, 4]]\n",
            "elephant = [[6], [4, 2], [2, 4]]\n",
            "if list(lc.values()) in bear:\n",
            "    print('Bear')\n",
            "elif list(lc.values()) in elephant:\n",
            "    print('Elephant')\n",
            "else:\n",
            "    print('Alien')\n",
            "\n",
            "# Task: There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "# Solution:\n",
            "H,W=map(int,input().split())\n",
            "h,w=map(int,input().split())\n",
            "print((H-h)*(W-w))\n",
            "\n",
            "# === Example 4 ===\n",
            "# Task: You are given a sequence of N integers.\n",
            "\n",
            "Your task is to find the smallest sub-sequence of these integers, such that it is not possible to split the sequence into two sequences of equal sums.\n",
            "\n",
            "Input\n",
            "\n",
            "The first line contains a single integer N (1 \\leq N \\leq 50000) — the number of elements in the sequence.\n",
            "\n",
            "The second line contains N integers A1, A2, ..., AN (1 \\leq A_i \\leq 10^9) — the elements of the sequence.\n",
            "\n",
            "Output\n",
            "\n",
            "Print the length of the smallest sub-sequence, such that it is not possible to split the sequence into two sequences of equal sums.\n",
            "# Solution:\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "s = sum(a)\n",
            "if s & 1:\n",
            "    print(1)\n",
            "    exit()\n",
            "\n",
            "s //= 2\n",
            "\n",
            "a.sort(reverse=True)\n",
            "\n",
            "for i in range(N):\n",
            "    s -= a[i]\n",
            "    if s <= 0:\n",
            "        print(i + 1)\n",
            "        exit()\n",
            "print(N)\n",
            "\n",
            "# === Example 5 ===\n",
            "# Task: You are given a sequence of N integers.\n",
            "\n",
            "Your task is to find the smallest sub-sequence of these integers, such that it is not possible to split the sequence into two sequences of equal sums.\n",
            "\n",
            "Input\n",
            "\n",
            "The first line contains a single integer N (1 \\leq N \\leq 50000) — the number of elements in the sequence.\n",
            "\n",
            "The second line contains N integers A1, A2, ..., AN (1 \\leq A_i \\leq 10^9) — the elements of the sequence.\n",
            "\n",
            "Output\n",
            "\n",
            "Print the length of the smallest sub-sequence, such that it is not possible to split the sequence into two sequences of equal sums.\n",
            "# Solution:\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "s = sum(a)\n",
            "if s & 1:\n"
          ]
        }
      ],
      "source": [
        "few_shot_generated = generate_solution_deepseek(few_shot_prompt)\n",
        "print(few_shot_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f31a28",
      "metadata": {
        "id": "87f31a28"
      },
      "source": [
        "The code generated by the model was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cb2fb9",
      "metadata": {
        "id": "12cb2fb9"
      },
      "outputs": [],
      "source": [
        "H,W=map(int,input().split())\n",
        "h,w=map(int,input().split())\n",
        "print((H-h)*(W-w))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88ed7357",
      "metadata": {
        "id": "88ed7357"
      },
      "source": [
        "Which correctly solves the problem, and actually is almost the same one as the one provided in the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "338df1c7",
      "metadata": {
        "id": "338df1c7"
      },
      "outputs": [],
      "source": [
        "a,b=map(int,input().split())\n",
        "c,d=map(int,input().split())\n",
        "print((a-c)*(b-d))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bffb8b20",
      "metadata": {
        "id": "bffb8b20"
      },
      "source": [
        "But we can see that after the model continued generating new examples and solution codes, like it was continuing the prompt. Long or confusing examples can cause the model to not recognize where to end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eedcfd4",
      "metadata": {
        "id": "5eedcfd4"
      },
      "outputs": [],
      "source": [
        "results_deepseek = {'zero-shot': [], 'one-shot': [], 'few-shot': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd74482",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddd74482",
        "outputId": "b04bc510-4606-4f45-b0ed-c780c420302b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mode: zero-shot, Example 219235\n",
            "bleu: 3.1388\n",
            "rouge1: 0.0556\n",
            "rougeL: 0.0556\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 116625\n",
            "bleu: 2.3394\n",
            "rouge1: 0.1863\n",
            "rougeL: 0.1242\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 161440\n",
            "bleu: 8.3381\n",
            "rouge1: 0.1355\n",
            "rougeL: 0.1030\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 177012\n",
            "bleu: 11.4481\n",
            "rouge1: 0.2696\n",
            "rougeL: 0.1569\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: zero-shot, Example 26857\n",
            "bleu: 14.0492\n",
            "rouge1: 0.3170\n",
            "rougeL: 0.1445\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 219235\n",
            "bleu: 1.2779\n",
            "rouge1: 0.0173\n",
            "rougeL: 0.0173\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 116625\n",
            "bleu: 2.8054\n",
            "rouge1: 0.0677\n",
            "rougeL: 0.0558\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 161440\n",
            "bleu: 5.9441\n",
            "rouge1: 0.1477\n",
            "rougeL: 0.0937\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 177012\n",
            "bleu: 11.4539\n",
            "rouge1: 0.2831\n",
            "rougeL: 0.1788\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: one-shot, Example 26857\n",
            "bleu: 4.1924\n",
            "rouge1: 0.1988\n",
            "rougeL: 0.0906\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 219235\n",
            "bleu: 0.6392\n",
            "rouge1: 0.0110\n",
            "rougeL: 0.0110\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 116625\n",
            "bleu: 1.7252\n",
            "rouge1: 0.0311\n",
            "rougeL: 0.0293\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 161440\n",
            "bleu: 1.9019\n",
            "rouge1: 0.0753\n",
            "rougeL: 0.0513\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 177012\n",
            "bleu: 5.6718\n",
            "rouge1: 0.1425\n",
            "rougeL: 0.1092\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n",
            "Mode: few-shot, Example 26857\n",
            "bleu: 3.5124\n",
            "rouge1: 0.1543\n",
            "rougeL: 0.0681\n",
            "exact_match: 0.0000\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "saved_results_deepseek = []\n",
        "\n",
        "for mode in ['zero-shot', 'one-shot', 'few-shot']:\n",
        "    for idx in sample_indices:\n",
        "        prompt = build_prompt_deepseek(df, idx, mode=mode, num_few_shot=3)\n",
        "        generated = generate_solution_deepseek(prompt, max_new_tokens=256)\n",
        "\n",
        "        expected = df.loc[idx, 'solution_code']\n",
        "\n",
        "        metrics = evaluate_metrics(expected, generated)\n",
        "\n",
        "        print(f\"Mode: {mode}, Example {idx}\")\n",
        "        #print(\"Generated:\\n\", generated)\n",
        "        #print(\"Expected:\\n\", expected)\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric}: {value:.4f}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "        results_deepseek[mode].append(metrics)\n",
        "\n",
        "        saved_results_deepseek.append({\n",
        "            'mode': mode,\n",
        "            'index': idx,\n",
        "            'problem_description': df.loc[idx, 'problem_description'],\n",
        "            'expected_solution': expected,\n",
        "            'generated_solution': generated,\n",
        "            'bleu': metrics['bleu'],\n",
        "            'rouge1': metrics['rouge1'],\n",
        "            'rougeL': metrics['rougeL'],\n",
        "            'exact_match': metrics['exact_match']\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93295bd4",
      "metadata": {
        "id": "93295bd4"
      },
      "source": [
        "Now we will run the three prompting approaches with a few samples to get some metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd723c00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd723c00",
        "outputId": "93ffb96a-d59a-46c1-de52-17cfded1f1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average metrics for zero-shot:\n",
            "bleu: 7.8627\n",
            "rouge1: 0.1928\n",
            "rougeL: 0.1168\n",
            "exact_match: 0.0000\n",
            "\n",
            "Average metrics for one-shot:\n",
            "bleu: 5.1347\n",
            "rouge1: 0.1429\n",
            "rougeL: 0.0872\n",
            "exact_match: 0.0000\n",
            "\n",
            "Average metrics for few-shot:\n",
            "bleu: 2.6901\n",
            "rouge1: 0.0828\n",
            "rougeL: 0.0537\n",
            "exact_match: 0.0000\n"
          ]
        }
      ],
      "source": [
        "for mode in results_deepseek:\n",
        "    avg_metrics = {}\n",
        "    for metric in results_deepseek[mode][0].keys():\n",
        "        avg_metrics[metric] = sum([ex[metric] for ex in results_deepseek[mode]]) / len(results_deepseek[mode])\n",
        "\n",
        "    print(f\"\\nAverage metrics for {mode}:\")\n",
        "    for metric, value in avg_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ac9424c",
      "metadata": {
        "id": "6ac9424c"
      },
      "source": [
        "Again we have worse metrics for the few-shot approach than to the other two. This can also be caused by the model continuing the generation after the solution, like the example above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a44df4c5",
      "metadata": {
        "id": "a44df4c5"
      },
      "outputs": [],
      "source": [
        "df_results_deepseek = pd.DataFrame(saved_results_deepseek)\n",
        "df_results_deepseek.to_csv('generated_codes_deepseek.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc9083ef",
      "metadata": {
        "id": "dc9083ef"
      },
      "outputs": [],
      "source": [
        "with open('generated_codes_deepseek.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(saved_results_deepseek, f, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a970c6f",
      "metadata": {
        "id": "3a970c6f"
      },
      "source": [
        "## Try Mistral-7B-Instruct-v0.1 on the task\n",
        "\n",
        "Mistral-7B-Instruct-v0.1 is a 7-billion parameter language model designed for instruction-following tasks. It offers strong performance in natural language understanding and generation. Due to its large size, we will use quantization techniques to reduce memory usage and enable efficient inference on limited hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "951c3887",
      "metadata": {
        "id": "951c3887"
      },
      "source": [
        "This code configures 4-bit quantization for loading a model with the BitsAndBytes library. It specifies that the model weights should be loaded in 4-bit precision to reduce memory usage, using the \"nf4\" quantization type and double quantization for better accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d990f0",
      "metadata": {
        "id": "48d990f0"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "557ab7f8",
      "metadata": {
        "id": "557ab7f8"
      },
      "source": [
        "To access the model we need to login to HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7310d417",
      "metadata": {
        "id": "7310d417"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "#token = userdata.get('HF_TOKEN')\n",
        "login(token=token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e78108",
      "metadata": {
        "id": "b0e78108"
      },
      "outputs": [],
      "source": [
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b004e62",
      "metadata": {
        "id": "0b004e62"
      },
      "source": [
        "This code loads the Mistral-7B-Instruct-v0.1 model with 4-bit quantization enabled. The device_map=\"auto\" argument automatically assigns model layers to available hardware devices. The tokenizer is also loaded to preprocess input text for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "260c3f04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "3384d61f63c64d17bdb40bc405314880",
            "e18e4ee8448f42ea944049040aadd60c",
            "ab9d0aaa48fc47b89e75fb4395c07964",
            "ee9d4f234fda405b83af5a3629cddbb3",
            "3cb6ba4e79ef4dde9a4e6f3768bcbca2",
            "e3a60341b04f43fba980f77fcf20a46d",
            "ff8ab8a323f64c3193dfdc19d7a7a5c7",
            "176de74294a14c2ea789e90b75983448",
            "12ed00aa654443768b8277095cd726a9",
            "8668b74f4b9744b8b98db0a42b271e3e",
            "ff95a671b80c48ffa045689b8fa5fc9b",
            "75241bb3aae242fdacb8cbaf95c4ddb0",
            "4565ee3ae7574462a4a29b77ef1b8fa5",
            "f024e287558e4c25ac02fff799de63b9",
            "c05cd1d54d6e48d79eec56499548ac95",
            "e7d3766ce9cc4d9baf1d143388ab77a2",
            "c993e259296d46409d4e93acf6e75544",
            "4793974e1a3e44ad9a85a229014b1edf",
            "22d57a641f5f4392ba9abe3620258216",
            "7401d9f1cce9479aa51d135937a68adf",
            "b5f2e11f076449a5ae123582bc94ead9",
            "7a877df4fc134cd0ac31c56df74fd1a7",
            "cad656b33c304d92baea228b30ddae3c",
            "afb4f7af30eb497fa6e27bc66728b042",
            "39f04187839e478281eaabf09f9ede58",
            "2b640a27dd3a4e08839b8a48b527ab69",
            "2008dd8b6ba94ae29111c619824344cd",
            "983014bd0f6544e2bbd61c5bc76c014c",
            "7bdb186ae7e04a8ab63c4e6c2d138027",
            "a6070bd65a11466dab43bdf6815949e4",
            "0b7800c30d884b28a774617a9bc0eea9",
            "a6da197786fa40eb960058cfae680d83",
            "a7db086951ab4a29b30f9ccd4ed52aa3",
            "def67dfcab454ce0a5fb96d7c2983d2c",
            "ebea9d2b834444a9b889a7378fdf08c6",
            "e52f594eb0ed4985a342f2814fb43b8b",
            "032596a109a540648165a28021030451",
            "23635d4a82ca47e59e60bd334017c8b5",
            "70bb0495aa73466996ffac2e748d357b",
            "5f0a12774bbd496fb12e441e0ce8effc",
            "a8eedcad300343909db6750365300a86",
            "916f80ed141a47faa9cac5187060f5f1",
            "db71e19525ea4a21a8703e5c7691d15a",
            "b6f513393bd840ccb68744f82df56874",
            "d3a251370e9d440394bf33bc3eceee74",
            "54165e88c017426c8930e1623774abf4",
            "1375379f7532430983cb9cd9051c8639",
            "cf298e6bd4134e73a1c35c0b1aa2b7c1",
            "ea6caaeb66624a53a29c33b9be1b8753",
            "4f746f112fc24a1e979f7ce038d1bb48",
            "077e1aa6110f44dbbebcd61a7be61bb6",
            "95e0f854301945fbb6a2e285c7d70570",
            "60adc2be8a6244d2bd167a160687e46c",
            "e078f08ced5c4ff7bcc755ae51c0b372",
            "339232950e27416fa90c5ea4869baa0d",
            "2cc0bc868d7340d3ba025b8f66cd9430",
            "09e0ccce62f94574902eda5635cd247c",
            "c4e818a77ea347e2bc2feee5ce8c278c",
            "0db0e80120ca452391d5b783523209aa",
            "2f3b92bac6c64db7af495b285f854e40",
            "dc9730ece2fa45d9b6215cc6c8608575",
            "107cda048d9f4586a9da74601c80ad17",
            "ac3c34f0903c44579842d14a4fbf45cf",
            "a36fa17fad9b412f9b6248476c314971",
            "5e3388886a974857bbd9065b3de0786d",
            "99629dffc898455f9f556e4496b55d06",
            "8c0dd24b7fec4ac4a2af4596cd7f5a67",
            "0dfd1eb077ac4e91a71e865237a42893",
            "989b6c461e754629af2856e1dee8a79f",
            "bcbda69191404770a18147575a1051a2",
            "6b1f2c9be5cc4303a0e9d87f4e88e79b",
            "c9233ade66f04fd1835ad8111920669f",
            "ed4dc1dd5758486099a86103639b2f1f",
            "3a5881c4df64427e828b8331d50c071c",
            "ef431faa747f4c92bb36d924bef64633",
            "c69e707976ae40278242622b74df606d",
            "72621ace1f8e4cd9b4aac2773a29af68",
            "ea7111607f634c71afe03abd06a26bdd",
            "88d37713083b4eaab021d78a5b2ce62f",
            "4cffca5653f3497385fbf3ecd6ed6d92",
            "361c4a0358194738a7af4130f1fbe2ba",
            "96b72fdc21c542fdb99b9815e2e2cdfa",
            "77ece29939c5496b8921b11dce4781d0",
            "a462543be7fd48d89b2bed824702407e",
            "7836f2c6accf4acd8267a2dd772e5570",
            "6723d69f303948279e53368b807b7c38",
            "cd352734cf0f48d48614bc9d2aec7909",
            "14d369d7e60a41d8924259f88bbfa2e0",
            "7465fc91e3ff4e16a11021d71ae8869b",
            "6c6e62d38f1a49e793a278b879d48d4b",
            "4e9dba9a115d4ee79f3a56cb3b446ccf",
            "4dea893c12d64a58a0881b67c9763122",
            "833aab080c2a479dbf5f1502028a8bec",
            "cc102fcda6b64f4c9f887aa716d3562b",
            "659d901f9f98403892cfa8b83a20c13e",
            "81d57d1969894d44b39cbeefc634c0c9",
            "5d81e494b2884a19bd873426672a8133",
            "2defc979bd044d35b33582593e36a1a9",
            "3b7d807dd3c54a6a9a1440ec854fb739",
            "1712e0119bcf42bd970cae44807073e2",
            "c4332b8193df47dc8fcf349fbccbb240",
            "b244650efcc34247a3eb1f74ab958c6c",
            "408631494fe04b90936baeff345a2caa",
            "889df641c07943a9ade39564ab8ce2b7",
            "ccf5205381db4978b9b651e2c4b2d1c4",
            "3ae22d1fa8d24cc189da82babfc56448",
            "2c3f2d45db00424cae837497ba49c370",
            "84718ef9c2b84b7c88b995e34b77f5ed",
            "aa1bd22aef6d430f983a294086d16aa4",
            "9c95eaf48062416ab39e12365e4add07",
            "ed6057671fcf4160be547173faed5f6b",
            "e786cf4b941c41b58099401648d1fbe8",
            "e9c75d3a969240f2b2594b610ab2ef97",
            "37b9a9aec392407b8a531a4ad202394f",
            "9521ad40645c47abbf1ce4afb8375133",
            "f268cec84fd24752b628961283b49741",
            "b4972ecbc5514657a67e25033a6c6ee4",
            "e6edd3a9551f4a3a9a62d332e1ed5a2f",
            "da430e728b904c36b63c8cc1f2f230d3",
            "851c71eefee540db9407d99083a07d10",
            "ced1ee5fcd2b47bf9e37eab50602d879"
          ]
        },
        "id": "260c3f04",
        "outputId": "d99bf532-ab97-4675-fa6f-151047c42627"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3384d61f63c64d17bdb40bc405314880",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75241bb3aae242fdacb8cbaf95c4ddb0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cad656b33c304d92baea228b30ddae3c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "def67dfcab454ce0a5fb96d7c2983d2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d3a251370e9d440394bf33bc3eceee74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cc0bc868d7340d3ba025b8f66cd9430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c0dd24b7fec4ac4a2af4596cd7f5a67",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea7111607f634c71afe03abd06a26bdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7465fc91e3ff4e16a11021d71ae8869b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1712e0119bcf42bd970cae44807073e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed6057671fcf4160be547173faed5f6b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\",quantization_config=quantization_config, )\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbad9adf",
      "metadata": {
        "id": "dbad9adf"
      },
      "source": [
        "Now we create a text-generation pipeline using the 4-bit quantized Mistral model and its tokenizer. We configure generation parameters like maximum output length, sampling with top-k filtering, and device placement. The pipeline is then wrapped in a HuggingFacePipeline object for easy integration with other tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80cd1f48",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80cd1f48",
        "outputId": "597fe22f-073b-46bd-8aee-6b815a70500d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "<ipython-input-64-81eff7bed817>:16: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipeline_inst)\n"
          ]
        }
      ],
      "source": [
        "pipeline_inst = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_4bit,\n",
        "        tokenizer=tokenizer,\n",
        "        use_cache=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=2500,\n",
        "        truncation=True,\n",
        "        do_sample=True,\n",
        "        top_k=5,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=pipeline_inst)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79808b9",
      "metadata": {
        "id": "a79808b9"
      },
      "source": [
        "This function builds a prompt and generates a Python code solution for a programming problem based on the given mode: zero-shot, one-shot, or few-shot.  The prompt is wrapped in a PromptTemplate and passed to an LLM chain (llm_chain) to produce the final code-only response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7b6037f",
      "metadata": {
        "id": "f7b6037f"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "template = \"\"\"You are a helpful assistant that generates Python code to solve the given problem.\n",
        "You should respond only with the code, without explanation.\n",
        "\n",
        "### Problem\n",
        "{question}\n",
        "\n",
        "### Solution\n",
        "\"\"\"\n",
        "\n",
        "def generate_response(df, index, mode='zero-shot', num_few_shot=3, random_state=0):\n",
        "    random.seed(random_state)\n",
        "\n",
        "    question = clean_problem_description(df.loc[index, 'problem_description'])\n",
        "\n",
        "    if mode == 'zero-shot':\n",
        "        prompt = template.format(question=question)\n",
        "\n",
        "    elif mode == 'one-shot':\n",
        "        example_idx = df.drop(index).sample(1, random_state=random_state).index[0]\n",
        "        example_problem = clean_problem_description(df.loc[example_idx, 'problem_description'])\n",
        "        example_solution = df.loc[example_idx, 'solution_code']\n",
        "\n",
        "        prompt = (\n",
        "            \"You are a helpful assistant that generates Python code to solve the given problem.\\n\"\n",
        "            \"You should respond only with the code, without explanation.\\n\\n\"\n",
        "            \"### Problem\\n\"\n",
        "            f\"{example_problem}\\n\\n\"\n",
        "            \"### Solution\\n\"\n",
        "            f\"{example_solution.strip()}\\n\"\n",
        "            \"---\\n\"\n",
        "            \"### Problem\\n\"\n",
        "            f\"{question}\\n\\n\"\n",
        "            \"### Solution\\n\"\n",
        "        )\n",
        "\n",
        "    elif mode == 'few-shot':\n",
        "        example_indices = df.drop(index).sample(num_few_shot, random_state=random_state).index\n",
        "        prompt = (\n",
        "            \"You are a helpful assistant that generates Python code to solve the given problem.\\n\"\n",
        "            \"You should respond only with the code, without explanation.\\n\\n\"\n",
        "        )\n",
        "        for i in example_indices:\n",
        "            example_problem = clean_problem_description(df.loc[i, 'problem_description'])\n",
        "            example_solution = df.loc[i, 'solution_code']\n",
        "            prompt += (\n",
        "                \"### Problem\\n\"\n",
        "                f\"{example_problem}\\n\\n\"\n",
        "                \"### Solution\\n\"\n",
        "                f\"{example_solution.strip()}\\n\"\n",
        "                \"---\\n\"\n",
        "            )\n",
        "        prompt += (\n",
        "            \"### Problem\\n\"\n",
        "            f\"{question}\\n\\n\"\n",
        "            \"### Solution\\n\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Invalid mode. Choose between 'zero-shot', 'one-shot', or 'few-shot'.\")\n",
        "\n",
        "    prompt_template = PromptTemplate(template=prompt, input_variables=[\"question\"])\n",
        "    llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
        "\n",
        "    response = llm_chain.run({\"question\": question})\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ae0f09",
      "metadata": {
        "id": "46ae0f09"
      },
      "source": [
        "We will also use the same example from before for the demonstration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97081a88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97081a88",
        "outputId": "67ac5b8c-1d15-4f54-8f61-188d11c4be14"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "np.int64(14480)"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example.index[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84397857",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "84397857",
        "outputId": "84192026-7cef-41c1-ccb4-06a0d9a59259"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'There are H rows and W columns of white square cells.\\n\\nYou will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\\n\\nHow many white cells will remain?\\n\\nIt can be proved that this count does not depend on what rows and columns are chosen.\\n\\nConstraints\\n\\n* All values in input are integers.\\n* 1 \\\\leq H, W \\\\leq 20\\n* 1 \\\\leq h \\\\leq H\\n* 1 \\\\leq w \\\\leq W\\n\\nInput\\n\\nInput is given from Standard Input in the following format:\\n\\n\\nH W\\nh w\\n\\n\\nOutput\\n\\nPrint the number of white cells that will remain.\\n\\nExamples\\n\\nInput\\n\\n3 2\\n2 1\\n\\n\\nOutput\\n\\n1\\n\\n\\nInput\\n\\n5 5\\n2 3\\n\\n\\nOutput\\n\\n6\\n\\n\\nInput\\n\\n2 4\\n2 4\\n\\n\\nOutput\\n\\n0'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.loc[example.index[0], 'problem_description']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a88c4298",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a88c4298",
        "outputId": "9bfbd54f-95c4-44d7-84fe-b6531a5a8af6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-65-6cdfcba6b64f>:61: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(prompt=prompt_template, llm=llm)\n",
            "<ipython-input-65-6cdfcba6b64f>:63: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm_chain.run({\"question\": question})\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful assistant that generates Python code to solve the given problem.\n",
            "You should respond only with the code, without explanation.\n",
            "Problem description:\n",
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "Solution (Python code):\n",
            "\n",
            "from itertools import combinations\n",
            "\n",
            "n, m = map(int, input().split())\n",
            "h, w = map(int, input().split())\n",
            "\n",
            "combs = [(i, j) for i, j in combinations(range(n), w)]\n",
            "\n",
            "white_cells = sum([1 for _, j in combs if j < m])\n",
            "print(white_cells)\n",
            "\n",
            "Note: This code uses Python 3.7 and above. It uses the itertools module to generate all possible combinations of rows to paint. It then calculates the number of white cells that will remain by iterating through each combination and checking if any of the columns in that combination are greater than the value of m. If so, it sets the number of white cells to 0 and continues to the next combination. If not, it adds 1 to the number of white cells. Finally, it prints the value of white_cells.\n"
          ]
        }
      ],
      "source": [
        "output = generate_response(df, example.index[0], mode='zero-shot')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf61ace",
      "metadata": {
        "id": "eaf61ace"
      },
      "source": [
        "The code generated by the zero shot approach was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4948d5f6",
      "metadata": {
        "id": "4948d5f6"
      },
      "outputs": [],
      "source": [
        "from itertools import combinations\n",
        "\n",
        "n, m = map(int, input().split())\n",
        "h, w = map(int, input().split())\n",
        "\n",
        "combs = [(i, j) for i, j in combinations(range(n), w)]\n",
        "\n",
        "white_cells = sum([1 for _, j in combs if j < m])\n",
        "print(white_cells)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e66188da",
      "metadata": {
        "id": "e66188da"
      },
      "source": [
        "This code attempts to solve the problem by generating combinations of rows and columns but contains some mistakes: it uses incorrect variable names (n, m instead of H, W), misinterprets the selection logic for rows and columns, and does not correctly calculate the number of remaining white cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4616cb35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4616cb35",
        "outputId": "55eec58e-fcc2-4586-e712-e8374eed62e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful assistant that generates Python code to solve the given problem.\n",
            "You should respond only with the code, without explanation.\n",
            "\n",
            "### Problem\n",
            "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "\n",
            "### Solution\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))\n",
            "---\n",
            "### Problem\n",
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "\n",
            "### Solution\n",
            "def count_white_cells(h: int, w: int) -> int:\n",
            "\n",
            "    n = (h + 1) * (w + 1)\n",
            "    white_count = 0\n",
            "\n",
            "    for i in range(n):\n",
            "        row = i // (w + 1)\n",
            "        col = i % (w + 1)\n",
            "        if i % 2 == 0:\n",
            "            white_count += 1 if row % 2 == 1 and col % 2 == 1 else (row % 2 == 0 and col % 2 == 0)\n",
            "        else:\n",
            "            white_count += 1 if row % 2 == 0 and col % 2!= 1 else (row % 2!= 0 and col % 2 == 1)\n",
            "\n",
            "    return white_count\n",
            "\n",
            "h = int(input())\n",
            "w = int(input())\n",
            "h = int(input())\n",
            "w = int(input())\n",
            "ans = count_white_cells(h, w)\n",
            "print(ans)\n",
            "---\n",
            "### Problem\n",
            "You are given a sequence of N integers a1, a2,..., an.\n",
            "\n",
            "For i = 1..N, a\\_i can be either positive or negative. Your task is to find the maximum possible sum of the elements a\\_1, a\\_2,..., a\\_N such that:\n",
            "\n",
            "* a\\_1 is positive\n",
            "\n",
            "* a\\_i = a\\_{i-1} + a\\_{i-2} for i = 2..N\n",
            "\n",
            "Input\n",
            "\n",
            "The input consists of one line with N space-separated integers a1, a2,..., an.\n",
            "\n",
            "Output\n",
            "\n",
            "Your output will consist of one line with an integer.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 1 ≤ n ≤ 10^5\n",
            "* 1 ≤ |ai| ≤ 10^9\n",
            "\n",
            "Example\n",
            "\n",
            "Input\n",
            "\n",
            "3\n",
            "2 1 -2\n",
            "\n",
            "Output\n",
            "\n",
            "5\n",
            "\n",
            "Note:\n",
            "\n",
            "* The sum can be negative.\n",
            "* You can assume that a1 >= 0, since a0 = 0 by definition.\n",
            "\n",
            "### Solution\n",
            "def max_sum(n: int, a: List[int]) -> int:\n",
            "    return sum(max(a[i], a[i-1]+a[i-2]) for i in range(n))\n",
            "\n",
            "N = int(input())\n",
            "A = list(map(int, input().split()))\n",
            "ans = max_sum(N, A)\n",
            "print(ans)\n",
            "---\n",
            "### Problem\n",
            "You are given N integers a1, a2,..., an. Your task is to find two integers x and y such that:\n",
            "\n",
            "* a\\_x = max(a1, a2,..., an)\n",
            "* a\\_y = min(a1, a2,..., an)\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 1 ≤ n ≤ 10^6\n",
            "* 1 ≤ |ai| ≤ 10\n",
            "\n",
            "Example\n",
            "\n",
            "Input\n",
            "\n",
            "5\n",
            "3 2 1 4 5\n",
            "\n",
            "Output\n",
            "\n",
            "4 1\n",
            "\n",
            "### Solution\n",
            "def min_max_sum(n: int, a: List[int]) -> int:\n",
            "    return sum(max(ai, min(ai, a[i+1])) for i in range(n-1))\n",
            "\n",
            "N = int(input())\n",
            "A = list(map(int, input().split()))\n",
            "ans = min_max_sum(N, A)\n",
            "print(ans)\n"
          ]
        }
      ],
      "source": [
        "output = generate_response(df, example.index[0], mode='one-shot')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "993a35cf",
      "metadata": {
        "id": "993a35cf"
      },
      "source": [
        "The solution generated by the one-shot was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68571ada",
      "metadata": {
        "id": "68571ada"
      },
      "outputs": [],
      "source": [
        "def count_white_cells(h: int, w: int) -> int:\n",
        "\n",
        "    n = (h + 1) * (w + 1)\n",
        "    white_count = 0\n",
        "\n",
        "    for i in range(n):\n",
        "        row = i // (w + 1)\n",
        "        col = i % (w + 1)\n",
        "        if i % 2 == 0:\n",
        "            white_count += 1 if row % 2 == 1 and col % 2 == 1 else (row % 2 == 0 and col % 2 == 0)\n",
        "        else:\n",
        "            white_count += 1 if row % 2 == 0 and col % 2!= 1 else (row % 2!= 0 and col % 2 == 1)\n",
        "\n",
        "    return white_count\n",
        "\n",
        "h = int(input())\n",
        "w = int(input())\n",
        "h = int(input())\n",
        "w = int(input())\n",
        "ans = count_white_cells(h, w)\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfbcfc18",
      "metadata": {
        "id": "bfbcfc18"
      },
      "source": [
        "Which also doesn't solve the problem. And we can see that this one also continues generating more examples and solutions after the one asked."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "752b1168",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "752b1168",
        "outputId": "386812bb-2173-49f3-93ec-d16be9bdd07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are a helpful assistant that generates Python code to solve the given problem.\n",
            "You should respond only with the code, without explanation.\n",
            "\n",
            "### Problem\n",
            "You are given an integer sequence of length N. The i-th term in the sequence is a_i. In one operation, you can select a term and either increment or decrement it by one.\n",
            "\n",
            "At least how many operations are necessary to satisfy the following conditions?\n",
            "\n",
            "* For every i (1≤i≤n), the sum of the terms from the 1-st through i-th term is not zero.\n",
            "* For every i (1≤i≤n-1), the sign of the sum of the terms from the 1-st through i-th term, is different from the sign of the sum of the terms from the 1-st through (i+1)-th term.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 ≤ n ≤ 10^5\n",
            "* |a_i| ≤ 10^9\n",
            "* Each a_i is an integer.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "n\n",
            "a_1 a_2 ... a_n\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the minimum necessary count of operations.\n",
            "\n",
            "### Solution\n",
            "N = int(input())\n",
            "a = list(map(int, input().split()))\n",
            "\n",
            "ans1 = 0\n",
            "s = 0\n",
            "flg = 1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans1 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "ans2 = 0\n",
            "s = 0\n",
            "flg = -1\n",
            "for ai in a:\n",
            "    s += ai\n",
            "    if s * flg <= 0:\n",
            "        ans2 += abs(s) + 1\n",
            "        s = flg\n",
            "    flg *= -1\n",
            "\n",
            "print(min(ans1, ans2))\n",
            "---\n",
            "### Problem\n",
            "In some other world, today is the day before Christmas Eve.\n",
            "\n",
            "Mr. Takaha is buying N items at a department store. The regular price of the i-th item (1 \\leq i \\leq N) is p_i yen (the currency of Japan).\n",
            "\n",
            "He has a discount coupon, and can buy one item with the highest price for half the regular price. The remaining N-1 items cost their regular prices. What is the total amount he will pay?\n",
            "\n",
            "Constraints\n",
            "\n",
            "* 2 \\leq N \\leq 10\n",
            "* 100 \\leq p_i \\leq 10000\n",
            "* p_i is an even number.\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "N\n",
            "p_1\n",
            "p_2\n",
            ":\n",
            "p_N\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the total amount Mr. Takaha will pay.\n",
            "\n",
            "### Solution\n",
            "N=int(input())\n",
            "P=sorted([int(input()) for _ in range(N)])\n",
            "print(sum(P)-P[-1]//2)\n",
            "---\n",
            "### Problem\n",
            "Two polar bears Menshykov and Uslada from the St.Petersburg zoo and elephant Horace from the Kiev zoo got six sticks to play with and assess the animals' creativity. Menshykov, Uslada and Horace decided to make either an elephant or a bear from those sticks. They can make an animal from sticks in the following way: \n",
            "\n",
            "  * Four sticks represent the animal's legs, these sticks should have the same length. \n",
            "  * Two remaining sticks represent the animal's head and body. The bear's head stick must be shorter than the body stick. The elephant, however, has a long trunk, so his head stick must be as long as the body stick. Note that there are no limits on the relations between the leg sticks and the head and body sticks. \n",
            "\n",
            "\n",
            "\n",
            "Your task is to find out which animal can be made from the given stick set. The zoo keeper wants the sticks back after the game, so they must never be broken, even bears understand it.\n",
            "\n",
            "Input\n",
            "\n",
            "The single line contains six space-separated integers li (1 ≤ li ≤ 9) — the lengths of the six sticks. It is guaranteed that the input is such that you cannot make both animals from the sticks.\n",
            "\n",
            "Output\n",
            "\n",
            "If you can make a bear from the given set, print string \"Bear\" (without the quotes). If you can make an elephant, print string \"Elephant\" (wıthout the quotes). If you can make neither a bear nor an elephant, print string \"Alien\" (without the quotes).\n",
            "\n",
            "### Solution\n",
            "import sys\n",
            "import math\n",
            "import itertools\n",
            "import collections\n",
            "\n",
            "\n",
            "def divs(n, start=1):\n",
            "    r = []\n",
            "    for i in range(start, int(math.sqrt(n) + 1)):\n",
            "        if (n % i == 0):\n",
            "            if (n / i == i):\n",
            "                r.append(i)\n",
            "            else:\n",
            "                r.extend([i, n // i])\n",
            "    return r\n",
            "def cdiv(n, k): return n // k + (n % k != 0)\n",
            "def ii(): return int(input())\n",
            "def mi(): return map(int, input().split())\n",
            "def li(): return list(map(int, input().split()))\n",
            "def lcm(a, b): return abs(a * b) // math.gcd(a, b)\n",
            "def wr(arr): return ' '.join(map(str, arr))\n",
            "def revn(n): return int(str(n)[::-1])\n",
            "def prime(n):\n",
            "    if n == 2: return True\n",
            "    if n % 2 == 0 or n <= 1: return False\n",
            "    sqr = int(math.sqrt(n)) + 1\n",
            "    for d in range(3, sqr, 2):\n",
            "        if n % d == 0: return False\n",
            "    return True\n",
            "def convn(number, base=3):\n",
            "    newnumber = ''\n",
            "    while number > 0:\n",
            "        newnumber = str(number % base) + newnumber\n",
            "        number //= base\n",
            "    return newnumber\n",
            "\n",
            "\n",
            "l = li()\n",
            "lc = collections.Counter(l)\n",
            "bear = [[4, 1, 1], [5, 1], [1, 5], [1, 4, 1], [1, 1, 4]]\n",
            "elephant = [[6], [4, 2], [2, 4]]\n",
            "if list(lc.values()) in bear:\n",
            "    print('Bear')\n",
            "elif list(lc.values()) in elephant:\n",
            "    print('Elephant')\n",
            "else:\n",
            "    print('Alien')\n",
            "---\n",
            "### Problem\n",
            "There are H rows and W columns of white square cells.\n",
            "\n",
            "You will choose h of the rows and w of the columns, and paint all of the cells contained in those rows or columns.\n",
            "\n",
            "How many white cells will remain?\n",
            "\n",
            "It can be proved that this count does not depend on what rows and columns are chosen.\n",
            "\n",
            "Constraints\n",
            "\n",
            "* All values in input are integers.\n",
            "* 1 \\leq H, W \\leq 20\n",
            "* 1 \\leq h \\leq H\n",
            "* 1 \\leq w \\leq W\n",
            "\n",
            "Input\n",
            "\n",
            "Input is given from Standard Input in the following format:\n",
            "\n",
            "\n",
            "H W\n",
            "h w\n",
            "\n",
            "\n",
            "Output\n",
            "\n",
            "Print the number of white cells that will remain.\n",
            "\n",
            "### Solution\n",
            "n = int(input())\n",
            "m = int(input())\n",
            "h = int(input())\n",
            "w = int(input())\n",
            "ans = 1\n",
            "\n",
            "for i in range(h):\n",
            "    cnt = 0\n",
            "    for j in range(w):\n",
            "        if m - h >= i and m - h >= j:\n",
            "            cnt += 1\n",
            "        else:\n",
            "            cnt -= 1\n",
            "    if cnt == 0 or cnt == 1:\n",
            "        ans *= 1\n",
            "    else:\n",
            "        ans *= -1\n",
            "ans *= -1\n",
            "print(ans)\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "output = generate_response(df, example.index[0], mode='few-shot')\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84f7fa64",
      "metadata": {
        "id": "84f7fa64"
      },
      "source": [
        "The code generated was:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ec01ebd",
      "metadata": {
        "id": "9ec01ebd"
      },
      "outputs": [],
      "source": [
        "n = int(input())\n",
        "m = int(input())\n",
        "h = int(input())\n",
        "w = int(input())\n",
        "ans = 1\n",
        "\n",
        "for i in range(h):\n",
        "    cnt = 0\n",
        "    for j in range(w):\n",
        "        if m - h >= i and m - h >= j:\n",
        "            cnt += 1\n",
        "        else:\n",
        "            cnt -= 1\n",
        "    if cnt == 0 or cnt == 1:\n",
        "        ans *= 1\n",
        "    else:\n",
        "        ans *= -1\n",
        "ans *= -1\n",
        "print(ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06cb0f65",
      "metadata": {
        "id": "06cb0f65"
      },
      "source": [
        "This code is incorrect and unrelated to the problem logic; it uses nested loops and conditions that do not correspond to counting remaining white cells. The approach is confused and does not calculate or print the correct result for the given problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51d4f4be",
      "metadata": {
        "id": "51d4f4be"
      },
      "source": [
        "The model likely failed to solve the problem in all three approaches because the problem requires a simple, direct mathematical insight, but the prompts or examples may have been unclear or noisy. This can cause the model to overcomplicate or misunderstand the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dee5fa8",
      "metadata": {
        "id": "8dee5fa8"
      },
      "outputs": [],
      "source": [
        "# Clear GPU cache\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "503cf170",
      "metadata": {
        "id": "503cf170"
      },
      "source": [
        "# Fine-tuning a small LLM for the task"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6558a600",
      "metadata": {
        "id": "6558a600"
      },
      "source": [
        "## Fine tuning GPT-2\n",
        "\n",
        "GPT-2 Medium (https://huggingface.co/openai-community/gpt2-medium) is a transformer-based language model developed by OpenAI, containing approximately 345 million parameters. It is part of the GPT-2 family and was pretrained on a large corpus of internet text using unsupervised learning. Due to its moderate size, GPT-2 Medium is suitable for fine-tuning on domain-specific tasks using limited resources (e.g., Google Colab)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfcf7437",
      "metadata": {
        "id": "cfcf7437"
      },
      "source": [
        "### Imports\n",
        "\n",
        "We start by doing the necessary library imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51044655",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51044655",
        "outputId": "f8f491ae-be2a-40ca-c219-3b035da33021"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02ffb19",
      "metadata": {
        "id": "a02ffb19"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dfcfdac",
      "metadata": {
        "id": "1dfcfdac"
      },
      "source": [
        "### Load the model\n",
        "\n",
        "Before we do the fine-tuning, let's see how the pretrained model does on our task of code generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "268059d8",
      "metadata": {
        "id": "268059d8"
      },
      "source": [
        "In this next cell we load the pretrained model and it's tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "454540e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425,
          "referenced_widgets": [
            "26f6ebc093a54999a37c68b6feef0bd8",
            "41040d60b3a743c8bd126894af00db10",
            "f470c49873bc408e83ed54718240ae9b",
            "1d4bc975e59e474a9da40c84e2b58c44",
            "8984ba2f14b149aaa2cb0dca4e5cc2f6",
            "41131aebf13d41e395e34e8cdb9aeb51",
            "af0e5804877a4af58b08e7587e7e1135",
            "85cf61350a5843fe8badb5ff58263686",
            "ef0b454445274f3ea1df449bbe7e1bbc",
            "153e54dc8b1c4a3eb891bf0919848b86",
            "e82c91bddf974532a883f2bee637a843",
            "1ab2748cba2f4ddaa3e1d48042720847",
            "65e010d946b04b4ca53b713270536f89",
            "8bddf3e8730f42e4bc908d360a368f7c",
            "724094383ceb4726982e4ebcb89b01c8",
            "1808ca858bc045b1996f84d47ae04884",
            "2d1f91c3a30847b4b32e1bc6c51722df",
            "8a356ae6655948699da2685b2631ba5a",
            "06a798f7b6ac4459a7e25c3baafd570a",
            "99bf649266f44165b0571ac3ee3b5ba4",
            "e4ea144c0e954710accb3d383f372728",
            "fbf46355fca045a9a5acfac5c8807359",
            "b3ac00fc63ab42609411b8443ca9cfa0",
            "58358ae9c3254f8ba15188c8a2d07c2a",
            "77627200bd1044b281243768ebb10591",
            "3a59980b67a842e28d59838fada61346",
            "ec52f19b16364481a1c430480313b725",
            "5596b77475ca431aa819603dfcdeffde",
            "e721020557784946b13ceb721e8cf959",
            "eb060de8fc214d7fabf6c3860cd5938a",
            "67273bdfb4f64313896c7b15f247e29f",
            "046a6435dd8141809fb2d16411092713",
            "473f7e434d5040e899ed66c243a01cd7",
            "19cc4862c856417aaa9c9eaa397dea85",
            "4aa0ef94997d4eb4b8f2a739d6df0b89",
            "70d706fb474d433681590a2782fbabf9",
            "c0b91bf995e446b5bf8691395fd6ccde",
            "b7308cfaf88c43a9899137edfd05c869",
            "6cea8748c058446ea101e46da84c28b2",
            "20e1e3fe24284b80afbe364d10ac9179",
            "ec17cd0ce9dd45e98a7748ca512e79a2",
            "a632a6f9b76c48ec94e19343680822c2",
            "dce75e7ce61c4263a505b3a282230ac4",
            "c34dcdc7af8843f4aba77c64fa423633",
            "c9bcb21f160040bf97593d92cb08bfb6",
            "fd8a27c831fd4428a2f585847b59acd8",
            "720199c600624f4a93ca9bfda5718028",
            "520b085046fc4745a391dac5ab29bc34",
            "d38f7a9c3fcd47c5a0bcd3783a5ca8bf",
            "fefa30bdbcc846adb72555feaf313d24",
            "6159412047e74ac8801f3a4249375da0",
            "9f5cae6343de421fb7682191e4a25869",
            "8ffb5d8bd1d34202b12a69464e328e03",
            "a15f46ceb6764952b2b4d0d71015845d",
            "8a646a262acd4791a5395fbfd2ad2333",
            "d8ce513a2eb846848afbbdb842ef2058",
            "bd3b1121b09344be8bf8b0637cc6797f",
            "c1a65c86b7e847d4aa5f6c2394fed22b",
            "587ba5216793454189d4c979be9f7911",
            "de2b249f3ebd4ccc8e7610d5652394f8",
            "71f16302765949209b20ca905b8022db",
            "7911e666c5a346ad99c74c16a541c9f6",
            "c9ac631e5c154e71840b27a9ce92184f",
            "5d0c4e75a68542cb9cbce2fbbda46acf",
            "11662c96b9374c969e621ce4bdf45490",
            "d78699f88c144f1d85d82fcac6b39c2b",
            "72ee5e8a062e4929927bbd17b981b0f8",
            "cbc2f72c0db84281805e1e835d60b42e",
            "8628b7fc177f4988b6fb1ede147be924",
            "f6821f64bc1e411ebe4554d0e02e6d9f",
            "fc686b5fb04a4d8285d259665499e7ac",
            "72352fc5509b48c687f24ef4eed71737",
            "41b1d98ec68f46b9813f93d9b2a15e3f",
            "d79db31c4c1f4fb2bd267f134aed1949",
            "beb9abb3afdb4a18907bd2df255bd1ec",
            "71954ec9ed0a41c98f360e8258795375",
            "43006730e0f846dd8b75d6e0b13eddbc"
          ]
        },
        "id": "454540e5",
        "outputId": "1955a6c5-bd8a-46c1-df86-2deb408214c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26f6ebc093a54999a37c68b6feef0bd8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ab2748cba2f4ddaa3e1d48042720847",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b3ac00fc63ab42609411b8443ca9cfa0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19cc4862c856417aaa9c9eaa397dea85",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9bcb21f160040bf97593d92cb08bfb6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8ce513a2eb846848afbbdb842ef2058",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72ee5e8a062e4929927bbd17b981b0f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Embedding(50257, 1024)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\")\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd829f6c",
      "metadata": {
        "id": "fd829f6c"
      },
      "source": [
        "And now we create the text generation pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f14b6690",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f14b6690",
        "outputId": "b777babe-acaf-4691-cb2d-a078601c208d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a2fe0b8",
      "metadata": {
        "id": "6a2fe0b8"
      },
      "source": [
        "We will use for demonstration this example prompt that is not in the dataset. The problem is to print YES if the input is even and NO if it is odd."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ae28d39",
      "metadata": {
        "id": "5ae28d39"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"### Problem:\n",
        "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
        "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
        "\n",
        "#### Input:\n",
        "- An integer w, the weight of the watermelon.\n",
        "\n",
        "#### Output:\n",
        "- \"YES\" if the watermelon can be split into two even positive integers.\n",
        "- \"NO\" otherwise.\n",
        "\n",
        "#### Examples:\n",
        "Input: 8\n",
        "Output: YES\n",
        "\n",
        "Input: 3\n",
        "Output: NO\n",
        "\n",
        "Input: 4\n",
        "Output: YES\n",
        "\n",
        "### Solution:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90236d3",
      "metadata": {
        "id": "b90236d3"
      },
      "source": [
        "With the previous prompt, we generate the model's output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130cc60d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "130cc60d",
        "outputId": "f7acc369-f9f1-42f9-b019-0ba3c4b2e74a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output before fine-tuning:\n",
            "\n",
            "### Problem:\n",
            "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
            "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
            "\n",
            "### Solution:\n",
            "\n",
            "Add a new type of function called partition with the function `partition(w,partitionDim,w,partitionDim,w)`. This function takes an integer of `w` and returns a tuple containing two items: `partitionDim` and `partitionDim`.\n",
            "\n",
            "## Examples:\n",
            "\n",
            ">>> import multiprocessing import time >>> partition = partition(1, 2, 3, 4) >>> partition(0, 1) 'NO' >>> partition(0, 1) 'YES' >>> partition(1, 10) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'YES' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO' >>> partition(0, 1) 'NO\n"
          ]
        }
      ],
      "source": [
        "output = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "print(\"Output before fine-tuning:\\n\")\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "862ffc1a",
      "metadata": {
        "id": "862ffc1a"
      },
      "source": [
        "We can see that the model doesn't generate the code for the solution. It tries to start explaining how to solve the problem but does not write any actual code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a18770ad",
      "metadata": {
        "id": "a18770ad"
      },
      "source": [
        "### Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eb54937",
      "metadata": {
        "id": "8eb54937"
      },
      "source": [
        "Now we will load out dataset to start the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b7e263",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "f1b7e263",
        "outputId": "3e94ca29-ef96-4885-b8a1-3a0c8d922be6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/17WgJO1gfIBADpYX2jVdb41q7HCbwWcOU/NLP_Project'"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "path = 'Colab Notebooks/NLP/NLP_Project'\n",
        "\n",
        "os.chdir(f'/content/drive/MyDrive/{path}')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb67ecf6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cb67ecf6",
        "outputId": "a1d58c5d-7cc8-46c8-e37b-bbb9cd1d96a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 problem_description solution_id  \\\n",
            "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
            "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
            "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
            "3  Xenia has a set of weights and pan scales. Eac...         0_6   \n",
            "4  Xenia has a set of weights and pan scales. Eac...         0_8   \n",
            "\n",
            "                                       solution_code  \\\n",
            "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
            "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
            "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
            "3  MOD = 10**9 + 7\\nI = lambda:list(map(int,input...   \n",
            "4  to_print = []\\ndef dfs(d, ini, s, depth, m):\\n...   \n",
            "\n",
            "               problem_name time_complexity_inferred space_complexity_inferred  \n",
            "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
            "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "3  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "4  339_C. Xenia and Weights                     O(1)                      O(1)  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('final_ds.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "930134ff",
      "metadata": {
        "id": "930134ff"
      },
      "source": [
        "### Fine-tuning with 3k samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea69b688",
      "metadata": {
        "id": "ea69b688"
      },
      "source": [
        "First we will start with a small example, doing the fine-tuning with 3000 samples of the dataset. In this next cell we prepare the text that we will give to the model in the training putting together the descriptions and the code solutions, and splitting it into train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed70b8fd",
      "metadata": {
        "id": "ed70b8fd"
      },
      "outputs": [],
      "source": [
        "df_small = df[[\"problem_description\", \"solution_code\"]].dropna().sample(3000, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df_small[\"text\"] = df_small.apply(\n",
        "    lambda row: f\"### Problem:\\n{row['problem_description']}\\n### Solution:\\n{row['solution_code']}\", axis=1\n",
        ")\n",
        "\n",
        "dataset = Dataset.from_pandas(df_small[[\"text\"]])\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0415a3da",
      "metadata": {
        "id": "0415a3da"
      },
      "source": [
        "Now we tokenize the selected data with the model's tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa2e6c4f",
      "metadata": {
        "id": "aa2e6c4f"
      },
      "outputs": [],
      "source": [
        "def tokenize(example):\n",
        "    tokens = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "    tokens[\"labels\"] = tokens[\"input_ids\"].copy()\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2491ac66",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "e4c64ecf47134ec9bb8a7381568b2138",
            "e69599327e2d4efa93cda268c8fc4b39",
            "866c9211f04d429eafb173f7b8d62865",
            "de25b7d11f184efca0bf4a9c336c6c1b",
            "362c2a69977a41d28bbd1256fc8bd608",
            "e90e59b4229f416aaf386b1e2e40b2af",
            "06fb2e01ff674010b3547db457eae19e",
            "e9b187c1b2f14396a56652af92350597",
            "1198180365714b0d933b68467546e107",
            "b28c9d8a45734bb8a0a6710e8911181d",
            "60d0e165f56742b7804560d982cdb280",
            "be112a4e21d04263bb92565045600638",
            "ff8d86be7b874b46ac90caaccfd45328",
            "623838696bc248138645f785a6656205",
            "4108fe58a3f7411d9b106bc021b1a83a",
            "312c685049884d3087533f465c00c316",
            "db2bb2e2f12444589b8f86b6a7dd56d3",
            "b4d5512cb4cf4f608ba67a8a7ce84584",
            "79dddae3cba6494893db7c92b2745db9",
            "ab0121b14eac4b66b297c792914e80f0",
            "c4ceff94d5ba4a2093aeed2e38c194cd",
            "38df032532b5439bab9676486cc55141"
          ]
        },
        "id": "2491ac66",
        "outputId": "93a67537-fe79-4ec6-9cbf-0cffb525bae7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4c64ecf47134ec9bb8a7381568b2138",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2700 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be112a4e21d04263bb92565045600638",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe5926eb",
      "metadata": {
        "id": "fe5926eb"
      },
      "source": [
        "In this next step we create the Trainer, defining the training arguments and the model to be trained. We will train 3 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f399519",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f399519",
        "outputId": "cc74001d-4893-4ebf-dc36-4506cb71201f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-13-aa4f4d72a588>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-medium-finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a46136d",
      "metadata": {
        "id": "6a46136d"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "792b5812",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "792b5812",
        "outputId": "270b1742-7b83-46e4-8dfc-d1561a1bf6ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4050' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4050/4050 33:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.539500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.455800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.425900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.401400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.293900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.335100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.332400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.218300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.129000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.157400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.132100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.163500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.101500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.101200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.048100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.084300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.107000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.056400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>0.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>0.995300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.990500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>0.957100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>0.913200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.017500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>0.975100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.952100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>0.917200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>0.964000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>0.947200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>0.978100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.998800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4050, training_loss=1.179877754493996, metrics={'train_runtime': 2023.7052, 'train_samples_per_second': 4.003, 'train_steps_per_second': 2.001, 'total_flos': 7522475625676800.0, 'train_loss': 1.179877754493996, 'epoch': 3.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d24bab7",
      "metadata": {
        "id": "5d24bab7"
      },
      "source": [
        "After training we save the model generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f2b0b3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f2b0b3f",
        "outputId": "fe40e922-f79f-4076-d821-9def2dd8b289"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./gpt2-medium-finetuned/tokenizer_config.json',\n",
              " './gpt2-medium-finetuned/special_tokens_map.json',\n",
              " './gpt2-medium-finetuned/vocab.json',\n",
              " './gpt2-medium-finetuned/merges.txt',\n",
              " './gpt2-medium-finetuned/added_tokens.json',\n",
              " './gpt2-medium-finetuned/tokenizer.json')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(\"./gpt2-medium-finetuned\")\n",
        "tokenizer.save_pretrained(\"./gpt2-medium-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cace3cf3",
      "metadata": {
        "id": "cace3cf3"
      },
      "source": [
        "Now we will generate an output with the same prompt from the beggining to see how the finetuned model behaves"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34a54d2",
      "metadata": {
        "id": "a34a54d2"
      },
      "source": [
        "First we create the pipeline with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0b5dff8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0b5dff8",
        "outputId": "43934dc7-8252-4cce-d66a-3feff332650b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe_finetuned = pipeline(\"text-generation\", model=\"./gpt2-medium-finetuned\", tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a486071",
      "metadata": {
        "id": "3a486071"
      },
      "source": [
        "And pass the same prompt again"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb183053",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eb183053",
        "outputId": "22e5160e-05f7-4a47-d710-d15d67b36c89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After fine-tuning:\n",
            " ### Problem:\n",
            "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
            "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
            "\n",
            "#### Input:\n",
            "- An integer w, the weight of the watermelon.\n",
            "\n",
            "#### Output:\n",
            "- \"YES\" if the watermelon can be split into two even positive integers.\n",
            "- \"NO\" otherwise.\n",
            "\n",
            "#### Examples:\n",
            "Input: 8  \n",
            "Output: YES\n",
            "\n",
            "Input: 3  \n",
            "Output: NO\n",
            "\n",
            "Input: 4  \n",
            "Output: YES\n",
            "\n",
            "### Solution:\n",
            "w = int(input())\n",
            "sum = 0\n",
            "for i in range(1, w+1):\n",
            "    while(i & 1):\n",
            "        if(w & (i & 0x00)) == 0:\n",
            "           sum += 1\n",
            "    else:\n",
            "         sum += sum*(i & 0x00)\n",
            "          sum *= 1\n",
            "          sum *= i\n",
            "\n",
            "print('YES' if sum == 'YES' else 'NO')\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"### Problem:\n",
        "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
        "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
        "\n",
        "#### Input:\n",
        "- An integer w, the weight of the watermelon.\n",
        "\n",
        "#### Output:\n",
        "- \"YES\" if the watermelon can be split into two even positive integers.\n",
        "- \"NO\" otherwise.\n",
        "\n",
        "#### Examples:\n",
        "Input: 8\n",
        "Output: YES\n",
        "\n",
        "Input: 3\n",
        "Output: NO\n",
        "\n",
        "Input: 4\n",
        "Output: YES\n",
        "\n",
        "### Solution:\n",
        "\"\"\"\n",
        "\n",
        "output_finetuned = pipe_finetuned(prompt, max_new_tokens=256)\n",
        "print(\"After fine-tuning:\\n\", output_finetuned[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9784a97",
      "metadata": {
        "id": "a9784a97"
      },
      "source": [
        "We can see that the output, even if it's not the correct solution for the problem, is actually well structured python code, an improvement from the first output generated pre-finetuning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f270a427",
      "metadata": {
        "id": "f270a427"
      },
      "source": [
        "### Fine-tuning with 10k samples\n",
        "\n",
        "Now we will take a larger piece of the dataset for the finetuning, with 10000 samples, and again divide into train and test sets and tokenize it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2eec113a",
      "metadata": {
        "id": "2eec113a"
      },
      "outputs": [],
      "source": [
        "df_10k = df[[\"problem_description\", \"solution_code\"]].dropna().sample(10000, random_state=42).reset_index(drop=True)\n",
        "df_10k[\"text\"] = df_10k.apply(\n",
        "    lambda row: f\"### Problem:\\n{row['problem_description']}\\n### Solution:\\n{row['solution_code']}\", axis=1\n",
        ")\n",
        "\n",
        "dataset_10k = Dataset.from_pandas(df_10k[[\"text\"]])\n",
        "dataset_10k = dataset_10k.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23211a00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "763b4bc496dd433d8720b6dc5bb60b4d",
            "876211be65e147f49a137ae25f9242c1",
            "edd23127ebd44e55910cecda82b3d525",
            "795f831527a94d6782863cc11198dabc",
            "70a52aa216874de9a3a3615270660d8c",
            "6f699d16b37d471f879bb73d7faa0f71",
            "93423f0b67234cbf862a3cad691802be",
            "8cd5e0d2fa664ffe92994f0cacbc3a4c",
            "5f51ae35625741d698f5a4dfc25ed987",
            "7e662dac1a594cbda5f497fe666bcc00",
            "d9e1581856ee47dbb8df0eb7e329811d",
            "18503693d4a94852a38b860372a88137",
            "96a464b34414475f8f3362c3091903ae",
            "529e5aca14b64c23bf80a64dc0dc0552",
            "6d4ead32d81f44d29fcde048850aa899",
            "ba80022d0a25408bb8d0163a26393a60",
            "d7977403e5a04d40ab7dfef0588be9f8",
            "55b40e4079b942e986f43dca2fa04188",
            "bc9b09f6958247a4840385e709f47713",
            "9e03c2d683344d8aa13691b13340dc1f",
            "7b8fb89182a44ba8b7052264dd15ea89",
            "d0e70101d38b4cfd8e2a085e07e3f925"
          ]
        },
        "id": "23211a00",
        "outputId": "38e76974-abf3-404d-b483-73fb3a46a7bc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "763b4bc496dd433d8720b6dc5bb60b4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/9000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18503693d4a94852a38b860372a88137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_10k_dataset = dataset_10k.map(tokenize, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a699b8d6",
      "metadata": {
        "id": "a699b8d6"
      },
      "source": [
        "We define the Trainer and the training arguments, this time with 2 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6609b1df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6609b1df",
        "outputId": "44e98a29-9308-4a7d-d0e9-033d905eb828"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-e0c5a36a666c>:15: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-medium-finetuned-10kds\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=5e-5,\n",
        "    save_steps=5000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=100,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_10k_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_10k_dataset[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3963a6",
      "metadata": {
        "id": "4f3963a6"
      },
      "source": [
        "And start the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ffad43d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2ffad43d",
        "outputId": "71c2efc7-1d6a-4f45-ee21-584d9b389681"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9000' max='9000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [9000/9000 1:02:14, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.961900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.583300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.469600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.473500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.450400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.364800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.378400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.363800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.285100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.268500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.293700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.242000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.242300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.236300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.227000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.255000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.165100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.165100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.160800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.167200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.185200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.159500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.139400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.113800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.046000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.095600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.153000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.055000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.067600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.133400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.058000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.091100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.072600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.062200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.007600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.027400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>1.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.006600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>1.005000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>1.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.984400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4600</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4700</td>\n",
              "      <td>0.872500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4800</td>\n",
              "      <td>0.863500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4900</td>\n",
              "      <td>0.877400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5100</td>\n",
              "      <td>0.831300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5200</td>\n",
              "      <td>0.855800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5300</td>\n",
              "      <td>0.851100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5400</td>\n",
              "      <td>0.867600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.863800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5600</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5700</td>\n",
              "      <td>0.837000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5800</td>\n",
              "      <td>0.832600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5900</td>\n",
              "      <td>0.875200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.889600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6100</td>\n",
              "      <td>0.846900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6200</td>\n",
              "      <td>0.846100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6300</td>\n",
              "      <td>0.816500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6400</td>\n",
              "      <td>0.850600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.870000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6600</td>\n",
              "      <td>0.831900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6700</td>\n",
              "      <td>0.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6800</td>\n",
              "      <td>0.808800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6900</td>\n",
              "      <td>0.820700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.798900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7100</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7200</td>\n",
              "      <td>0.822700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7300</td>\n",
              "      <td>0.825900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7400</td>\n",
              "      <td>0.838500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.816100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7600</td>\n",
              "      <td>0.780700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7700</td>\n",
              "      <td>0.833000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7800</td>\n",
              "      <td>0.766500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7900</td>\n",
              "      <td>0.803200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.803900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8100</td>\n",
              "      <td>0.805700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8200</td>\n",
              "      <td>0.787700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8300</td>\n",
              "      <td>0.786500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8400</td>\n",
              "      <td>0.795900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.789600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8600</td>\n",
              "      <td>0.812400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8700</td>\n",
              "      <td>0.782500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8800</td>\n",
              "      <td>0.817500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8900</td>\n",
              "      <td>0.796500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.861200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=9000, training_loss=1.0219143998887803, metrics={'train_runtime': 3736.2941, 'train_samples_per_second': 4.818, 'train_steps_per_second': 2.409, 'total_flos': 1.6716612501504e+16, 'train_loss': 1.0219143998887803, 'epoch': 2.0})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56c31a21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "56c31a21",
        "outputId": "367dedcf-e22d-45b9-8064-12e9cff44980"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [500/500 00:48]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 0.7891965508460999,\n",
              " 'eval_runtime': 49.0426,\n",
              " 'eval_samples_per_second': 20.39,\n",
              " 'eval_steps_per_second': 10.195,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fafc714",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fafc714",
        "outputId": "854348fc-9928-4bbe-c2d0-4a9f572feee1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./gpt2-medium-finetuned-10k/tokenizer_config.json',\n",
              " './gpt2-medium-finetuned-10k/special_tokens_map.json',\n",
              " './gpt2-medium-finetuned-10k/vocab.json',\n",
              " './gpt2-medium-finetuned-10k/merges.txt',\n",
              " './gpt2-medium-finetuned-10k/added_tokens.json',\n",
              " './gpt2-medium-finetuned-10k/tokenizer.json')"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.save_model(\"./gpt2-medium-finetuned-10k\")\n",
        "tokenizer.save_pretrained(\"./gpt2-medium-finetuned-10k\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "764642d3",
      "metadata": {
        "id": "764642d3"
      },
      "source": [
        "Now we will run some metrics on the test set using 30 samples. First we will generate the output with the finetuned version for these samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1769bd20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1769bd20",
        "outputId": "eaeab30f-245d-465c-c42e-0f89edf15e3c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "100%|██████████| 30/30 [12:44<00:00, 25.49s/it]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_path = \"./gpt2-medium-finetuned-10k\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device= -1\n",
        ")\n",
        "\n",
        "test_dataset = dataset_10k[\"test\"]\n",
        "\n",
        "expected_codes = []\n",
        "generated_codes = []\n",
        "\n",
        "for example in tqdm(test_dataset.select(range(30))):\n",
        "    full_text = example[\"text\"]\n",
        "\n",
        "    try:\n",
        "        parts = full_text.split(\"### Problem:\\n\")[1].split(\"### Solution:\\n\")\n",
        "        description = parts[0].strip()\n",
        "        real_code = parts[1].strip()\n",
        "    except (IndexError, AttributeError):\n",
        "        continue\n",
        "\n",
        "    prompt = f\"### Problem:\\n{description}\\n### Solution:\\n\"\n",
        "\n",
        "    if not prompt.strip():\n",
        "        continue\n",
        "\n",
        "    max_prompt_tokens = 768\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_prompt_tokens)\n",
        "    prompt = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "    try:\n",
        "        output = generator(prompt, max_new_tokens=256, num_return_sequences=1, do_sample=False)[0][\"generated_text\"]\n",
        "    except RuntimeError as e:\n",
        "        print(\"GPU error:\", e)\n",
        "        continue\n",
        "\n",
        "    generated_code = output.split(\"### Solution:\\n\")[-1].strip()\n",
        "\n",
        "    expected_codes.append(real_code)\n",
        "    generated_codes.append(generated_code)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e34a0f85",
      "metadata": {
        "id": "e34a0f85"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_codes = pd.DataFrame({\n",
        "    \"reference_code\": expected_codes,\n",
        "    \"generated_code\": generated_codes\n",
        "})\n",
        "\n",
        "df_codes.to_csv(\"codes_comparison.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "737bc0c0",
      "metadata": {
        "id": "737bc0c0"
      },
      "source": [
        "Let's also run the model on the previous example to see the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25503504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25503504",
        "outputId": "ce6408e9-5ba7-49a7-81b5-4113de3a4925"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Problem:\n",
            "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
            "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
            "\n",
            "#### Examples:\n",
            "Input: 8\n",
            "Output: YES\n",
            "\n",
            "Input: 3\n",
            "Output: NO\n",
            "\n",
            "### Solution:\n",
            "w=int(input())\n",
            "for i in range(len(str(w))):\n",
            "    if str(i)==str(w-1):\n",
            "        print(\"YES\")\n",
            "    else:\n",
            "        print(\"NO\")\n"
          ]
        }
      ],
      "source": [
        "prompt = \"\"\"### Problem:\n",
        "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight.\n",
        "Write a Python function that receives an integer `w` (1 <= w <= 100) and returns \"YES\" if it's possible, or \"NO\" otherwise.\n",
        "\n",
        "#### Examples:\n",
        "Input: 8\n",
        "Output: YES\n",
        "\n",
        "Input: 3\n",
        "Output: NO\n",
        "\n",
        "### Solution:\n",
        "\"\"\"\n",
        "\n",
        "output = generator(prompt, max_new_tokens=256, do_sample=True, temperature=0.7)\n",
        "print(output[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "233676c5",
      "metadata": {
        "id": "233676c5"
      },
      "source": [
        "We can see that it does not generate the correct code for the problem, but it improves in comparison with the generated output from the very first model, since now we have well structured python code."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2f68231",
      "metadata": {
        "id": "b2f68231"
      },
      "source": [
        "We will also get the outputs from the original model for comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99e07987",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955,
          "referenced_widgets": [
            "ed83e1ed41c748cf9a47e17913801469",
            "57b1f6fabd5b4b518b941f3c60203b08",
            "d9e03181a2c14344b5ebc50893ebc1c8",
            "2c89b9b25f5a4f79a4e89d8e89d8e1f0",
            "42da2de819d246918718d843b964306c",
            "604b7132b3e14f449ac099f9eb702798",
            "12d30fbab7494586ab5dd13a315aa86f",
            "73c8499c66cc4d8c8dd4d812fbd52f55",
            "a6d716189c484723a3ae1e060eee5e32",
            "0db20d82d41548c6a2563a8d875f8558",
            "446efa47f7ca4cb39f6116eacc1a4331",
            "d270a55583344839b338243c1caa6888",
            "b8c495000d464cbbbcade3c90739df91",
            "6b2ef1938db646b5869d784fa6d31748",
            "d45348643b20451dbe032970df11a8e0",
            "8f308e82fdf141ef9b435a0ee11b9a84",
            "fe86d91d75b34bc88cce078847f83278",
            "77988a7cf1ef4d2e83c6645589c39466",
            "9ec4f85ee7eb40379876d3b9478ad5d8",
            "e467dc6317714b9cae8858b0475bf2f3",
            "ea4867c91b5b4c998064f56f694d4b20",
            "12747700a26a49d181e61fbcca7c3acd",
            "3d16f215d75b42ef900541e750227291",
            "236d7bb973354196bc18f348d8e21a06",
            "76ec335ddb584005ab2e63b844e71455",
            "497a3a3b31a94f3eb33fd51c3dd6dfb2",
            "0ddc362b4c29466e9de2da9d9325467f",
            "3aeb2172b5774caf8d5a54338928761f",
            "171b64cf4d624b6ea7990471d8a5b605",
            "4713b9fa2da3434e908c131399e435f0",
            "bb15fd914e404ebb82d203b6874a324c",
            "38d1f9e6d1104186a7c2bd129a49ea42",
            "4ec7e8282801455ea519c7151c4ec722",
            "bef7b99bcb2445f6bb0976f2d997aaa4",
            "7b93c339a2bc40b2b018f16b1adc662a",
            "064799fa1f754a799bf27a485cecdaa5",
            "201d2989f8194baeb2511e23d2453689",
            "84c49ee2542c4a45a8b6caf742bc825b",
            "d509dce7e84c41208599d742e8070628",
            "f2a5be1decc948a9852244562f98a223",
            "03e0668bb4b844ac89af7ef227313414",
            "b10e1bd73cfd401b92d25f7fb96dbe6e",
            "617ff60d327a4a15af686465280ea255",
            "e07b85eddffb44919f887b2c8e0fe83b",
            "f5f8b2ad2aef4617be213f663ddde0e1",
            "f2f1cc2598444dcf8ebd0d9638b1f9b3",
            "dc2ca14b44ff48bcb707c36b1f0a4738",
            "b18d849fcdbf462facf1d188d7ee8bc9",
            "01b3b4f794934673907d2a99a7aafcf2",
            "c36619dd9aea4a45895e8382d45940df",
            "e070d91c40654089b4daa5851fbb73db",
            "3b71790773194746a357ba9d2c25d04e",
            "05029e9ac5d64343b021e15aedcc6a58",
            "fa5b986a8c9b4221828a8e0d5d74bcf9",
            "730b2b769c1c4489891fdac6a977c33f",
            "45ad4d74c30e472eb5d9d9299083ec3f",
            "ed5e0d104b02488bba9bcd101cf1a8ce",
            "694e375fb7b64b979cfd049aa984d15e",
            "72026cb44fbe42658e7c3565f0e36501",
            "949265c8282c4f8d818b30aed3e6f0d8",
            "f9beb81bdb024d9dab64f6bb5b1edc48",
            "77023adc14c044bfb3e79190dc34a4f3",
            "adfbb988f3a5495bb09f07f83f44646b",
            "b77d1f6505e74d80a9293b9b134e3122",
            "b36f6dbb2c5e4c3d95b849535e8ce08e",
            "b841db9bfab74197a497995e9897e4d5",
            "7618b92b1bf94b50a99b5615a6f611af",
            "760b85ae61d34ce2883b57951024c695",
            "1fec0f43ea034c40b8ce8433db2b8896",
            "0900c9aa90174baa99097eff8e91dadd",
            "6c16b80fd19f486ab0b0d250d3b98b43",
            "5127f9475dcc476f9408cfc6eae8127d",
            "945b6a154d4d4d47a8e918c42d68200b",
            "1b4a938dc9004f4a877b3619776a0350",
            "0d37f63bc1084cdbb4a5329d617e1347",
            "4b0fda11cbf34daa8e9d15c3ddd5d433",
            "6752e27532df4a9093bffa35d4f3949d"
          ]
        },
        "id": "99e07987",
        "outputId": "0e6aa6b8-3b4c-447f-8c8b-f3036ea0a7a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed83e1ed41c748cf9a47e17913801469",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d270a55583344839b338243c1caa6888",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d16f215d75b42ef900541e750227291",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bef7b99bcb2445f6bb0976f2d997aaa4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f5f8b2ad2aef4617be213f663ddde0e1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45ad4d74c30e472eb5d9d9299083ec3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7618b92b1bf94b50a99b5615a6f611af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  3%|▎         | 1/30 [00:58<28:19, 58.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "  7%|▋         | 2/30 [02:03<29:06, 62.36s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 10%|█         | 3/30 [03:03<27:31, 61.17s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 13%|█▎        | 4/30 [04:00<25:48, 59.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 17%|█▋        | 5/30 [04:57<24:25, 58.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 20%|██        | 6/30 [05:54<23:14, 58.09s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 23%|██▎       | 7/30 [06:51<22:07, 57.70s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 27%|██▋       | 8/30 [07:45<20:47, 56.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 30%|███       | 9/30 [08:53<21:02, 60.11s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 33%|███▎      | 10/30 [09:48<19:31, 58.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 37%|███▋      | 11/30 [10:51<18:56, 59.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 40%|████      | 12/30 [11:44<17:19, 57.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 43%|████▎     | 13/30 [12:47<16:50, 59.41s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 47%|████▋     | 14/30 [13:39<15:15, 57.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 50%|█████     | 15/30 [14:32<13:57, 55.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 53%|█████▎    | 16/30 [15:21<12:34, 53.89s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 57%|█████▋    | 17/30 [16:19<11:54, 54.97s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 60%|██████    | 18/30 [17:15<11:02, 55.23s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 63%|██████▎   | 19/30 [18:07<09:58, 54.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 67%|██████▋   | 20/30 [19:06<09:17, 55.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 70%|███████   | 21/30 [20:11<08:45, 58.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 73%|███████▎  | 22/30 [21:12<07:55, 59.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 77%|███████▋  | 23/30 [22:05<06:42, 57.48s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 80%|████████  | 24/30 [23:02<05:43, 57.21s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 83%|████████▎ | 25/30 [24:01<04:49, 57.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 87%|████████▋ | 26/30 [25:07<04:01, 60.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 90%|█████████ | 27/30 [25:56<02:50, 56.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 93%|█████████▎| 28/30 [26:54<01:54, 57.29s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            " 97%|█████████▋| 29/30 [27:52<00:57, 57.51s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "100%|██████████| 30/30 [28:46<00:00, 57.56s/it]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "model_name = \"gpt2-medium\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=-1\n",
        ")\n",
        "\n",
        "test_dataset = dataset_10k[\"test\"]\n",
        "\n",
        "expected_codes_gpt2 = []\n",
        "generated_codes_gpt2 = []\n",
        "\n",
        "for example in tqdm(test_dataset.select(range(30))):\n",
        "    full_text = example[\"text\"]\n",
        "\n",
        "    try:\n",
        "        parts = full_text.split(\"### Problem:\\n\")[1].split(\"### Solution:\\n\")\n",
        "        description = parts[0].strip()\n",
        "        real_code = parts[1].strip()\n",
        "    except (IndexError, AttributeError):\n",
        "        continue\n",
        "\n",
        "    prompt = f\"### Problem:\\n{description}\\n### Solution:\\n\"\n",
        "\n",
        "    if not prompt.strip():\n",
        "        continue\n",
        "\n",
        "    max_prompt_tokens = 768\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=max_prompt_tokens)\n",
        "    prompt = tokenizer.decode(inputs[\"input_ids\"][0], skip_special_tokens=True)\n",
        "\n",
        "    try:\n",
        "        output = generator(prompt, max_new_tokens=256, num_return_sequences=1, do_sample=False)[0][\"generated_text\"]\n",
        "    except RuntimeError as e:\n",
        "        print(\"GPU error:\", e)\n",
        "        continue\n",
        "\n",
        "    generated_code = output.split(\"### Solution:\\n\")[-1].strip()\n",
        "\n",
        "    expected_codes_gpt2.append(real_code)\n",
        "    generated_codes_gpt2.append(generated_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b3c1ef",
      "metadata": {
        "id": "c7b3c1ef"
      },
      "outputs": [],
      "source": [
        "df_codes = pd.DataFrame({\n",
        "    \"reference_code\": expected_codes_gpt2,\n",
        "    \"generated_code\": generated_codes_gpt2\n",
        "})\n",
        "\n",
        "df_codes.to_csv(\"codes_comparison_gpt2.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52ab7adf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52ab7adf",
        "outputId": "937ca749-617d-4135-c43c-0adc7b287d71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=eef5c3cff69b0071032aab654ac2062804a828d7fa5b82cf4afb07be1ae8d809\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe0e0de6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe0e0de6",
        "outputId": "eb2bba63-a988-442f-b496-f7e4818f4a1b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74f3a92e",
      "metadata": {
        "id": "74f3a92e"
      },
      "source": [
        "Here's the function to calculate the metrics. Exact Match checks if the generated code is exactly the same as the reference. Levenshtein Similarity measures how similar two strings are based on the number of edit operations needed to match them. BLEU Score evaluates n-gram overlap between generated and reference texts, commonly used in machine translation. ROUGE-L captures the longest common subsequence, focusing on the structural similarity between outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41381556",
      "metadata": {
        "id": "41381556"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import difflib\n",
        "\n",
        "def exact_match(pred, ref):\n",
        "    return pred.strip() == ref.strip()\n",
        "\n",
        "def levenshtein_ratio(a, b):\n",
        "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def bleu_score(pred, ref):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(pred), smoothing_function=smoothie)\n",
        "\n",
        "def rouge_l_score(pred, ref):\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    return scorer.score(ref, pred)['rougeL'].fmeasure\n",
        "\n",
        "def compare_all(expected_codes, generated, label=\"\"):\n",
        "    print(f\"\\n=== Metrics for: {label} ===\")\n",
        "\n",
        "    em_list = []\n",
        "    lev_list = []\n",
        "    bleu_list = []\n",
        "    rouge_list = []\n",
        "\n",
        "    for pred, ref in zip(generated, expected_codes):\n",
        "        em_list.append(exact_match(pred, ref))\n",
        "        lev_list.append(levenshtein_ratio(pred, ref))\n",
        "        bleu_list.append(bleu_score(pred, ref))\n",
        "        rouge_list.append(rouge_l_score(pred, ref))\n",
        "\n",
        "    print(f\"Exact Match: {np.mean(em_list):.3f}\")\n",
        "    print(f\"Levenshtein Similarity: {np.mean(lev_list):.3f}\")\n",
        "    print(f\"BLEU Score: {np.mean(bleu_list):.3f}\")\n",
        "    print(f\"ROUGE-L Score: {np.mean(rouge_list):.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8dfa697",
      "metadata": {
        "id": "b8dfa697"
      },
      "outputs": [],
      "source": [
        "codes_finetuned = pd.read_csv('codes_comparison.csv')\n",
        "#codes_gpt2 = pd.read_csv('code_comparison_gpt2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ce98833",
      "metadata": {
        "id": "0ce98833"
      },
      "outputs": [],
      "source": [
        "expected_codes = codes_finetuned[\"reference_code\"].tolist()\n",
        "generated_codes_finetuned = codes_finetuned[\"generated_code\"].tolist()\n",
        "\n",
        "#expected_codes_gpt2 = codes_gpt2[\"reference_code\"].tolist()\n",
        "#generated_codes_gpt2 = codes_gpt2[\"generated_code\"].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dd1a720",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dd1a720",
        "outputId": "239f30ba-ad21-481f-e767-a648ecc0e06d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Metrics for: GPT-2 Original ===\n",
            "Exact Match: 0.000\n",
            "Levenshtein Similarity: 0.026\n",
            "BLEU Score: 0.004\n",
            "ROUGE-L Score: 0.057\n",
            "\n",
            "=== Metrics for: GPT-2 Fine-tuned ===\n",
            "Exact Match: 0.000\n",
            "Levenshtein Similarity: 0.282\n",
            "BLEU Score: 0.157\n",
            "ROUGE-L Score: 0.326\n"
          ]
        }
      ],
      "source": [
        "compare_all(expected_codes_gpt2, generated_codes_gpt2, label=\"GPT-2 Original\")\n",
        "compare_all(expected_codes, generated_codes_finetuned, label=\"GPT-2 Fine-tuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695d30f5",
      "metadata": {
        "id": "695d30f5"
      },
      "source": [
        "The metrics show that the fine-tuned GPT-2 model significantly outperforms the original GPT-2 in generating code closer to the expected solutions. While both models have an Exact Match score of 0, indicating no perfect matches, the fine-tuned model achieves much higher similarity scores across Levenshtein, BLEU, and ROUGE-L metrics. This suggests that fine-tuning helps the model produce code that is structurally and lexically more similar to the reference, improving overall generation quality even if exact reproduction is rare."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2f87278",
      "metadata": {
        "id": "e2f87278"
      },
      "source": [
        "## Fine tuning TinyLlama\n",
        "\n",
        "TinyLlama-1.1B-Chat-v1.0 (https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0) is a compact, chat-optimized language model with 1.1B parameters, based on the Llama 2 architecture. It was pretrained on 3T tokens and fine-tuned using UltraChat and UltraFeedback datasets to improve dialogue quality. Despite its small size, it performs well in chat tasks, especially on limited hardware."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f00f211",
      "metadata": {
        "id": "3f00f211"
      },
      "source": [
        "### Imports\n",
        "\n",
        "We start by doing the necessary library imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44ef89c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44ef89c7",
        "outputId": "05bb7773-4a4a-4860-d511-316e0bcf139c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.0/348.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q peft accelerate transformers datasets bitsandbytes trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0106ef94",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0106ef94",
        "outputId": "1013c7c3-12a0-41cb-f215-9e9222a3ede9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b75af3f7",
      "metadata": {
        "id": "b75af3f7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "import pandas as pd\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from google.colab import drive\n",
        "import os\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13e35ec",
      "metadata": {
        "id": "b13e35ec"
      },
      "source": [
        "### Load the model\n",
        "\n",
        "Before we do the fine-tuning, let's see how the pretrained model does on our task of code generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b6d49b",
      "metadata": {
        "id": "92b6d49b"
      },
      "source": [
        "In this next cell we will create a text-generation pipeline with the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4bab61d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369,
          "referenced_widgets": [
            "eecadc090b1b42b3a4c1703da53df287",
            "2534366b3e404c26b921f399b08a2aee",
            "dafc40c78c744fd7935987a9af215a7a",
            "82f4d81d218747c09acba954f056250e",
            "fb1a2ab23c4e4d69827d3e65bf572981",
            "82f7dabcc825434a8373a15b4df33dfa",
            "ef0aa683430f43e1a36c9139c2b469eb",
            "65d1ce86bb354c5894dba73f87047506",
            "51fa7908ac1649f7833b414297a3c2c1",
            "78cd0618810e43969b1802f5619084b8",
            "91a44d595650442bbca3a6ea74f62db6",
            "b8cf4592769247a38ee657c26507dc6a",
            "c1c467c195ee4ccfb7cdb54a17ae684d",
            "7492c046e02a46a1b440afddefcb023f",
            "04ddc4aaab1e4019bdf6eec9288dbaa7",
            "b8368c2229e84f0c91eaed240d8cd4dd",
            "cb88cb6396e74c3ebfc21b7bc1d04626",
            "2e53664623c34ef0ae49f7cb33d1b93e",
            "a0b09fbad3074af0a3c59c789f131f60",
            "79cf95a9af5e43ed9d9efb5aab70d762",
            "e51dd3c892aa49188fb37c6f080fc280",
            "52043f8d440842f7a2580757ec37cd04",
            "417c909e6e3e49bb926dcedbc326b90e",
            "d3d05551a89f48cfaeab36f2a34cc63d",
            "aa9d4d747a284edabd190bb63de461d7",
            "da82c898776c4ad6b9650c4d38ca2d2c",
            "cc47b67c54be417f8ec1067461083102",
            "1724304ac1dc41cebc900a1d16894f98",
            "80e623612ddb40b4adce28b2540ed2ff",
            "7a02668878c8459c8872637802b0acda",
            "0f6bd9a9348e41e0be86630a7d4be91b",
            "2637c7ee64b640d2a24513e68068082c",
            "684463e481144d43bdd6301dad753f2f",
            "063c83b9041f4dda9ed5d79e05a856b2",
            "5c1ab7f232b54620b789df0ad3c2c94c",
            "f37ad61971eb46b7be79b846ecb8d746",
            "d7561a3abd404f23b7ee3eedd09b22b8",
            "73e2d80bbe6a4b79b7bb8f0d165afcf2",
            "ff099077bb0f4fadb9ec06932189f02a",
            "02011c41ea9844f1a998f1ded293929d",
            "9a141e7fb5424e81b468e405781954c5",
            "0d51d00a04d74820937ea625ee7d5ada",
            "c87f92f79aee436fa5ce85b0f467ab01",
            "54f3a36990b446a598b16c55c6b3a942",
            "2f3324642780460c841c49c8124a8511",
            "1a3db02b40314fad9afc138199d61ed8",
            "975136e433944a70a8f13052a8ae207e",
            "596e29de49cf484c8ab4c3a1b6aca745",
            "b98f38cc1ff345258560520e4a1db594",
            "7d41929dbf244966879d1663ed1112a8",
            "d9a1e5308e2e4be3a1a19ebcee181a60",
            "25657f59f2d44e2195cea51ecb3d9e57",
            "1173fa03d3b94a408b456b54ac8c5f4a",
            "af49bb0c9dd4454b9674d3824b8210dd",
            "8fc428194c514719839d00971354f295",
            "ca8dfa0593724bcabc310538271d39e3",
            "7493a7ca2da54771873e8eebe246ca89",
            "1938c68d0dd14ebb873456fb9d08d48c",
            "2b89f76b31aa48e8be8e8a5a83c71ede",
            "3b8f307172da478d8a8fc13c6035c2d9",
            "27066d146b524ca9a54fe9994bd120cf",
            "9adc15331c49484d9665aebbbc18bada",
            "c041367bb6ec4cef870b02a4eb95cce8",
            "7cb75d156923435c9645be61cb61ad31",
            "482351c57f28447ea4d9af250c5b3f28",
            "e89cf61617274ebd8fe4ee7072bb32dc",
            "d6429252274d48d28d46a857e7dfcf0f",
            "c2da9cf8384d4411ac18839e8cd4cc8b",
            "98d428ee68114cd98a228d0af5c8e999",
            "b6053e8a17924275bc0ba3a095230c34",
            "3b7f1096eb224f4ba3c6d18d687cf846",
            "aad9ba88663d43d4b12e95ca3a0ac761",
            "65fc8e82a3404afdbfe1d27b45badbeb",
            "27f63169528949f0b27c4d7f9c28dbf6",
            "9a1513c407284eadb4ba3594d38ee344",
            "bb174bc3afde4ae2b4104ab83e6d6f6c",
            "d1e1b86eb0804ca0ba7196945b90a47d"
          ]
        },
        "id": "a4bab61d",
        "outputId": "0d6b62d0-96fe-4ffb-d724-c46953d6106f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eecadc090b1b42b3a4c1703da53df287",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/608 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8cf4592769247a38ee657c26507dc6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "417c909e6e3e49bb926dcedbc326b90e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "063c83b9041f4dda9ed5d79e05a856b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2f3324642780460c841c49c8124a8511",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca8dfa0593724bcabc310538271d39e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6429252274d48d28d46a857e7dfcf0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/551 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37534b58",
      "metadata": {
        "id": "37534b58"
      },
      "source": [
        "And we will pass as the messages a problem that is not in our bigger dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0729a62e",
      "metadata": {
        "id": "0729a62e"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.\"}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4c9c817",
      "metadata": {
        "id": "b4c9c817"
      },
      "source": [
        "Now we will generate the output using the messages above as input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd250cf4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd250cf4",
        "outputId": "46b03182-1aff-44ed-b28d-6fe454ed5227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are a helpful Python assistant.</s>\n",
            "<|user|>\n",
            "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.</s>\n",
            "<|assistant|>\n",
            "Here's a Python function that returns 'YES' if it's possible to split a watermelon of weight `w` into two parts, each of even positive weight, and 'NO' otherwise:\n",
            "\n",
            "```python\n",
            "def is_splitable(w):\n",
            "    \"\"\"\n",
            "    Takes a weight `w` and returns 'YES' if it's possible to split the watermelon into two parts,\n",
            "    each of even positive weight, or 'NO' otherwise.\n",
            "    \"\"\"\n",
            "    if w <= 0:\n",
            "        return \"Watermelon is not splitable!\"\n",
            "    return \"YES\" if (w % 2) == 0 else \"NO\"\n",
            "```\n",
            "\n",
            "Here's an example usage:\n",
            "\n",
            "```python\n",
            "watermelon_wgt = 100\n",
            "is_splitable = is_splitable(watermelon_wgt)\n",
            "print(is_splitable)  # Output: YES\n",
            "```\n",
            "\n",
            "This function takes an integer `w` (1 <= w <= 100) as input and returns either 'YES' (if it's possible to split the watermelon into two\n"
          ]
        }
      ],
      "source": [
        "prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
        "print(outputs[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "359baa7d",
      "metadata": {
        "id": "359baa7d"
      },
      "source": [
        "We can see that the model already produces python code for the task, as request in the system prompt. It also tries to explain the code generated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e0c933b",
      "metadata": {
        "id": "0e0c933b"
      },
      "source": [
        "### Load the dataset\n",
        "\n",
        "Now we will load out dataset to start the fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67b4fb7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "67b4fb7b",
        "outputId": "19eca7c0-db8c-438f-89d8-e2e9323cc854"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/.shortcut-targets-by-id/17WgJO1gfIBADpYX2jVdb41q7HCbwWcOU/NLP_Project'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "path = 'Colab Notebooks/NLP/NLP_Project'\n",
        "\n",
        "os.chdir(f'/content/drive/MyDrive/{path}')\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb04310a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb04310a",
        "outputId": "d5c379a1-6eba-4dd0-8624-b12093d09ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                 problem_description solution_id  \\\n",
            "0  Xenia has a set of weights and pan scales. Eac...         0_0   \n",
            "1  Xenia has a set of weights and pan scales. Eac...         0_2   \n",
            "2  Xenia has a set of weights and pan scales. Eac...         0_4   \n",
            "3  Xenia has a set of weights and pan scales. Eac...         0_6   \n",
            "4  Xenia has a set of weights and pan scales. Eac...         0_8   \n",
            "\n",
            "                                       solution_code  \\\n",
            "0  __author__ = 'ratnesh.mishra'\\n\\nweights = map...   \n",
            "1  import sys\\nsys.setrecursionlimit (1000000)\\n\\...   \n",
            "2  import sys\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n...   \n",
            "3  MOD = 10**9 + 7\\nI = lambda:list(map(int,input...   \n",
            "4  to_print = []\\ndef dfs(d, ini, s, depth, m):\\n...   \n",
            "\n",
            "               problem_name time_complexity_inferred space_complexity_inferred  \n",
            "0  339_C. Xenia and Weights                     O(1)                   O(n**2)  \n",
            "1  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "2  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "3  339_C. Xenia and Weights                     O(1)                      O(1)  \n",
            "4  339_C. Xenia and Weights                     O(1)                      O(1)  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('final_ds.csv')\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a52365ea",
      "metadata": {
        "id": "a52365ea"
      },
      "source": [
        "### Fine tuning with 1000 samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a30fe27",
      "metadata": {
        "id": "8a30fe27"
      },
      "source": [
        "First we will start with a small example, doing the fine-tuning with 1000 samples of the dataset. In this next cell we prepare the text that we will give to the model in the training putting together the descriptions and the code solutions in the messages format, and splitting it into train and test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5a24cff",
      "metadata": {
        "id": "f5a24cff"
      },
      "outputs": [],
      "source": [
        "df_small = df[[\"problem_description\", \"solution_code\"]].dropna().sample(1000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebaf5040",
      "metadata": {
        "id": "ebaf5040"
      },
      "outputs": [],
      "source": [
        "def format_messages(row):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": row[\"problem_description\"]},\n",
        "        {\"role\": \"assistant\", \"content\": row[\"solution_code\"]}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7be7617",
      "metadata": {
        "id": "c7be7617"
      },
      "outputs": [],
      "source": [
        "df_small[\"messages\"] = df_small.apply(format_messages, axis=1)\n",
        "dataset = Dataset.from_pandas(df_small[[\"messages\"]])\n",
        "dataset = dataset.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3322cc1a",
      "metadata": {
        "id": "3322cc1a"
      },
      "source": [
        "Here we define the original model name and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fa0c5a9",
      "metadata": {
        "id": "4fa0c5a9"
      },
      "outputs": [],
      "source": [
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0bb00a",
      "metadata": {
        "id": "dd0bb00a"
      },
      "source": [
        "This next function is used to tokenize the prepared dataset with the model's tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bedbd6ba",
      "metadata": {
        "id": "bedbd6ba"
      },
      "outputs": [],
      "source": [
        "def apply_chat_template(example):\n",
        "    prompt = tokenizer.apply_chat_template(example[\"messages\"], tokenize=False, add_generation_prompt=False)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "642d8159",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "25b465778d464fce93eb26ddb1c84454",
            "30d47bdbcf4f4c799f37914feeab92e5",
            "cfa606017dc04af3925a8f17101a61fb",
            "20fdb380f149478bba6e8a3ccda23abd",
            "dc3738ae56f34e1c9839282fc92d789e",
            "5245bbe0dd974d7da5cdbf394f1e041c",
            "b1eb89b0688d40128f9a3756a677cd31",
            "4e1ea2daef0f4d0299ed98dc66e62ed3",
            "a47978c61a964d12adb7779a5cf8fc81",
            "57bad2c2e87a482aad3895e7e2f48a7c",
            "8dfb51fb55fa483ca3c46c024b1062dc",
            "056990113e454b8a841fd46191bb6d53",
            "00eb3bf4cee6476e90c9e9ad5e502c91",
            "484d6da962824dbbb7a7d48c3059cfc2",
            "1a9eba7a81c44941b55866139630b8a5",
            "12b0b34ab220484dbf0aaa24b5f6358e",
            "fc176e05515649ceae590f6fb57620d5",
            "912a5349a92f41d08e796106b44bc467",
            "e135619ee45c4165a74c1302e310a641",
            "585e6c6adf304eb2b064efeb03fd9683",
            "4705379ccc0d4f50a421b6b9a3e12532",
            "3bbc6c038d1b42f7b54e07babf59109c"
          ]
        },
        "id": "642d8159",
        "outputId": "ddd55527-a730-4cac-98e8-f0545d9a163a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "25b465778d464fce93eb26ddb1c84454",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/900 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "056990113e454b8a841fd46191bb6d53",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset = dataset.map(apply_chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f11ef43",
      "metadata": {
        "id": "5f11ef43"
      },
      "source": [
        "This next cell loads the base model in 4-bit precision and prepares it for k-bit training. LoRA adapters are configured to fine-tune only the q_proj and v_proj layers in a memory-efficient way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a5cf6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1a5cf6a",
        "outputId": "92217805-2721-42f0-8331-171a4751ca01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b016f0f",
      "metadata": {
        "id": "0b016f0f"
      },
      "source": [
        "In this next step we create the Trainer, defining the training arguments and the model to be trained. We will train 2 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5edf7c82",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5edf7c82",
        "outputId": "d318af0c-d467-4744-940c-42cb8390f3e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-19-d8dc1c25d451>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tinyllama-chat-finetuned\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=100,\n",
        "    eval_steps=250,\n",
        "    save_steps=250,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5121254",
      "metadata": {
        "id": "e5121254"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6a0589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cc6a0589",
        "outputId": "502669c9-97e2-44c8-c999-329fe05011d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [900/900 11:28, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.537500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.176600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.178200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.132600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.160100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.147200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.150500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.059700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=900, training_loss=1.1935513051350912, metrics={'train_runtime': 689.5834, 'train_samples_per_second': 2.61, 'train_steps_per_second': 1.305, 'total_flos': 5726668220006400.0, 'train_loss': 1.1935513051350912, 'epoch': 2.0})"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "821cb03a",
      "metadata": {
        "id": "821cb03a"
      },
      "source": [
        "After training we save the model generated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d087ae",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88d087ae",
        "outputId": "b9854122-0b74-43f6-e1d5-cab747bada96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('tinyllama-lora-finetuned/tokenizer_config.json',\n",
              " 'tinyllama-lora-finetuned/special_tokens_map.json',\n",
              " 'tinyllama-lora-finetuned/tokenizer.model',\n",
              " 'tinyllama-lora-finetuned/added_tokens.json',\n",
              " 'tinyllama-lora-finetuned/tokenizer.json')"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"tinyllama-lora-finetuned\")\n",
        "tokenizer.save_pretrained(\"tinyllama-lora-finetuned\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6de1815d",
      "metadata": {
        "id": "6de1815d"
      },
      "source": [
        "Now we will generate an output with the same prompt from the beggining to see how the finetuned model behaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c39b33f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c39b33f",
        "outputId": "36b9ba35-aae2-4789-9254-08772de46db6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are a helpful Python assistant.</s>\n",
            "<|user|>\n",
            "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.</s>\n",
            "<|assistant|>\n",
            "w = int(input())\n",
            "if w % 2 == 0:\n",
            "    if w == 0:\n",
            "        print('NO')\n",
            "        return\n",
            "    print('YES')\n",
            "else:\n",
            "    print('NO')\n"
          ]
        }
      ],
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "model = PeftModel.from_pretrained(base_model, \"tinyllama-lora-finetuned\")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "output = pipe(prompt, max_new_tokens=256, do_sample=True)\n",
        "print(output[0][\"generated_text\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a00600a6",
      "metadata": {
        "id": "a00600a6"
      },
      "source": [
        "We can see that the model generated the correct code, but now without the explanations, which is more aligned with the examples from the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60f0f80",
      "metadata": {
        "id": "f60f0f80"
      },
      "source": [
        "### Fine tuning with 5000 samples\n",
        "\n",
        "Now we will try the finetuning with a little more samples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2fe62ab",
      "metadata": {
        "id": "a2fe62ab"
      },
      "source": [
        "We take 5000 samples of the dataset, divide again into train and test set, apply the messages format and tokenize it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "161475c8",
      "metadata": {
        "id": "161475c8"
      },
      "outputs": [],
      "source": [
        "df_medium = df[[\"problem_description\", \"solution_code\"]].dropna().sample(5000, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4998723b",
      "metadata": {
        "id": "4998723b"
      },
      "outputs": [],
      "source": [
        "df_medium[\"messages\"] = df_medium.apply(format_messages, axis=1)\n",
        "dataset_medium = Dataset.from_pandas(df_medium[[\"messages\"]])\n",
        "dataset_medium = dataset_medium.train_test_split(test_size=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "643de157",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "7f6aa18a31844dc78a811c9c6e30a067",
            "fde2d5539d844dd097e6a307a015d3f2",
            "89b678fd2c3546d08c4cee74301dd593",
            "c6bbefd0d5634d13a23a261ef1dd881c",
            "8172a4e9be5a40fd9c6a8a0579697ecc",
            "560277464c1345dd969ccc26e4086f96",
            "681001fe9b264bd28114502ad0e1955f",
            "6286ae9e5b684d39ba630b73e246794c",
            "906a2f374581433b9e66a992a01ddd33",
            "4896f62801a1441d853faa6145218568",
            "6e1d592f886a4e25a5e4db978eaaf1de",
            "f2ba02c66ebb429daf8d34b5b2dbb0d9",
            "31aac9b09562456b9a100da237add872",
            "d2399b0894094f4aabf3f69021611597",
            "953e0883e53d4784ad59146a80c9ef56",
            "8341ecf8a0534159a89e0c418701b600",
            "45798f3b912c4f9aa68751b590acef96",
            "2b8a95e2ca5b441a9658ce1678a432c5",
            "2b63ced9bf0e4df3ac1d7f6332a588d9",
            "81b922f35dd64c91ae41c4bf1aa0e6c1",
            "4e2b5cfd695746368c89d0ad5c324660",
            "cae07ae9320c4e9fa1203a75de62bc2f"
          ]
        },
        "id": "643de157",
        "outputId": "aac7e59d-fbea-46d2-a914-96df6f74efdf"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f6aa18a31844dc78a811c9c6e30a067",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/4500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2ba02c66ebb429daf8d34b5b2dbb0d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenized_dataset_medium = dataset_medium.map(apply_chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b02a742",
      "metadata": {
        "id": "3b02a742"
      },
      "source": [
        "Here we define again the training arguments and the Trainer, taking the new dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "964c6e18",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "964c6e18",
        "outputId": "2f39423a-0f84-41a0-913d-adcabbdb353d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-15-a8e69201f326>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./tinyllama-chat-finetuned-mediumds\",\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=100,\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset_medium[\"train\"],\n",
        "    eval_dataset=tokenized_dataset_medium[\"test\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df34dda0",
      "metadata": {
        "id": "df34dda0"
      },
      "source": [
        "And start the training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21454e44",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "21454e44",
        "outputId": "9a6c47b8-bc65-4fdd-867a-2c9f28744f1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4500/4500 55:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.226700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.172400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.161700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.138800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.161600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.112300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.087900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.139900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.139700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.082400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.120700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.086400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.066400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.083000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.082600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.085600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.088600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.106500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.093000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.082700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.025200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.091400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.042900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.069700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.018700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.055500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.044900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.047300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.056200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.019000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.019700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.023100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.051400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.076600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>1.075200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>1.016800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>1.055900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>1.044200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>1.048700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>1.053400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>1.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>1.015800</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=4500, training_loss=1.0915350663926866, metrics={'train_runtime': 3303.0539, 'train_samples_per_second': 2.725, 'train_steps_per_second': 1.362, 'total_flos': 2.8633341100032e+16, 'train_loss': 1.0915350663926866, 'epoch': 2.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "734fac32",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "734fac32",
        "outputId": "e75cb216-7ae1-4ace-fe83-9696270fcdd6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 00:58]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'eval_loss': 1.0485718250274658,\n",
              " 'eval_runtime': 58.3072,\n",
              " 'eval_samples_per_second': 8.575,\n",
              " 'eval_steps_per_second': 4.288,\n",
              " 'epoch': 2.0}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.evaluate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "900b7c71",
      "metadata": {
        "id": "900b7c71"
      },
      "source": [
        "Now we will generate again an output for the first prompt to see how the model behaves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c34c025e",
      "metadata": {
        "id": "c34c025e"
      },
      "outputs": [],
      "source": [
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                                bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "adapter_path = \"./tinyllama-chat-finetuned-mediumds/checkpoint-4500\"\n",
        "model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cc2ddb7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cc2ddb7",
        "outputId": "fe670740-c532-4a80-eb82-e75f9313a96f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|system|>\n",
            "You are a helpful Python assistant.</s>\n",
            "<|user|>\n",
            "Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.</s>\n",
            "<|assistant|>\n",
            "def is_positive_even(w):\n",
            "    if 1000/w == 54:\n",
            "        return True\n",
            "    if w % 2 == 0:\n",
            "        return True\n",
            "    \n",
            "    \n",
            "print(\"YES\", (is_positive_even(w) == True))\n"
          ]
        }
      ],
      "source": [
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Petya wants to split a watermelon of weight `w` into two parts, each of even positive weight. Write a Python function that receives an integer `w` (1 <= w <= 100) and returns 'YES' if it's possible, or 'NO' otherwise.\"}\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "output = pipe(prompt, max_new_tokens=256, do_sample=True)\n",
        "\n",
        "print(output[0][\"generated_text\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf596b10",
      "metadata": {
        "id": "bf596b10"
      },
      "source": [
        "Now we will run some metrics on the test set using 20 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcea66fd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcea66fd",
        "outputId": "04cf0c2c-153c-4057-89ba-8dd4ea577fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=bc8f39e186df29ce86ecfae01a2a7b7de1c6a5340fdcbcaa23126f28de3a6836\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f42595d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f42595d",
        "outputId": "76530d32-2235-4aa7-d675-91ed39e831b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59cf0eba",
      "metadata": {
        "id": "59cf0eba"
      },
      "source": [
        "Here's the function to calculate the metrics. Exact Match checks if the generated code is exactly the same as the reference. Levenshtein Similarity measures how similar two strings are based on the number of edit operations needed to match them. BLEU Score evaluates n-gram overlap between generated and reference texts, commonly used in machine translation. ROUGE-L captures the longest common subsequence, focusing on the structural similarity between outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f926664a",
      "metadata": {
        "id": "f926664a"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "import numpy as np\n",
        "import difflib\n",
        "\n",
        "def exact_match(pred, ref):\n",
        "    return pred.strip() == ref.strip()\n",
        "\n",
        "def levenshtein_ratio(a, b):\n",
        "    return difflib.SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "def bleu_score(pred, ref):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    return sentence_bleu([nltk.word_tokenize(ref)], nltk.word_tokenize(pred), smoothing_function=smoothie)\n",
        "\n",
        "def rouge_l_score(pred, ref):\n",
        "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
        "    return scorer.score(ref, pred)['rougeL'].fmeasure\n",
        "\n",
        "def compare_all(expected_codes, generated, label=\"\"):\n",
        "    print(f\"\\n=== Metrics for: {label} ===\")\n",
        "\n",
        "    em_list = []\n",
        "    lev_list = []\n",
        "    bleu_list = []\n",
        "    rouge_list = []\n",
        "\n",
        "    for pred, ref in zip(generated, expected_codes):\n",
        "        em_list.append(exact_match(pred, ref))\n",
        "        lev_list.append(levenshtein_ratio(pred, ref))\n",
        "        bleu_list.append(bleu_score(pred, ref))\n",
        "        rouge_list.append(rouge_l_score(pred, ref))\n",
        "\n",
        "    print(f\"Exact Match: {np.mean(em_list):.3f}\")\n",
        "    print(f\"Levenshtein Similarity: {np.mean(lev_list):.3f}\")\n",
        "    print(f\"BLEU Score: {np.mean(bleu_list):.3f}\")\n",
        "    print(f\"ROUGE-L Score: {np.mean(rouge_list):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9775d4d",
      "metadata": {
        "id": "a9775d4d"
      },
      "source": [
        "Now we will generate the outputs for the original model and for the finetuned model, and run the metrics function on the generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe5527e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fe5527e",
        "outputId": "e101a5a9-9833-46ff-c08c-97437ac83224"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n",
            "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FalconMambaForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'OlmoeForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
            " 50%|█████     | 10/20 [02:25<02:00, 12.01s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "100%|██████████| 20/20 [05:01<00:00, 15.09s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Metrics for: TinyLlama Original ===\n",
            "Exact Match: 0.000\n",
            "Levenshtein Similarity: 0.178\n",
            "BLEU Score: 0.105\n",
            "ROUGE-L Score: 0.239\n",
            "\n",
            "=== Metrics for: TinyLlama Fine-tuned ===\n",
            "Exact Match: 0.000\n",
            "Levenshtein Similarity: 0.202\n",
            "BLEU Score: 0.148\n",
            "ROUGE-L Score: 0.300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "def extract_code_from_output(output_text):\n",
        "    if \"<|assistant|>\" in output_text:\n",
        "        return output_text.split(\"<|assistant|>\")[-1].strip()\n",
        "    else:\n",
        "        return output_text.strip()\n",
        "\n",
        "base_model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                               bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "adapter_path = \"./tinyllama-chat-finetuned-mediumds/checkpoint-4500\"\n",
        "finetuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "pipe_orig = pipeline(\"text-generation\", model=base_model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "pipe_ft = pipeline(\"text-generation\", model=finetuned_model, tokenizer=tokenizer, device_map=\"auto\")\n",
        "\n",
        "test_dataset = dataset_medium[\"test\"].select(range(20))\n",
        "\n",
        "expected_codes = []\n",
        "generated_orig = []\n",
        "generated_ft = []\n",
        "\n",
        "for example in tqdm(test_dataset):\n",
        "    messages = example[\"messages\"]\n",
        "    description = \"\"\n",
        "    real_code = \"\"\n",
        "\n",
        "    for m in messages:\n",
        "        if m[\"role\"] == \"user\":\n",
        "            description = m[\"content\"].strip()\n",
        "        elif m[\"role\"] == \"assistant\":\n",
        "            real_code = m[\"content\"].strip()\n",
        "\n",
        "    if not description or not real_code:\n",
        "        continue\n",
        "\n",
        "    prompt_messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful Python assistant.\"},\n",
        "        {\"role\": \"user\", \"content\": description}\n",
        "    ]\n",
        "\n",
        "    prompt = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "    output_orig = pipe_orig(prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
        "    code_orig = extract_code_from_output(output_orig)\n",
        "\n",
        "    output_ft = pipe_ft(prompt, max_new_tokens=256, do_sample=True)[0][\"generated_text\"]\n",
        "    code_ft = extract_code_from_output(output_ft)\n",
        "\n",
        "    expected_codes.append(real_code)\n",
        "    generated_orig.append(code_orig)\n",
        "    generated_ft.append(code_ft)\n",
        "\n",
        "df_orig = pd.DataFrame({\n",
        "    \"reference_code\": expected_codes,\n",
        "    \"generated_code_original\": generated_orig\n",
        "})\n",
        "df_orig.to_csv(\"codes_comparison_tinyllama_original.csv\", index=False)\n",
        "\n",
        "df_ft = pd.DataFrame({\n",
        "    \"reference_code\": expected_codes,\n",
        "    \"generated_code_finetuned\": generated_ft\n",
        "})\n",
        "df_ft.to_csv(\"codes_comparison_tinyllama_finetuned.csv\", index=False)\n",
        "\n",
        "compare_all(expected_codes, generated_orig, label=\"TinyLlama Original\")\n",
        "compare_all(expected_codes, generated_ft, label=\"TinyLlama Fine-tuned\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a872c9",
      "metadata": {
        "id": "e3a872c9"
      },
      "source": [
        "The evaluation metrics show that the fine-tuned TinyLlama model outperforms the original in all measured aspects: Levenshtein similarity increased from 0.178 to 0.202, BLEU score improved from 0.105 to 0.148, and ROUGE-L rose from 0.239 to 0.300. Although these improvements are moderate, they reflect consistent gains in code generation quality. The relatively small increase is expected because the base model was already capable of generating Python code to some extent before fine-tuning."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}